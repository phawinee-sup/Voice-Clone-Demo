{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c00aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4093d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "N_MELS = 80\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SPEAKER_EMB_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5da0931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, phoneme_ids, speaker_id):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text = torch.LongTensor(phoneme_ids).unsqueeze(0).to(DEVICE)\n",
    "        spk = torch.LongTensor([speaker_id]).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        pred_mel, pred_dur = model(text, spk)\n",
    "\n",
    "        mel = pred_mel.squeeze(0).cpu()  # (80, T)\n",
    "\n",
    "    return mel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9db366ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_to_wav(mel):\n",
    "    mel = torch.exp(mel)  # inverse log\n",
    "    mel = mel.numpy()\n",
    "\n",
    "    wav = librosa.feature.inverse.mel_to_audio(\n",
    "        mel,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_iter=60\n",
    "    )\n",
    "    return wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7819b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LoRA Linear\n",
    "# =========================\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_f, out_f, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_f, in_f),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.A = nn.Parameter(torch.randn(r, in_f) * 0.01)\n",
    "        self.B = nn.Parameter(torch.randn(out_f, r) * 0.01)\n",
    "        self.scale = alpha / r\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = x @ self.weight.T\n",
    "        lora = (x @ self.A.T @ self.B.T) * self.scale\n",
    "        return base + lora\n",
    "\n",
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "class ResearchTTS_LoRA(nn.Module):\n",
    "    def __init__(self, vocab, dim=256, num_speakers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab, dim)\n",
    "        self.spk_embed = nn.Embedding(num_speakers, SPEAKER_EMB_DIM)\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            dim + SPEAKER_EMB_DIM,\n",
    "            dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.duration = nn.Conv1d(dim, 1, 3, padding=1)\n",
    "\n",
    "        self.decoder = nn.LSTM(dim, dim, batch_first=True)\n",
    "\n",
    "        self.mel_proj = LoRALinear(dim, N_MELS)\n",
    "\n",
    "        # Freeze backbone\n",
    "        for p in self.embed.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.decoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, text, speaker_id):\n",
    "        x = self.embed(text)\n",
    "\n",
    "        spk = self.spk_embed(speaker_id).squeeze(1)\n",
    "        spk = spk.unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "\n",
    "        x = torch.cat([x, spk], dim=-1)\n",
    "\n",
    "        x, _ = self.encoder(x)\n",
    "\n",
    "        dur = self.duration(x.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        # ===== Length Regulator =====\n",
    "        expanded = []\n",
    "        for b in range(x.size(0)):\n",
    "            reps = torch.clamp(dur[b].round().long(), min=1)\n",
    "            expanded_seq = torch.repeat_interleave(x[b], reps, dim=0)\n",
    "            expanded.append(expanded_seq)\n",
    "\n",
    "        x = torch.nn.utils.rnn.pad_sequence(expanded, batch_first=True)\n",
    "\n",
    "        x, _ = self.decoder(x)\n",
    "        mel = self.mel_proj(x).transpose(1, 2)\n",
    "\n",
    "        return mel, dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0180e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(wav, sr):\n",
    "    vad = webrtcvad.Vad(2)\n",
    "    frame = int(sr * 0.02)  # 20ms\n",
    "    voiced = []\n",
    "\n",
    "    for i in range(0, len(wav) - frame, frame):\n",
    "        chunk = wav[i:i+frame]\n",
    "        pcm = (chunk * 32768).astype(np.int16).tobytes()\n",
    "        try:\n",
    "            if vad.is_speech(pcm, sr):\n",
    "                voiced.extend(chunk)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return np.array(voiced) if len(voiced) > 0 else wav\n",
    "\n",
    "\n",
    "def wav_to_mel(wav):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=wav,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mels=N_MELS\n",
    "    )\n",
    "    return torch.FloatTensor(np.log(mel + 1e-6))\n",
    "\n",
    "class TTSDataset(Dataset):\n",
    "    def __init__(self, manifest):\n",
    "        self.data = json.load(open(manifest))\n",
    "        speakers = sorted({d[\"speaker\"] for d in self.data})\n",
    "        self.spk2id = {s: i for i, s in enumerate(speakers)}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        wav, _ = librosa.load(item[\"audio\"], sr=SAMPLE_RATE)\n",
    "        wav = trim_silence(wav, SAMPLE_RATE)\n",
    "\n",
    "        mel = wav_to_mel(wav)\n",
    "\n",
    "        text = torch.LongTensor(item[\"phonemes\"])\n",
    "\n",
    "        # pseudo duration\n",
    "        dur_value = max(1, mel.shape[1] // len(text))\n",
    "        duration = torch.ones(len(text)) * dur_value\n",
    "\n",
    "        speaker_id = torch.LongTensor([self.spk2id[item[\"speaker\"]]])\n",
    "\n",
    "        return text, duration, mel, speaker_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb77d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inference_whisper.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inference_whisper = json.load(f)\n",
    "\n",
    "for data_dict in inference_whisper:\n",
    "    data_dict[\"speaker\"] = \"Lisa\"\n",
    "\n",
    "with open(\"inference_whisper_Lisa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inference_whisper, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a32c5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# โหลดโมเดล\n",
    "dataset = TTSDataset(\"inference_whisper_Lisa.json\")\n",
    "\n",
    "max_token = max(max(d[\"phonemes\"]) for d in dataset.data)\n",
    "vocab_size = max_token + 10\n",
    "\n",
    "model = ResearchTTS_LoRA(\n",
    "    vocab=vocab_size,\n",
    "    num_speakers=len(dataset.spk2id)\n",
    ").to(DEVICE)\n",
    "\n",
    "model = ResearchTTS_LoRA(\n",
    "    vocab=5643,\n",
    "    num_speakers=4\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(\"research_tts_lora_multispk_whisper.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# phoneme\n",
    "phonemes = dataset.data[0][\"phonemes\"]\n",
    "speaker_name = dataset.data[0][\"speaker\"]\n",
    "speaker_id = dataset.spk2id[speaker_name]\n",
    "\n",
    "mel = inference(model, phonemes, speaker_id)\n",
    "\n",
    "wav = mel_to_wav(mel)\n",
    "\n",
    "wav = wav / np.max(np.abs(wav))\n",
    "sf.write(\"inference_whisper_Lisa_output.wav\", wav, SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7483525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
