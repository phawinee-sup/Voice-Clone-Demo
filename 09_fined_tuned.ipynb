{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e64f6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.57.3\n",
    "# !pip install qwen-tts==0.1.1\n",
    "# !pip install peft==0.10.0\n",
    "# !pip install datasets librosa accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea649fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/QwenLM/Qwen3-TTS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd Qwen3-TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00e67048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f997d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96023b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import torch\n",
    "# import torchaudio\n",
    "# from tqdm import tqdm\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from transformers import AutoConfig\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# from qwen_tts.core.models.processing_qwen3_tts import Qwen3TTSProcessor\n",
    "# from qwen_tts.core.models.modeling_qwen3_tts import Qwen3TTSForConditionalGeneration\n",
    "\n",
    "# # =============================\n",
    "# # CONFIG\n",
    "# # =============================\n",
    "\n",
    "# OUTPUT_DIR = \"output\"\n",
    "# MODEL_NAME = \"Qwen/Qwen3-TTS-12Hz-0.6B-Base\"\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# SAMPLE_RATE = 24000\n",
    "\n",
    "# EPOCHS = 3\n",
    "# BATCH_SIZE = 2\n",
    "# LR = 2e-4\n",
    "\n",
    "# # =============================\n",
    "# # DATASET\n",
    "# # =============================\n",
    "\n",
    "# class TTSDataset(Dataset):\n",
    "#     def __init__(self, manifest_path, processor):\n",
    "#         self.processor = processor\n",
    "\n",
    "#         with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             self.data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "#         speakers = sorted(set(d[\"speaker\"] for d in self.data))\n",
    "#         self.spk2id = {s: i for i, s in enumerate(speakers)}\n",
    "#         print(\"Speaker mapping:\", self.spk2id)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.data[idx]\n",
    "\n",
    "#         wav, sr = torchaudio.load(item[\"audio\"])\n",
    "#         wav = wav.mean(0)  # mono\n",
    "\n",
    "#         if sr != SAMPLE_RATE:\n",
    "#             wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
    "\n",
    "#         speaker_id = self.spk2id[item[\"speaker\"]]\n",
    "\n",
    "#         inputs = self.processor(\n",
    "#             text=item[\"text\"],\n",
    "#             audio=wav,\n",
    "#             sampling_rate=SAMPLE_RATE,\n",
    "#             speaker_id=speaker_id,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "#             \"labels\": inputs[\"audio_values\"].squeeze(0),  # üî• ‡πÉ‡∏ä‡πâ audio_values\n",
    "#         }\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "#         [b[\"input_ids\"] for b in batch],\n",
    "#         batch_first=True,\n",
    "#         padding_value=0\n",
    "#     )\n",
    "\n",
    "#     labels = torch.nn.utils.rnn.pad_sequence(\n",
    "#         [b[\"labels\"] for b in batch],\n",
    "#         batch_first=True,\n",
    "#         padding_value=0.0  # üî• audio ‡πÉ‡∏ä‡πâ 0 ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà -100\n",
    "#     )\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "\n",
    "\n",
    "# # =============================\n",
    "# # TRAIN\n",
    "# # =============================\n",
    "\n",
    "# def train(manifest_path, output_dir):\n",
    "\n",
    "#     print(\"Loading processor...\")\n",
    "#     processor = Qwen3TTSProcessor.from_pretrained(\n",
    "#         MODEL_NAME,\n",
    "#         trust_remote_code=True\n",
    "#     )\n",
    "\n",
    "#     print(\"Loading model...\")\n",
    "#     model = Qwen3TTSForConditionalGeneration.from_pretrained(\n",
    "#         MODEL_NAME,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         trust_remote_code=True\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     # LoRA\n",
    "#     lora_config = LoraConfig(\n",
    "#         r=32,\n",
    "#         lora_alpha=64,\n",
    "#         target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#         lora_dropout=0.05,\n",
    "#         bias=\"none\",\n",
    "#         task_type=\"CAUSAL_LM\"\n",
    "#     )\n",
    "\n",
    "#     model = get_peft_model(model, lora_config)\n",
    "#     model.print_trainable_parameters()\n",
    "\n",
    "#     dataset = TTSDataset(manifest_path, processor)\n",
    "\n",
    "#     dataloader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=True,\n",
    "#         collate_fn=collate_fn\n",
    "#     )\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "#     model.train()\n",
    "\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         total_loss = 0\n",
    "\n",
    "#         for batch in tqdm(dataloader):\n",
    "#             batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "#             outputs = model(\n",
    "#                 input_ids=batch[\"input_ids\"],\n",
    "#                 labels=batch[\"labels\"]\n",
    "#             )\n",
    "\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(f\"[{output_dir}] Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     model.save_pretrained(output_dir)\n",
    "#     processor.save_pretrained(output_dir)\n",
    "\n",
    "#     print(f\"Training complete ‚Üí saved to {output_dir}\")\n",
    "\n",
    "\n",
    "# # =============================\n",
    "# # GENERATE (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å speaker)\n",
    "# # =============================\n",
    "\n",
    "# def generate(text, speaker_name):\n",
    "\n",
    "#     processor = Qwen3TTSProcessor.from_pretrained(\n",
    "#         OUTPUT_DIR,\n",
    "#         trust_remote_code=True\n",
    "#     )\n",
    "\n",
    "#     model = Qwen3TTSForConditionalGeneration.from_pretrained(\n",
    "#         OUTPUT_DIR,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         trust_remote_code=True\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     inputs = processor(text=text, return_tensors=\"pt\")\n",
    "#     input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "#     codes, _ = model.generate(\n",
    "#         input_ids=input_ids,   # üî• ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà []\n",
    "#         languages=[\"auto\"],\n",
    "#         speakers=[speaker_name],\n",
    "#     )\n",
    "\n",
    "#     wav = processor.decode(codes[0])\n",
    "\n",
    "#     torchaudio.save(\n",
    "#         f\"output_{speaker_name}.wav\",\n",
    "#         wav.unsqueeze(0).cpu(),\n",
    "#         SAMPLE_RATE\n",
    "#     )\n",
    "\n",
    "#     print(\"Saved:\", f\"output_{speaker_name}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b891e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# base_path = \"C:/Users/User/Comvi/Voice/\"\n",
    "\n",
    "# def json_to_jsonl(input_path, output_path):\n",
    "#     with open(base_path+input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     for data_dict in data:\n",
    "#         data_dict[\"audio\"] = base_path + data_dict[\"audio\"]\n",
    "\n",
    "#     with open(base_path+output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for item in data:\n",
    "#             f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "#     print(\"‚úÖ Converted to JSONL:\", base_path+output_path)\n",
    "\n",
    "\n",
    "# json_to_jsonl(\"manifest_whisper_text.json\", \"manifest_whisper_text.jsonl\")\n",
    "# json_to_jsonl(\"manifest_pathumma_text.json\", \"manifest_pathumma_text.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6925b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Converted 2558 samples\n",
      "Saved to: C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\n",
      "‚úÖ Converted 2590 samples\n",
      "Saved to: C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_manifest_for_qwen3(\n",
    "    input_manifest,\n",
    "    output_manifest,\n",
    "    ref_audio_path\n",
    "):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏õ‡∏•‡∏á manifest ‡πÄ‡∏î‡∏¥‡∏° -> format ‡∏ó‡∏µ‡πà Qwen3-TTS official ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "    \n",
    "    Parameters:\n",
    "    - input_manifest: path ‡πÑ‡∏ü‡∏•‡πå jsonl ‡πÄ‡∏î‡∏¥‡∏°\n",
    "    - output_manifest: path ‡πÑ‡∏ü‡∏•‡πå jsonl ‡πÉ‡∏´‡∏°‡πà\n",
    "    - ref_audio_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå reference ‡πÄ‡∏™‡∏µ‡∏¢‡∏á A (‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏±‡πâ‡∏á dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(input_manifest, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_manifest, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        for line in fin:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            item = json.loads(line)\n",
    "\n",
    "            text = f\"[{item['speaker']}] \" + item[\"text\"]\n",
    "\n",
    "            new_item = {\n",
    "                \"audio\": item[\"audio\"],\n",
    "                \"text\": text,\n",
    "                \"ref_audio\": ref_audio_path[item['speaker']]\n",
    "            }\n",
    "\n",
    "            fout.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"‚úÖ Converted {count} samples\")\n",
    "    print(f\"Saved to: {output_manifest}\")\n",
    "\n",
    "ref_dict = {\"Lisa\": \"C:/Users/User/Comvi/Voice/Lisa/dataset_denoised/gEMrqw-pAy4/SPEAKER_01_sent_0001_DeepFilterNet3.wav\",\n",
    "                    \"Bambam\": \"C:/Users/User/Comvi/Voice/Bambam/dataset_denoised/zbo7Mk3ryaI/SPEAKER_00_sent_0001_DeepFilterNet3.wav\",\n",
    "                    \"IU\": \"C:/Users/User/Comvi/Voice/IU/dataset_denoised/wCbUWU4l_Ko/SPEAKER_01_sent_0000_DeepFilterNet3.wav\",\n",
    "                    \"IVE\": \"C:/Users/User/Comvi/Voice/IVE/dataset_denoised/_037bSnAyRg/SPEAKER_01_sent_0007_DeepFilterNet3.wav\"\n",
    "                    }\n",
    "\n",
    "convert_manifest_for_qwen3(\n",
    "    input_manifest=\"C:/Users/User/Comvi/Voice/manifest_whisper_text.jsonl\",\n",
    "    output_manifest=\"C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\",\n",
    "    ref_audio_path=ref_dict\n",
    ")\n",
    "\n",
    "convert_manifest_for_qwen3(\n",
    "    input_manifest=\"C:/Users/User/Comvi/Voice/manifest_pathumma_text.jsonl\",\n",
    "    output_manifest=\"C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\",\n",
    "    ref_audio_path=ref_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(\"C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\", \"C:/Users/User/Comvi/Voice/research_tts_whisper\")\n",
    "# print(\"\\n\\n\")\n",
    "# train(\"C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\", \"C:/Users/User/Comvi/Voice/research_tts_pathumma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d774f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Comvi\\Voice\\Qwen3-TTS\\finetuning\n"
     ]
    }
   ],
   "source": [
    "%cd Qwen3-TTS/finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad52add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def convert_to_24k(input_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Convert audio file to 24kHz mono WAV.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): path to original audio file\n",
    "        output_path (str, optional): output file path.\n",
    "                                     If None, will overwrite with _24k.wav suffix\n",
    "\n",
    "    Returns:\n",
    "        str: path to converted file\n",
    "    \"\"\"\n",
    "\n",
    "    # Load audio (auto-detect original sample rate)\n",
    "    audio, sr = librosa.load(input_path, sr=None, mono=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=24000)\n",
    "        sr = 24000\n",
    "\n",
    "    # Set output path\n",
    "    if output_path is None:\n",
    "        base, ext = os.path.splitext(input_path)\n",
    "        output_path = base + \"_24k.wav\"\n",
    "\n",
    "    # Save as WAV 24kHz\n",
    "    sf.write(output_path, audio, sr)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "folder_list = [\"C:/Users/User/Comvi/Voice/Lisa\", \"C:/Users/User/Comvi/Voice/Bambam\", \"C:/Users/User/Comvi/Voice/IU\", \"C:/Users/User/Comvi/Voice/IVE\"]\n",
    "file_path_list = []\n",
    "\n",
    "for folder in folder_list:\n",
    "    sub_folder = folder + \"/dataset\"\n",
    "    for file in os.listdir(sub_folder):\n",
    "        folder_path = folder + \"/dataset/\" + file\n",
    "        for wav_file in os.listdir(folder_path):\n",
    "            if wav_file.endswith(\".wav\"):\n",
    "                file_path = folder_path + \"/\" + wav_file\n",
    "                file_path_list.append(file_path)\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    convert_to_24k(file_path, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800b620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import soundfile as sf\n",
    "\n",
    "manifest_path = \"C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data_dict = json.loads(line.strip())\n",
    "        wav_path = data_dict[\"audio\"]\n",
    "\n",
    "        wav, sr = sf.read(wav_path)\n",
    "\n",
    "        if sr != 24000:\n",
    "            print(\"NOT 24K:\", wav_path, \"->\", sr)\n",
    "\n",
    "manifest_path = \"C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data_dict = json.loads(line.strip())\n",
    "        wav_path = data_dict[\"audio\"]\n",
    "\n",
    "        wav, sr = sf.read(wav_path)\n",
    "\n",
    "        if sr != 24000:\n",
    "            print(\"NOT 24K:\", wav_path, \"->\", sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd59eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********\n",
      "Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.\n",
      "********\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sox' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python prepare_data.py --device cuda:0 --tokenizer_model_path Qwen/Qwen3-TTS-Tokenizer-12Hz --input_jsonl \"C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\" --output_jsonl \"C:/Users/User/Comvi/Voice/manifest_whisper_train_with_code.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91c11ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********\n",
      "Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.\n",
      "********\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sox' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python prepare_data.py --device cuda:0 --tokenizer_model_path Qwen/Qwen3-TTS-Tokenizer-12Hz --input_jsonl \"C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\" --output_jsonl \"C:/Users/User/Comvi/Voice/manifest_pathumma_train_with_code.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10fd98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python sft_12hz.py --init_model_path \"C:/Users/User/.cache/huggingface/hub/models--Qwen--Qwen3-TTS-12Hz-1.7B-Base/snapshots/fd4b254389122332181a7c3db7f27e918eec64e3\" --output_model_path \"C:/Users/User/Comvi/Voice/output_whisper\" --train_jsonl \"C:/Users/User/Comvi/Voice/manifest_whisper_train_with_code.jsonl\" --batch_size 1 --lr 1e-6 --num_epochs 10 --speaker_name multi_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python sft_12hz.py --init_model_path \"C:/Users/User/.cache/huggingface/hub/models--Qwen--Qwen3-TTS-12Hz-1.7B-Base/snapshots/fd4b254389122332181a7c3db7f27e918eec64e3\" --output_model_path \"C:/Users/User/Comvi/Voice/output_pathumma\" --train_jsonl \"C:/Users/User/Comvi/Voice/manifest_pathumma_train_with_code.jsonl\" --batch_size 1 --lr 1e-6 --num_epochs 10 --speaker_name multi_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f28d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from qwen_tts import Qwen3TTSModel\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "tts = Qwen3TTSModel.from_pretrained(\n",
    "    \"output/checkpoint-epoch-4\",\n",
    "    device_map=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "ref_dict = {\"Lisa\": \"C:/Users/User/Comvi/Voice/Lisa/dataset_denoised/gEMrqw-pAy4/SPEAKER_01_sent_0001_DeepFilterNet3.wav\",\n",
    "            \"Bambam\": \"C:/Users/User/Comvi/Voice/Bambam/dataset_denoised/zbo7Mk3ryaI/SPEAKER_00_sent_0001_DeepFilterNet3.wav\",\n",
    "            \"IU\": \"C:/Users/User/Comvi/Voice/IU/dataset_denoised/wCbUWU4l_Ko/SPEAKER_01_sent_0000_DeepFilterNet3.wav\",\n",
    "            \"IVE\": \"C:/Users/User/Comvi/Voice/IVE/dataset_denoised/_037bSnAyRg/SPEAKER_01_sent_0007_DeepFilterNet3.wav\"\n",
    "            }\n",
    "\n",
    "def generate_voice(speaker_name, text_input, output_file):\n",
    "    ref_audio_path = ref_dict[speaker_name]\n",
    "    text_input = f\"[{speaker_name}] \" + text_input\n",
    "\n",
    "    wavs, sr = tts.generate_custom_voice(\n",
    "        text=text_input,\n",
    "        ref_audio=ref_audio_path,\n",
    "    )\n",
    "\n",
    "    sf.write(output_file, wavs[0], sr)\n",
    "\n",
    "song1 = \"\"\"‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏î‡∏≤‡∏ß‡∏î‡∏ß‡∏á‡∏ô‡∏∂‡∏á‡πÇ‡∏Ñ‡∏à‡∏£‡πÉ‡∏ô‡∏≠‡∏ß‡∏Å‡∏≤‡∏®..\n",
    "‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏â‡∏±‡∏ô‡∏Å‡πá‡∏°‡∏≠‡∏á‡πÇ‡∏•‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°...\n",
    "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏û‡∏ö‡πÄ‡∏à‡∏≠..\n",
    "‡∏Ñ‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß.. ‡∏ñ‡∏π‡∏Å‡∏°‡∏≤‡πÅ‡∏ó‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏î‡∏ß‡∏á‡∏ï‡∏≤‡∏Ñ‡∏π‡πà‡∏ô‡∏±‡πâ‡∏ô‡∏Ç‡∏≠‡∏á..‡πÄ‡∏ò‡∏≠...\n",
    "‡∏õ‡∏è‡∏¥‡∏ó‡∏¥‡∏ô‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏ö‡∏ß‡∏±‡∏ô.. ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡πÄ‡∏ù‡πâ‡∏≤‡∏Ñ‡∏≠‡∏¢‡∏ô‡∏±‡∏ö‡∏ô‡∏≤‡∏ó‡∏µ...\n",
    "‡∏Ç‡∏≠‡∏á‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏â‡∏±‡∏ô‡∏ô‡∏±‡∏ö‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ò‡∏≠‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô...\n",
    "\n",
    "‡∏Ç‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°..\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏ï‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏û‡∏ö‡πÄ‡∏ò‡∏≠‡∏à‡∏ô‡∏ß‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢..\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≠‡∏ô‡∏î‡∏π‡∏î‡∏≤‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ò‡∏≠‡∏≠‡∏µ‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏ß‡∏±‡∏ô\n",
    "‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏ô‡πÑ‡∏õ‡∏à‡∏∏‡∏°‡∏û‡∏¥‡∏ï‡πÄ‡∏ò‡∏≠‡∏ã‡∏±‡∏Å‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏â‡∏±‡∏ô‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô‡πÜ... ‡∏ô‡∏∞‡πÄ‡∏ò‡∏≠.....\n",
    "\n",
    "‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏•‡∏á‡∏£‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏±‡∏ô‡∏ö‡∏ó‡πÄ‡∏û‡∏•‡∏á‡∏£‡∏≠‡πÅ‡∏ä‡∏£‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏ò‡∏≠‡πÑ‡∏î‡πâ‡∏ü‡∏±‡∏á..\n",
    "‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏≠‡∏µ‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏£‡∏¥‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô...\n",
    "‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏≠‡∏µ‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏•‡∏π‡∏Å‡πÄ‡∏£‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏Å‡∏ß‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏°‡∏≤\n",
    "‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤...\n",
    "\n",
    "‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏î‡∏≤‡∏ß‡∏´‡∏≤‡∏á‡∏î‡∏ß‡∏á‡∏ô‡∏∂‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÇ‡∏Ñ‡∏à‡∏£‡πÉ‡∏ô‡∏≠‡∏ß‡∏Å‡∏≤‡∏®..\n",
    "‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏à‡∏∞‡∏°‡∏µ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏≤..\n",
    "‡∏î‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏â‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô\n",
    "‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô\n",
    "‡πÑ‡∏î‡πâ‡∏°‡∏µ‡πÄ‡∏ò‡∏≠‡∏£‡∏≠‡∏î‡∏π‡∏°‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏â‡∏±‡∏ô...\n",
    "\n",
    "‡∏Ç‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°..\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏ï‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏û‡∏ö‡πÄ‡∏ò‡∏≠‡∏à‡∏ô‡∏ß‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢..\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≠‡∏ô‡∏î‡∏π‡∏î‡∏≤‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ò‡∏≠‡∏≠‡∏µ‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏ß‡∏±‡∏ô\n",
    "‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏ô‡πÑ‡∏õ‡∏à‡∏∏‡∏°‡∏û‡∏¥‡∏ï‡πÄ‡∏ò‡∏≠‡∏ã‡∏±‡∏Å‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏â‡∏±‡∏ô‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô‡πÜ... ‡∏ô‡∏∞‡πÄ‡∏ò‡∏≠.....\n",
    "\n",
    "\n",
    "‡∏Ç‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°..\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡∏•‡∏∑‡∏°‡∏ï‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏û‡∏ö‡πÄ‡∏ò‡∏≠‡∏à‡∏ô‡∏ß‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢..\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≠‡∏ô‡∏î‡∏π‡∏î‡∏≤‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ò‡∏≠‡∏≠‡∏µ‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏ß‡∏±‡∏ô\n",
    "‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏ô‡πÑ‡∏õ‡∏à‡∏∏‡∏°‡∏û‡∏¥‡∏ï‡πÄ‡∏ò‡∏≠‡∏ã‡∏±‡∏Å‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏â‡∏±‡∏ô‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô‡πÜ... ‡∏ô‡∏∞‡πÄ‡∏ò‡∏≠.....\n",
    "\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≠‡∏ô‡∏î‡∏π‡∏î‡∏≤‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ò‡∏≠‡∏≠‡∏µ‡∏Å‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏ß‡∏±‡∏ô\n",
    "‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏ô‡πÑ‡∏õ‡∏à‡∏∏‡∏°‡∏û‡∏¥‡∏ï‡πÄ‡∏ò‡∏≠‡∏ã‡∏±‡∏Å‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á.....\n",
    "‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏â‡∏±‡∏ô‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô‡πÜ... ‡∏ô‡∏∞‡πÄ‡∏ò‡∏≠.....\"\"\"\n",
    "\n",
    "song2 = \"\"\"\n",
    "‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏û‡πâ‡∏≠..‡∏ô‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡∏°‡∏≠‡πà‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏á‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏™‡∏á‡∏Å‡∏£‡∏≤‡∏ô‡∏ï‡πå..\n",
    "‡πÄ‡∏ï‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏Æ‡πâ‡∏≤‡∏ô.. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•‡∏ß‡∏±‡∏ô‡πÑ‡∏´‡∏•...\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏¢‡∏¥‡πâ‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏´‡∏¢‡∏≠‡∏Å ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏Æ‡∏±‡∏Å‡∏≠‡πâ‡∏≤‡∏¢‡πÄ‡∏ö‡∏¥‡∏î‡πÉ‡∏à..\n",
    "‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ã‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏•‡πà.. ‡πÄ‡∏à‡πâ‡∏≤‡∏ß‡πà‡∏≤‡∏´‡∏±‡∏ß‡πÉ‡∏à‡πÄ‡∏Æ‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô\n",
    "\n",
    "‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ô‡∏™‡∏≠‡∏á‡∏Ñ‡∏∑‡∏ô.. ‡πÄ‡∏à‡πâ‡∏≤‡∏°‡∏≤‡∏Ç‡∏≠‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏Æ‡πá‡∏î‡∏á‡∏≤‡∏ô..\n",
    "‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏ö‡∏≤‡∏ô.. ‡∏ö‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡πà‡∏≤‡∏ô‡∏™‡∏¥‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏´‡∏≤..\n",
    "‡∏≠‡πâ‡∏≤‡∏¢‡∏Å‡πá‡∏ñ‡πà‡∏≤‡∏à‡∏ô‡πÄ‡∏®‡∏£‡πâ‡∏≤.. ‡∏Å‡∏≠‡∏î‡∏£‡∏π‡∏õ‡πÄ‡∏à‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏ô‡πâ‡∏≥‡∏ï‡∏≤..\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏Ñ‡∏á‡∏•‡∏∑‡∏°‡∏™‡∏±‡∏ç‡∏ç‡∏≤..‡∏™‡∏≤‡∏°‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏™....\n",
    "\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å...‡πÄ‡∏à‡∏≠ ‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏¥‡∏î‡∏Æ‡∏≠‡∏î ‡∏£‡∏≠‡∏ß‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏≠‡∏î...‡πÄ‡∏ò‡∏≠\n",
    "‡πÄ‡∏ù‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏ù‡∏±‡∏ô‡∏•‡∏∞‡πÄ‡∏°‡∏≠‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏Æ‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏ò‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡∏ö‡∏≠‡∏Å\n",
    "‡∏≠‡πâ‡∏≤‡∏¢‡∏ö‡πà‡∏Æ‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏¢‡∏≠‡∏Å\n",
    "‡∏¢‡πà‡∏≤‡∏ô‡∏Ñ‡∏ô‡∏™‡∏ß‡∏¢‡∏ô‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏°‡∏≤‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏≠‡πâ‡∏≤‡∏¢‡∏ä‡πâ‡∏≥..\n",
    "\n",
    "‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏ö‡∏≤‡∏ô.. ‡∏≠‡∏µ‡∏Å‡πÑ‡∏°‡πà‡∏ô‡∏≤‡∏ô‡∏Å‡πá‡∏Ñ‡∏á‡∏™‡∏¥‡πÄ‡∏â‡∏≤..\n",
    "‡∏≠‡πâ‡∏≤‡∏¢‡∏Å‡πá‡∏£‡∏≠‡πÄ‡∏à‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏Å‡πà‡∏≤ ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏™‡∏≤‡∏ß‡∏ú‡∏π‡πâ‡πÉ‡∏î‡∏´‡∏ô‡∏≠..‡∏ô‡∏≤‡∏á\n",
    "‡∏ô‡∏±‡πà‡∏á...‡∏Ñ‡∏∂‡∏î‡∏Æ‡∏≠‡∏î..‡∏ö‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≠‡∏ô‡∏à‡∏ô‡∏ü‡πâ‡∏≤‡∏™‡∏≤‡∏á..\n",
    "‡∏¢‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Æ‡∏±‡∏Å‡πÄ‡∏Æ‡∏≤‡πÅ‡∏ï‡∏Å‡∏°‡πà‡∏≤‡∏á\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏•‡∏∑‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏≠‡∏á‡πÄ‡∏Æ‡∏≤...\n",
    "\n",
    "‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏ö‡∏≤‡∏ô.. ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏á‡πÄ‡∏à‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏™..\n",
    "‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ù‡∏ô‡∏ö‡πà‡πÇ‡∏î‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà..\n",
    "‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏¢‡∏Å‡∏∞‡πÑ‡∏´‡∏•‡∏´‡∏¢‡πà‡∏≤‡∏ß...\n",
    "‡πÉ‡∏à‡∏™‡∏ß‡∏≠‡∏¢..‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏¢‡∏ñ‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏ô‡∏≤‡∏ß....\n",
    "‡∏à‡∏ô‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏Ç‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏¢‡πÄ‡∏´‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏â‡∏≤\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ö‡πà..‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤.....\n",
    "\n",
    "‡∏≠‡∏¢‡∏≤‡∏Å...‡πÄ‡∏à‡∏≠ ‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏¥‡∏î‡∏Æ‡∏≠‡∏î ‡∏£‡∏≠‡∏ß‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏≠‡∏î...‡πÄ‡∏ò‡∏≠\n",
    "‡πÄ‡∏ù‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏ù‡∏±‡∏ô‡∏•‡∏∞‡πÄ‡∏°‡∏≠‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏Æ‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏ò‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡∏ö‡∏≠‡∏Å\n",
    "‡∏≠‡πâ‡∏≤‡∏¢‡∏ö‡πà‡∏Æ‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏¢‡∏≠‡∏Å\n",
    "‡∏¢‡πà‡∏≤‡∏ô‡∏Ñ‡∏ô‡∏™‡∏ß‡∏¢‡∏ô‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏°‡∏≤‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏≠‡πâ‡∏≤‡∏¢‡∏ä‡πâ‡∏≥....\n",
    "\n",
    "‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏ö‡∏≤‡∏ô.. ‡∏≠‡∏µ‡∏Å‡πÑ‡∏°‡πà‡∏ô‡∏≤‡∏ô‡∏Å‡πá‡∏Ñ‡∏á‡∏™‡∏¥‡πÄ‡∏â‡∏≤..\n",
    "‡∏≠‡πâ‡∏≤‡∏¢‡∏Å‡πá‡∏£‡∏≠‡πÄ‡∏à‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏Å‡πà‡∏≤ ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏™‡∏≤‡∏ß‡∏ú‡∏π‡πâ‡πÉ‡∏î‡∏´‡∏ô‡∏≠..‡∏ô‡∏≤‡∏á\n",
    "‡∏ô‡∏±‡πà‡∏á...‡∏Ñ‡∏∂‡∏î‡∏Æ‡∏≠‡∏î..‡∏ö‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≠‡∏ô‡∏à‡∏ô‡∏ü‡πâ‡∏≤‡∏™‡∏≤‡∏á..\n",
    "‡∏¢‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Æ‡∏±‡∏Å‡πÄ‡∏Æ‡∏≤‡πÅ‡∏ï‡∏Å‡∏°‡πà‡∏≤‡∏á\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏•‡∏∑‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏≠‡∏á‡πÄ‡∏Æ‡∏≤...\n",
    "\n",
    "‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏ö‡∏≤‡∏ô.. ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏á‡πÄ‡∏à‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏™..\n",
    "‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ù‡∏ô‡∏ö‡πà‡πÇ‡∏î‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà..\n",
    "‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏¢‡∏Å‡∏∞‡πÑ‡∏´‡∏•‡∏´‡∏¢‡πà‡∏≤‡∏ß...\n",
    "‡πÉ‡∏à‡∏™‡∏ß‡∏≠‡∏¢..‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏¢‡∏ñ‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤....‡∏´‡∏ô‡∏≤‡∏ß....\n",
    "‡∏à‡∏ô‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏Ç‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏¢‡πÄ‡∏´‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏â‡∏≤\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ö‡πà..‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤........\n",
    "\n",
    "\n",
    "‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏ö‡∏≤‡∏ô.. ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏á‡πÄ‡∏à‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏™..\n",
    "‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ù‡∏ô‡∏ö‡πà‡πÇ‡∏î‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà..\n",
    "‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏¢‡∏Å‡∏∞‡πÑ‡∏´‡∏•‡∏´‡∏¢‡πà‡∏≤‡∏ß...\n",
    "‡πÉ‡∏à‡∏™‡∏ß‡∏≠‡∏¢..‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏¢‡∏ñ‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤....‡∏´‡∏ô‡∏≤‡∏ß....\n",
    "‡∏à‡∏ô‡∏î‡∏≠‡∏Å‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏Ç‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏¢‡πÄ‡∏´‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏â‡∏≤\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ö‡πà..‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤........\n",
    "\n",
    "‡∏à‡∏ô‡∏Å‡∏£‡∏∞‡πÄ‡∏à‡∏µ‡∏¢‡∏ß‡∏Ç‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏¢‡πÄ‡∏´‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏â‡∏≤\n",
    "‡πÄ‡∏à‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ö‡πà‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤....\"\"\"\n",
    "\n",
    "generate_voice(\"Lisa\", song1, \"C:/Users/User/Comvi/Voice/Lisa_song1.wav\")\n",
    "generate_voice(\"Lisa\", song2, \"C:/Users/User/Comvi/Voice/Lisa_song2.wav\")\n",
    "\n",
    "generate_voice(\"Bambam\", song1, \"C:/Users/User/Comvi/Voice/Bambam_song1.wav\")\n",
    "generate_voice(\"Bambam\", song2, \"C:/Users/User/Comvi/Voice/Bambam_song2.wav\")\n",
    "\n",
    "generate_voice(\"IU\", song1, \"C:/Users/User/Comvi/Voice/IU_song1.wav\")\n",
    "generate_voice(\"IU\", song2, \"C:/Users/User/Comvi/Voice/IU_song2.wav\")\n",
    "\n",
    "generate_voice(\"IVE\", song1, \"C:/Users/User/Comvi/Voice/IVE_song1.wav\")\n",
    "generate_voice(\"IVE\", song2, \"C:/Users/User/Comvi/Voice/IVE_song2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6602b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
