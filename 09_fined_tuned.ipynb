{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea649fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/QwenLM/Qwen3-TTS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00e67048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6925b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted 2558 samples\n",
      "Saved to: C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\n",
      "✅ Converted 2590 samples\n",
      "Saved to: C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_manifest_for_qwen3(\n",
    "    input_manifest,\n",
    "    output_manifest,\n",
    "    ref_audio_path\n",
    "):\n",
    "    \"\"\"\n",
    "    แปลง manifest เดิม -> format ที่ Qwen3-TTS official ต้องการ\n",
    "    \n",
    "    Parameters:\n",
    "    - input_manifest: path ไฟล์ jsonl เดิม\n",
    "    - output_manifest: path ไฟล์ jsonl ใหม่\n",
    "    - ref_audio_path: path ของไฟล์ reference เสียง A (ใช้ไฟล์เดียวทั้ง dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(input_manifest, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_manifest, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        for line in fin:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            item = json.loads(line)\n",
    "\n",
    "            text = f\"[{item['speaker']}] \" + item[\"text\"]\n",
    "\n",
    "            new_item = {\n",
    "                \"audio\": item[\"audio\"],\n",
    "                \"text\": text,\n",
    "                \"ref_audio\": ref_audio_path[item['speaker']]\n",
    "            }\n",
    "\n",
    "            fout.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"✅ Converted {count} samples\")\n",
    "    print(f\"Saved to: {output_manifest}\")\n",
    "\n",
    "ref_dict = {\"Lisa\": \"C:/Users/User/Comvi/Voice/Lisa/dataset_denoised/gEMrqw-pAy4/SPEAKER_01_sent_0001_DeepFilterNet3.wav\",\n",
    "                    \"Bambam\": \"C:/Users/User/Comvi/Voice/Bambam/dataset_denoised/zbo7Mk3ryaI/SPEAKER_00_sent_0001_DeepFilterNet3.wav\",\n",
    "                    \"IU\": \"C:/Users/User/Comvi/Voice/IU/dataset_denoised/wCbUWU4l_Ko/SPEAKER_01_sent_0000_DeepFilterNet3.wav\",\n",
    "                    \"IVE\": \"C:/Users/User/Comvi/Voice/IVE/dataset_denoised/_037bSnAyRg/SPEAKER_01_sent_0007_DeepFilterNet3.wav\"\n",
    "                    }\n",
    "\n",
    "convert_manifest_for_qwen3(\n",
    "    input_manifest=\"C:/Users/User/Comvi/Voice/manifest_whisper_text.jsonl\",\n",
    "    output_manifest=\"C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\",\n",
    "    ref_audio_path=ref_dict\n",
    ")\n",
    "\n",
    "convert_manifest_for_qwen3(\n",
    "    input_manifest=\"C:/Users/User/Comvi/Voice/manifest_pathumma_text.jsonl\",\n",
    "    output_manifest=\"C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\",\n",
    "    ref_audio_path=ref_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee204b4-6229-4163-b5a1-5b530fac3339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Qwen3-TTS/finetuning\n"
     ]
    }
   ],
   "source": [
    "%cd Qwen3-TTS/finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad52add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def convert_to_24k(input_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Convert audio file to 24kHz mono WAV.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): path to original audio file\n",
    "        output_path (str, optional): output file path.\n",
    "                                     If None, will overwrite with _24k.wav suffix\n",
    "\n",
    "    Returns:\n",
    "        str: path to converted file\n",
    "    \"\"\"\n",
    "\n",
    "    # Load audio (auto-detect original sample rate)\n",
    "    audio, sr = librosa.load(input_path, sr=None, mono=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=24000)\n",
    "        sr = 24000\n",
    "\n",
    "    # Set output path\n",
    "    if output_path is None:\n",
    "        base, ext = os.path.splitext(input_path)\n",
    "        output_path = base + \"_24k.wav\"\n",
    "\n",
    "    # Save as WAV 24kHz\n",
    "    sf.write(output_path, audio, sr)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "folder_list = [\"C:/Users/User/Comvi/Voice/Lisa\", \"C:/Users/User/Comvi/Voice/Bambam\", \"C:/Users/User/Comvi/Voice/IU\", \"C:/Users/User/Comvi/Voice/IVE\"]\n",
    "file_path_list = []\n",
    "\n",
    "for folder in folder_list:\n",
    "    sub_folder = folder + \"/dataset\"\n",
    "    for file in os.listdir(sub_folder):\n",
    "        folder_path = folder + \"/dataset/\" + file\n",
    "        for wav_file in os.listdir(folder_path):\n",
    "            if wav_file.endswith(\".wav\"):\n",
    "                file_path = folder_path + \"/\" + wav_file\n",
    "                file_path_list.append(file_path)\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    convert_to_24k(file_path, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800b620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import soundfile as sf\n",
    "\n",
    "manifest_path = \"C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data_dict = json.loads(line.strip())\n",
    "        wav_path = data_dict[\"audio\"]\n",
    "\n",
    "        wav, sr = sf.read(wav_path)\n",
    "\n",
    "        if sr != 24000:\n",
    "            print(\"NOT 24K:\", wav_path, \"->\", sr)\n",
    "\n",
    "manifest_path = \"C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data_dict = json.loads(line.strip())\n",
    "        wav_path = data_dict[\"audio\"]\n",
    "\n",
    "        wav, sr = sf.read(wav_path)\n",
    "\n",
    "        if sr != 24000:\n",
    "            print(\"NOT 24K:\", wav_path, \"->\", sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd59eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********\n",
      "Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.\n",
      "********\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sox' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python prepare_data.py --device cuda:0 --tokenizer_model_path Qwen/Qwen3-TTS-Tokenizer-12Hz --input_jsonl \"C:/Users/User/Comvi/Voice/manifest_whisper_train_raw.jsonl\" --output_jsonl \"C:/Users/User/Comvi/Voice/manifest_whisper_train_with_code.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91c11ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********\n",
      "Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.\n",
      "********\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sox' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python prepare_data.py --device cuda:0 --tokenizer_model_path Qwen/Qwen3-TTS-Tokenizer-12Hz --input_jsonl \"C:/Users/User/Comvi/Voice/manifest_pathumma_train_raw.jsonl\" --output_jsonl \"C:/Users/User/Comvi/Voice/manifest_pathumma_train_with_code.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e648ac6-e745-40c9-bbef-dcd78dbbea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (0.27.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Collecting shellingham (from huggingface_hub)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typer-slim (from huggingface_hub)\n",
      "  Downloading typer_slim-0.23.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.14.0)\n",
      "Collecting typer>=0.23.0 (from typer-slim->huggingface_hub)\n",
      "  Downloading typer-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.23.0->typer-slim->huggingface_hub)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.23.0->typer-slim->huggingface_hub)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from typer>=0.23.0->typer-slim->huggingface_hub)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface_hub)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface_hub) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface_hub)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m227.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.23.0-py3-none-any.whl (3.4 kB)\n",
      "Downloading typer-0.23.0-py3-none-any.whl (56 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading rich-14.3.2-py3-none-any.whl (309 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: tqdm, shellingham, mdurl, hf-xet, click, annotated-doc, markdown-it-py, rich, typer, typer-slim, huggingface_hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [huggingface_hub] [huggingface_hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-doc-0.0.4 click-8.3.1 hf-xet-1.2.0 huggingface_hub-1.4.1 markdown-it-py-4.0.0 mdurl-0.1.2 rich-14.3.2 shellingham-1.5.4 tqdm-4.67.3 typer-0.23.0 typer-slim-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8732584b-740c-4bcd-86cc-ccd1b8792b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `snapshot_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4140cae513a649f4b132130a3ed474fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047124df7cd94493a968f1f4269809af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/Qwen3-TTS/finetuning/Qwen3-TTS-12Hz-1.7B-Base'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n",
    "    local_dir=\"./Qwen3-TTS-12Hz-1.7B-Base\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752955ef-6fa0-4500-ba27-fc503b76ba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved to: ../../manifest_whisper_train_with_code_fixed.jsonl\n",
      "Done. Saved to: ../../manifest_pathumma_train_with_code_fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import soundfile as sf\n",
    "\n",
    "manifest_path = \"../../manifest_whisper_train_with_code.jsonl\"\n",
    "output_path = \"../../manifest_whisper_train_with_code_fixed.jsonl\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f_in, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "\n",
    "    for line in f_in:\n",
    "        data_dict = json.loads(line.strip())\n",
    "\n",
    "        wav_path = data_dict[\"audio\"]\n",
    "        new_path = wav_path.replace(\"C:/Users/User/Comvi/Voice/\", \"../../\")\n",
    "        data_dict[\"audio\"] = new_path\n",
    "\n",
    "        ref_path = data_dict[\"ref_audio\"]\n",
    "        new_ref_path = ref_path.replace(\"C:/Users/User/Comvi/Voice/\", \"../../\")\n",
    "        data_dict[\"ref_audio\"] = new_ref_path\n",
    "\n",
    "        # เขียนกลับเป็น json 1 บรรทัด\n",
    "        f_out.write(json.dumps(data_dict, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Done. Saved to:\", output_path)\n",
    "\n",
    "manifest_path = \"../../manifest_pathumma_train_with_code.jsonl\"\n",
    "output_path = \"../../manifest_pathumma_train_with_code_fixed.jsonl\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f_in, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "\n",
    "    for line in f_in:\n",
    "        data_dict = json.loads(line.strip())\n",
    "\n",
    "        wav_path = data_dict[\"audio\"]\n",
    "        new_path = wav_path.replace(\"C:/Users/User/Comvi/Voice/\", \"../../\")\n",
    "        data_dict[\"audio\"] = new_path\n",
    "\n",
    "        ref_path = data_dict[\"ref_audio\"]\n",
    "        new_ref_path = ref_path.replace(\"C:/Users/User/Comvi/Voice/\", \"../../\")\n",
    "        data_dict[\"ref_audio\"] = new_ref_path\n",
    "\n",
    "        # เขียนกลับเป็น json 1 บรรทัด\n",
    "        f_out.write(json.dumps(data_dict, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Done. Saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a10fd98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: sox: not found\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:529: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Epoch 0 | Step 0 | Loss: 15.8386\n",
      "Epoch 0 | Step 10 | Loss: 14.2030\n",
      "Epoch 0 | Step 20 | Loss: 14.0107\n",
      "Epoch 0 | Step 30 | Loss: 12.1982\n",
      "Epoch 0 | Step 40 | Loss: 12.6076\n",
      "Epoch 0 | Step 50 | Loss: 12.7813\n",
      "Epoch 0 | Step 60 | Loss: 13.9061\n",
      "Epoch 0 | Step 70 | Loss: 13.8017\n",
      "Epoch 0 | Step 80 | Loss: 13.9025\n",
      "Epoch 0 | Step 90 | Loss: 12.2090\n",
      "Epoch 0 | Step 100 | Loss: 11.9164\n",
      "Epoch 0 | Step 110 | Loss: 14.6128\n",
      "Epoch 0 | Step 120 | Loss: 12.1083\n",
      "Epoch 0 | Step 130 | Loss: 13.3431\n",
      "Epoch 0 | Step 140 | Loss: 14.6870\n",
      "Epoch 0 | Step 150 | Loss: 13.4364\n",
      "Epoch 0 | Step 160 | Loss: 12.6391\n",
      "Epoch 0 | Step 170 | Loss: 12.8253\n",
      "Epoch 0 | Step 180 | Loss: 14.0234\n",
      "Epoch 0 | Step 190 | Loss: 12.7351\n",
      "Epoch 0 | Step 200 | Loss: 8.8535\n",
      "Epoch 0 | Step 210 | Loss: 12.7984\n",
      "Epoch 0 | Step 220 | Loss: 13.9042\n",
      "Epoch 0 | Step 230 | Loss: 12.5116\n",
      "Epoch 0 | Step 240 | Loss: 12.0386\n",
      "Epoch 0 | Step 250 | Loss: 10.9689\n",
      "Epoch 0 | Step 260 | Loss: 11.8935\n",
      "Epoch 0 | Step 270 | Loss: 10.8338\n",
      "Epoch 0 | Step 280 | Loss: 12.0742\n",
      "Epoch 0 | Step 290 | Loss: 10.6511\n",
      "Epoch 0 | Step 300 | Loss: 11.3825\n",
      "Epoch 0 | Step 310 | Loss: 11.8877\n",
      "Epoch 0 | Step 320 | Loss: 11.7507\n",
      "Epoch 0 | Step 330 | Loss: 13.2414\n",
      "Epoch 0 | Step 340 | Loss: 12.5654\n",
      "Epoch 0 | Step 350 | Loss: 11.7486\n",
      "Epoch 0 | Step 360 | Loss: 12.8229\n",
      "Epoch 0 | Step 370 | Loss: 12.3565\n",
      "Epoch 0 | Step 380 | Loss: 12.4863\n",
      "Epoch 0 | Step 390 | Loss: 12.2102\n",
      "Epoch 0 | Step 400 | Loss: 11.8341\n",
      "Epoch 0 | Step 410 | Loss: 11.9515\n",
      "Epoch 0 | Step 420 | Loss: 11.0347\n",
      "Epoch 0 | Step 430 | Loss: 11.7198\n",
      "Epoch 0 | Step 440 | Loss: 11.4645\n",
      "Epoch 0 | Step 450 | Loss: 12.3507\n",
      "Epoch 0 | Step 460 | Loss: 12.1320\n",
      "Epoch 0 | Step 470 | Loss: 9.9370\n",
      "Epoch 0 | Step 480 | Loss: 10.7619\n",
      "Epoch 0 | Step 490 | Loss: 10.7619\n",
      "Epoch 0 | Step 500 | Loss: 9.8508\n",
      "Epoch 0 | Step 510 | Loss: 11.9725\n",
      "Epoch 0 | Step 520 | Loss: 12.2175\n",
      "Epoch 0 | Step 530 | Loss: 10.9125\n",
      "Epoch 0 | Step 540 | Loss: 12.3373\n",
      "Epoch 0 | Step 550 | Loss: 10.4537\n",
      "Epoch 0 | Step 560 | Loss: 10.8137\n",
      "Epoch 0 | Step 570 | Loss: 12.7654\n",
      "Epoch 0 | Step 580 | Loss: 11.0695\n",
      "Epoch 0 | Step 590 | Loss: 10.5318\n",
      "Epoch 0 | Step 600 | Loss: 10.4954\n",
      "Epoch 0 | Step 610 | Loss: 10.8650\n",
      "Epoch 0 | Step 620 | Loss: 8.8066\n",
      "Epoch 0 | Step 630 | Loss: 11.1290\n",
      "Epoch 0 | Step 640 | Loss: 10.0719\n",
      "Epoch 0 | Step 650 | Loss: 11.0809\n",
      "Epoch 0 | Step 660 | Loss: 10.1313\n",
      "Epoch 0 | Step 670 | Loss: 9.4820\n",
      "Epoch 0 | Step 680 | Loss: 9.2722\n",
      "Epoch 0 | Step 690 | Loss: 11.2349\n",
      "Epoch 0 | Step 700 | Loss: 9.4485\n",
      "Epoch 0 | Step 710 | Loss: 10.0442\n",
      "Epoch 0 | Step 720 | Loss: 8.8678\n",
      "Epoch 0 | Step 730 | Loss: 10.2377\n",
      "Epoch 0 | Step 740 | Loss: 10.3984\n",
      "Epoch 0 | Step 750 | Loss: 10.0093\n",
      "Epoch 0 | Step 760 | Loss: 10.8074\n",
      "Epoch 0 | Step 770 | Loss: 11.1910\n",
      "Epoch 0 | Step 780 | Loss: 8.9574\n",
      "Epoch 0 | Step 790 | Loss: 10.7908\n",
      "Epoch 0 | Step 800 | Loss: 10.4214\n",
      "Epoch 0 | Step 810 | Loss: 10.8629\n",
      "Epoch 0 | Step 820 | Loss: 8.6923\n",
      "Epoch 0 | Step 830 | Loss: 11.2807\n",
      "Epoch 0 | Step 840 | Loss: 10.0723\n",
      "Epoch 0 | Step 850 | Loss: 10.7996\n",
      "Epoch 0 | Step 860 | Loss: 10.6248\n",
      "Epoch 0 | Step 870 | Loss: 10.4790\n",
      "Epoch 0 | Step 880 | Loss: 10.6161\n",
      "Epoch 0 | Step 890 | Loss: 9.7867\n",
      "Epoch 0 | Step 900 | Loss: 10.3910\n",
      "Epoch 0 | Step 910 | Loss: 10.4656\n",
      "Epoch 0 | Step 920 | Loss: 10.7811\n",
      "Epoch 0 | Step 930 | Loss: 10.5014\n",
      "Epoch 0 | Step 940 | Loss: 9.2558\n",
      "Epoch 0 | Step 950 | Loss: 9.4533\n",
      "Epoch 0 | Step 960 | Loss: 10.3164\n",
      "Epoch 0 | Step 970 | Loss: 9.7733\n",
      "Epoch 0 | Step 980 | Loss: 10.3160\n",
      "Epoch 0 | Step 990 | Loss: 10.3021\n",
      "Epoch 0 | Step 1000 | Loss: 11.1704\n",
      "Epoch 0 | Step 1010 | Loss: 10.7098\n",
      "Epoch 0 | Step 1020 | Loss: 10.5124\n",
      "Epoch 0 | Step 1030 | Loss: 10.7556\n",
      "Epoch 0 | Step 1040 | Loss: 9.9950\n",
      "Epoch 0 | Step 1050 | Loss: 9.8756\n",
      "Epoch 0 | Step 1060 | Loss: 10.3677\n",
      "Epoch 0 | Step 1070 | Loss: 10.9623\n",
      "Epoch 0 | Step 1080 | Loss: 10.4097\n",
      "Epoch 0 | Step 1090 | Loss: 10.5703\n",
      "Epoch 0 | Step 1100 | Loss: 9.3840\n",
      "Epoch 0 | Step 1110 | Loss: 8.3697\n",
      "Epoch 0 | Step 1120 | Loss: 10.0002\n",
      "Epoch 0 | Step 1130 | Loss: 10.8529\n",
      "Epoch 0 | Step 1140 | Loss: 10.1730\n",
      "Epoch 0 | Step 1150 | Loss: 9.8306\n",
      "Epoch 0 | Step 1160 | Loss: 10.8977\n",
      "Epoch 0 | Step 1170 | Loss: 10.5541\n",
      "Epoch 0 | Step 1180 | Loss: 11.6433\n",
      "Epoch 0 | Step 1190 | Loss: 8.7897\n",
      "Epoch 0 | Step 1200 | Loss: 10.0614\n",
      "Epoch 0 | Step 1210 | Loss: 9.9821\n",
      "Epoch 0 | Step 1220 | Loss: 10.5378\n",
      "Epoch 0 | Step 1230 | Loss: 9.2878\n",
      "Epoch 0 | Step 1240 | Loss: 10.5063\n",
      "Epoch 0 | Step 1250 | Loss: 10.9387\n",
      "Epoch 0 | Step 1260 | Loss: 10.9125\n",
      "Epoch 0 | Step 1270 | Loss: 9.4076\n",
      "Epoch 0 | Step 1280 | Loss: 8.4756\n",
      "Epoch 0 | Step 1290 | Loss: 9.4221\n",
      "Epoch 0 | Step 1300 | Loss: 11.0472\n",
      "Epoch 0 | Step 1310 | Loss: 9.2838\n",
      "Epoch 0 | Step 1320 | Loss: 10.4776\n",
      "Epoch 0 | Step 1330 | Loss: 9.5800\n",
      "Epoch 0 | Step 1340 | Loss: 9.5831\n",
      "Epoch 0 | Step 1350 | Loss: 9.5960\n",
      "Epoch 0 | Step 1360 | Loss: 9.7046\n",
      "Epoch 0 | Step 1370 | Loss: 9.8586\n",
      "Epoch 0 | Step 1380 | Loss: 9.7729\n",
      "Epoch 0 | Step 1390 | Loss: 9.7288\n",
      "Epoch 0 | Step 1400 | Loss: 10.3979\n",
      "Epoch 0 | Step 1410 | Loss: 9.9005\n",
      "Epoch 0 | Step 1420 | Loss: 9.9055\n",
      "Epoch 0 | Step 1430 | Loss: 9.7131\n",
      "Epoch 0 | Step 1440 | Loss: 9.2183\n",
      "Epoch 0 | Step 1450 | Loss: 9.1216\n",
      "Epoch 0 | Step 1460 | Loss: 8.4145\n",
      "Epoch 0 | Step 1470 | Loss: 8.4737\n",
      "Epoch 0 | Step 1480 | Loss: 10.7331\n",
      "Epoch 0 | Step 1490 | Loss: 10.7179\n",
      "Epoch 0 | Step 1500 | Loss: 8.7424\n",
      "Epoch 0 | Step 1510 | Loss: 9.9659\n",
      "Epoch 0 | Step 1520 | Loss: 9.8605\n",
      "Epoch 0 | Step 1530 | Loss: 9.3895\n",
      "Epoch 0 | Step 1540 | Loss: 10.5524\n",
      "Epoch 0 | Step 1550 | Loss: 9.5639\n",
      "Epoch 0 | Step 1560 | Loss: 9.5008\n",
      "Epoch 0 | Step 1570 | Loss: 9.6327\n",
      "Epoch 0 | Step 1580 | Loss: 9.7285\n",
      "Epoch 0 | Step 1590 | Loss: 9.3600\n",
      "Epoch 0 | Step 1600 | Loss: 10.0890\n",
      "Epoch 0 | Step 1610 | Loss: 8.4526\n",
      "Epoch 0 | Step 1620 | Loss: 9.1968\n",
      "Epoch 0 | Step 1630 | Loss: 7.9692\n",
      "Epoch 0 | Step 1640 | Loss: 9.4953\n",
      "Epoch 0 | Step 1650 | Loss: 9.7634\n",
      "Epoch 0 | Step 1660 | Loss: 9.4230\n",
      "Epoch 0 | Step 1670 | Loss: 10.0472\n",
      "Epoch 0 | Step 1680 | Loss: 10.6843\n",
      "Epoch 0 | Step 1690 | Loss: 8.7662\n",
      "Epoch 0 | Step 1700 | Loss: 7.4008\n",
      "Epoch 0 | Step 1710 | Loss: 9.7080\n",
      "Epoch 0 | Step 1720 | Loss: 10.0826\n",
      "Epoch 0 | Step 1730 | Loss: 9.5294\n",
      "Epoch 0 | Step 1740 | Loss: 8.8844\n",
      "Epoch 0 | Step 1750 | Loss: 9.5965\n",
      "Epoch 0 | Step 1760 | Loss: 9.6764\n",
      "Epoch 0 | Step 1770 | Loss: 9.5793\n",
      "Epoch 0 | Step 1780 | Loss: 8.3140\n",
      "Epoch 0 | Step 1790 | Loss: 10.2894\n",
      "Epoch 0 | Step 1800 | Loss: 9.5466\n",
      "Epoch 0 | Step 1810 | Loss: 9.8294\n",
      "Epoch 0 | Step 1820 | Loss: 9.4041\n",
      "Epoch 0 | Step 1830 | Loss: 9.3688\n",
      "Epoch 0 | Step 1840 | Loss: 9.5561\n",
      "Epoch 0 | Step 1850 | Loss: 9.4117\n",
      "Epoch 0 | Step 1860 | Loss: 9.1278\n",
      "Epoch 0 | Step 1870 | Loss: 9.4301\n",
      "Epoch 0 | Step 1880 | Loss: 9.9404\n",
      "Epoch 0 | Step 1890 | Loss: 9.5693\n",
      "Epoch 0 | Step 1900 | Loss: 9.0342\n",
      "Epoch 0 | Step 1910 | Loss: 9.4660\n",
      "Epoch 0 | Step 1920 | Loss: 9.8774\n",
      "Epoch 0 | Step 1930 | Loss: 8.5984\n",
      "Epoch 0 | Step 1940 | Loss: 10.5620\n",
      "Epoch 0 | Step 1950 | Loss: 8.9787\n",
      "Epoch 0 | Step 1960 | Loss: 9.3586\n",
      "Epoch 0 | Step 1970 | Loss: 10.3404\n",
      "Epoch 0 | Step 1980 | Loss: 10.2835\n",
      "Epoch 0 | Step 1990 | Loss: 9.6683\n",
      "Epoch 0 | Step 2000 | Loss: 8.4897\n",
      "Epoch 0 | Step 2010 | Loss: 9.2187\n",
      "Epoch 0 | Step 2020 | Loss: 9.2321\n",
      "Epoch 0 | Step 2030 | Loss: 10.2284\n",
      "Epoch 0 | Step 2040 | Loss: 7.9260\n",
      "Epoch 0 | Step 2050 | Loss: 7.2454\n",
      "Epoch 0 | Step 2060 | Loss: 8.9859\n",
      "Epoch 0 | Step 2070 | Loss: 9.2370\n",
      "Epoch 0 | Step 2080 | Loss: 9.4921\n",
      "Epoch 0 | Step 2090 | Loss: 8.9958\n",
      "Epoch 0 | Step 2100 | Loss: 10.2415\n",
      "Epoch 0 | Step 2110 | Loss: 9.2040\n",
      "Epoch 0 | Step 2120 | Loss: 10.3975\n",
      "Epoch 0 | Step 2130 | Loss: 9.2756\n",
      "Epoch 0 | Step 2140 | Loss: 8.6219\n",
      "Epoch 0 | Step 2150 | Loss: 9.3583\n",
      "Epoch 0 | Step 2160 | Loss: 9.2525\n",
      "Epoch 0 | Step 2170 | Loss: 9.6720\n",
      "Epoch 0 | Step 2180 | Loss: 9.1198\n",
      "Epoch 0 | Step 2190 | Loss: 9.2595\n",
      "Epoch 0 | Step 2200 | Loss: 9.4616\n",
      "Epoch 0 | Step 2210 | Loss: 9.0805\n",
      "Epoch 0 | Step 2220 | Loss: 9.5030\n",
      "Epoch 0 | Step 2230 | Loss: 7.8955\n",
      "Epoch 0 | Step 2240 | Loss: 9.3456\n",
      "Epoch 0 | Step 2250 | Loss: 9.7401\n",
      "Epoch 0 | Step 2260 | Loss: 8.8388\n",
      "Epoch 0 | Step 2270 | Loss: 8.6580\n",
      "Epoch 0 | Step 2280 | Loss: 9.2675\n",
      "Epoch 0 | Step 2290 | Loss: 8.1497\n",
      "Epoch 0 | Step 2300 | Loss: 8.5538\n",
      "Epoch 0 | Step 2310 | Loss: 8.6435\n",
      "Epoch 0 | Step 2320 | Loss: 8.8924\n",
      "Epoch 0 | Step 2330 | Loss: 8.8013\n",
      "Epoch 0 | Step 2340 | Loss: 7.0459\n",
      "Epoch 0 | Step 2350 | Loss: 9.2423\n",
      "Epoch 0 | Step 2360 | Loss: 9.5266\n",
      "Epoch 0 | Step 2370 | Loss: 8.9621\n",
      "Epoch 0 | Step 2380 | Loss: 8.7341\n",
      "Epoch 0 | Step 2390 | Loss: 9.0899\n",
      "Epoch 0 | Step 2400 | Loss: 8.9169\n",
      "Epoch 0 | Step 2410 | Loss: 9.5569\n",
      "Epoch 0 | Step 2420 | Loss: 8.7027\n",
      "Epoch 0 | Step 2430 | Loss: 8.6367\n",
      "Epoch 0 | Step 2440 | Loss: 9.9135\n",
      "Epoch 0 | Step 2450 | Loss: 9.1057\n",
      "Epoch 0 | Step 2460 | Loss: 9.7285\n",
      "Epoch 0 | Step 2470 | Loss: 9.1500\n",
      "Epoch 0 | Step 2480 | Loss: 8.7941\n",
      "Epoch 0 | Step 2490 | Loss: 9.6842\n",
      "Epoch 0 | Step 2500 | Loss: 8.1608\n",
      "Epoch 0 | Step 2510 | Loss: 9.4471\n",
      "Epoch 0 | Step 2520 | Loss: 9.0031\n",
      "Epoch 0 | Step 2530 | Loss: 9.0069\n",
      "Epoch 0 | Step 2540 | Loss: 8.6387\n",
      "Epoch 0 | Step 2550 | Loss: 8.6815\n",
      "Epoch 1 | Step 0 | Loss: 8.6384\n",
      "Epoch 1 | Step 10 | Loss: 9.6685\n",
      "Epoch 1 | Step 20 | Loss: 9.0111\n",
      "Epoch 1 | Step 30 | Loss: 9.4853\n",
      "Epoch 1 | Step 40 | Loss: 6.9239\n",
      "Epoch 1 | Step 50 | Loss: 9.0846\n",
      "Epoch 1 | Step 60 | Loss: 8.1851\n",
      "Epoch 1 | Step 70 | Loss: 9.4205\n",
      "Epoch 1 | Step 80 | Loss: 10.0893\n",
      "Epoch 1 | Step 90 | Loss: 9.2937\n",
      "Epoch 1 | Step 100 | Loss: 8.7174\n",
      "Epoch 1 | Step 110 | Loss: 9.6235\n",
      "Epoch 1 | Step 120 | Loss: 9.6684\n",
      "Epoch 1 | Step 130 | Loss: 9.7588\n",
      "Epoch 1 | Step 140 | Loss: 10.1947\n",
      "Epoch 1 | Step 150 | Loss: 9.9192\n",
      "Epoch 1 | Step 160 | Loss: 8.6718\n",
      "Epoch 1 | Step 170 | Loss: 7.4331\n",
      "Epoch 1 | Step 180 | Loss: 8.8626\n",
      "Epoch 1 | Step 190 | Loss: 9.0166\n",
      "Epoch 1 | Step 200 | Loss: 10.1027\n",
      "Epoch 1 | Step 210 | Loss: 9.1592\n",
      "Epoch 1 | Step 220 | Loss: 9.5729\n",
      "Epoch 1 | Step 230 | Loss: 9.9456\n",
      "Epoch 1 | Step 240 | Loss: 10.2689\n",
      "Epoch 1 | Step 250 | Loss: 9.4192\n",
      "Epoch 1 | Step 260 | Loss: 7.9657\n",
      "Epoch 1 | Step 270 | Loss: 9.4335\n",
      "Epoch 1 | Step 280 | Loss: 7.9581\n",
      "Epoch 1 | Step 290 | Loss: 9.7497\n",
      "Epoch 1 | Step 300 | Loss: 9.1864\n",
      "Epoch 1 | Step 310 | Loss: 9.3997\n",
      "Epoch 1 | Step 320 | Loss: 9.7142\n",
      "Epoch 1 | Step 330 | Loss: 9.3923\n",
      "Epoch 1 | Step 340 | Loss: 9.4818\n",
      "Epoch 1 | Step 350 | Loss: 9.7912\n",
      "Epoch 1 | Step 360 | Loss: 9.2210\n",
      "Epoch 1 | Step 370 | Loss: 9.2762\n",
      "Epoch 1 | Step 380 | Loss: 9.1199\n",
      "Epoch 1 | Step 390 | Loss: 7.8084\n",
      "Epoch 1 | Step 400 | Loss: 8.9088\n",
      "Epoch 1 | Step 410 | Loss: 9.1678\n",
      "Epoch 1 | Step 420 | Loss: 8.6751\n",
      "Epoch 1 | Step 430 | Loss: 9.1704\n",
      "Epoch 1 | Step 440 | Loss: 7.6075\n",
      "Epoch 1 | Step 450 | Loss: 9.6742\n",
      "Epoch 1 | Step 460 | Loss: 9.8096\n",
      "Epoch 1 | Step 470 | Loss: 7.3553\n",
      "Epoch 1 | Step 480 | Loss: 7.0289\n",
      "Epoch 1 | Step 490 | Loss: 8.3504\n",
      "Epoch 1 | Step 500 | Loss: 8.9117\n",
      "Epoch 1 | Step 510 | Loss: 8.9286\n",
      "Epoch 1 | Step 520 | Loss: 8.5835\n",
      "Epoch 1 | Step 530 | Loss: 9.3292\n",
      "Epoch 1 | Step 540 | Loss: 10.1567\n",
      "Epoch 1 | Step 550 | Loss: 8.8003\n",
      "Epoch 1 | Step 560 | Loss: 8.7498\n",
      "Epoch 1 | Step 570 | Loss: 9.8388\n",
      "Epoch 1 | Step 580 | Loss: 8.0182\n",
      "Epoch 1 | Step 590 | Loss: 10.0069\n",
      "Epoch 1 | Step 600 | Loss: 9.2064\n",
      "Epoch 1 | Step 610 | Loss: 9.3324\n",
      "Epoch 1 | Step 620 | Loss: 8.0828\n",
      "Epoch 1 | Step 630 | Loss: 8.9474\n",
      "Epoch 1 | Step 640 | Loss: 8.8983\n",
      "Epoch 1 | Step 650 | Loss: 9.5516\n",
      "Epoch 1 | Step 660 | Loss: 6.4820\n",
      "Epoch 1 | Step 670 | Loss: 8.7322\n",
      "Epoch 1 | Step 680 | Loss: 8.4826\n",
      "Epoch 1 | Step 690 | Loss: 9.6295\n",
      "Epoch 1 | Step 700 | Loss: 9.6244\n",
      "Epoch 1 | Step 710 | Loss: 9.0003\n",
      "Epoch 1 | Step 720 | Loss: 9.2086\n",
      "Epoch 1 | Step 730 | Loss: 9.8640\n",
      "Epoch 1 | Step 740 | Loss: 8.8956\n",
      "Epoch 1 | Step 750 | Loss: 8.4976\n",
      "Epoch 1 | Step 760 | Loss: 9.4893\n",
      "Epoch 1 | Step 770 | Loss: 8.8162\n",
      "Epoch 1 | Step 780 | Loss: 8.8575\n",
      "Epoch 1 | Step 790 | Loss: 9.5826\n",
      "Epoch 1 | Step 800 | Loss: 8.9277\n",
      "Epoch 1 | Step 810 | Loss: 8.9456\n",
      "Epoch 1 | Step 820 | Loss: 9.1798\n",
      "Epoch 1 | Step 830 | Loss: 8.5216\n",
      "Epoch 1 | Step 840 | Loss: 6.9561\n",
      "Epoch 1 | Step 850 | Loss: 9.8701\n",
      "Epoch 1 | Step 860 | Loss: 7.5163\n",
      "Epoch 1 | Step 870 | Loss: 8.7377\n",
      "Epoch 1 | Step 880 | Loss: 8.6094\n",
      "Epoch 1 | Step 890 | Loss: 9.3407\n",
      "Epoch 1 | Step 900 | Loss: 10.0406\n",
      "Epoch 1 | Step 910 | Loss: 7.0662\n",
      "Epoch 1 | Step 920 | Loss: 9.2030\n",
      "Epoch 1 | Step 930 | Loss: 8.7050\n",
      "Epoch 1 | Step 940 | Loss: 8.8050\n",
      "Epoch 1 | Step 950 | Loss: 8.1838\n",
      "Epoch 1 | Step 960 | Loss: 9.1671\n",
      "Epoch 1 | Step 970 | Loss: 8.7202\n",
      "Epoch 1 | Step 980 | Loss: 9.3485\n",
      "Epoch 1 | Step 990 | Loss: 9.1636\n",
      "Epoch 1 | Step 1000 | Loss: 9.1455\n",
      "Epoch 1 | Step 1010 | Loss: 8.9910\n",
      "Epoch 1 | Step 1020 | Loss: 9.1466\n",
      "Epoch 1 | Step 1030 | Loss: 8.5925\n",
      "Epoch 1 | Step 1040 | Loss: 7.9131\n",
      "Epoch 1 | Step 1050 | Loss: 8.1271\n",
      "Epoch 1 | Step 1060 | Loss: 9.1004\n",
      "Epoch 1 | Step 1070 | Loss: 9.4912\n",
      "Epoch 1 | Step 1080 | Loss: 9.1000\n",
      "Epoch 1 | Step 1090 | Loss: 8.8104\n",
      "Epoch 1 | Step 1100 | Loss: 9.0492\n",
      "Epoch 1 | Step 1110 | Loss: 8.0967\n",
      "Epoch 1 | Step 1120 | Loss: 8.2120\n",
      "Epoch 1 | Step 1130 | Loss: 8.8721\n",
      "Epoch 1 | Step 1140 | Loss: 8.9009\n",
      "Epoch 1 | Step 1150 | Loss: 8.3898\n",
      "Epoch 1 | Step 1160 | Loss: 9.8953\n",
      "Epoch 1 | Step 1170 | Loss: 8.6692\n",
      "Epoch 1 | Step 1180 | Loss: 9.4081\n",
      "Epoch 1 | Step 1190 | Loss: 9.3047\n",
      "Epoch 1 | Step 1200 | Loss: 9.2618\n",
      "Epoch 1 | Step 1210 | Loss: 8.0349\n",
      "Epoch 1 | Step 1220 | Loss: 8.7494\n",
      "Epoch 1 | Step 1230 | Loss: 8.7963\n",
      "Epoch 1 | Step 1240 | Loss: 8.8968\n",
      "Epoch 1 | Step 1250 | Loss: 9.9861\n",
      "Epoch 1 | Step 1260 | Loss: 9.2622\n",
      "Epoch 1 | Step 1270 | Loss: 9.9838\n",
      "Epoch 1 | Step 1280 | Loss: 10.1085\n",
      "Epoch 1 | Step 1290 | Loss: 9.5526\n",
      "Epoch 1 | Step 1300 | Loss: 8.6500\n",
      "Epoch 1 | Step 1310 | Loss: 9.3824\n",
      "Epoch 1 | Step 1320 | Loss: 9.0790\n",
      "Epoch 1 | Step 1330 | Loss: 8.6159\n",
      "Epoch 1 | Step 1340 | Loss: 8.7138\n",
      "Epoch 1 | Step 1350 | Loss: 7.0642\n",
      "Epoch 1 | Step 1360 | Loss: 7.4503\n",
      "Epoch 1 | Step 1370 | Loss: 9.7996\n",
      "Epoch 1 | Step 1380 | Loss: 8.9779\n",
      "Epoch 1 | Step 1390 | Loss: 8.8155\n",
      "Epoch 1 | Step 1400 | Loss: 8.1478\n",
      "Epoch 1 | Step 1410 | Loss: 8.9039\n",
      "Epoch 1 | Step 1420 | Loss: 9.3446\n",
      "Epoch 1 | Step 1430 | Loss: 8.1896\n",
      "Epoch 1 | Step 1440 | Loss: 9.8717\n",
      "Epoch 1 | Step 1450 | Loss: 7.9000\n",
      "Epoch 1 | Step 1460 | Loss: 9.2914\n",
      "Epoch 1 | Step 1470 | Loss: 8.5085\n",
      "Epoch 1 | Step 1480 | Loss: 8.6368\n",
      "Epoch 1 | Step 1490 | Loss: 8.1568\n",
      "Epoch 1 | Step 1500 | Loss: 7.5285\n",
      "Epoch 1 | Step 1510 | Loss: 9.5280\n",
      "Epoch 1 | Step 1520 | Loss: 8.9435\n",
      "Epoch 1 | Step 1530 | Loss: 9.6008\n",
      "Epoch 1 | Step 1540 | Loss: 9.1495\n",
      "Epoch 1 | Step 1550 | Loss: 8.2054\n",
      "Epoch 1 | Step 1560 | Loss: 9.9460\n",
      "Epoch 1 | Step 1570 | Loss: 8.4105\n",
      "Epoch 1 | Step 1580 | Loss: 9.9830\n",
      "Epoch 1 | Step 1590 | Loss: 9.2203\n",
      "Epoch 1 | Step 1600 | Loss: 9.0419\n",
      "Epoch 1 | Step 1610 | Loss: 7.6131\n",
      "Epoch 1 | Step 1620 | Loss: 8.8860\n",
      "Epoch 1 | Step 1630 | Loss: 8.4118\n",
      "Epoch 1 | Step 1640 | Loss: 9.3306\n",
      "Epoch 1 | Step 1650 | Loss: 8.8723\n",
      "Epoch 1 | Step 1660 | Loss: 9.6500\n",
      "Epoch 1 | Step 1670 | Loss: 8.5147\n",
      "Epoch 1 | Step 1680 | Loss: 9.6352\n",
      "Epoch 1 | Step 1690 | Loss: 9.7666\n",
      "Epoch 1 | Step 1700 | Loss: 8.5171\n",
      "Epoch 1 | Step 1710 | Loss: 8.4245\n",
      "Epoch 1 | Step 1720 | Loss: 8.5847\n",
      "Epoch 1 | Step 1730 | Loss: 8.8731\n",
      "Epoch 1 | Step 1740 | Loss: 9.2549\n",
      "Epoch 1 | Step 1750 | Loss: 8.9852\n",
      "Epoch 1 | Step 1760 | Loss: 8.2792\n",
      "Epoch 1 | Step 1770 | Loss: 8.2732\n",
      "Epoch 1 | Step 1780 | Loss: 8.5225\n",
      "Epoch 1 | Step 1790 | Loss: 9.6261\n",
      "Epoch 1 | Step 1800 | Loss: 7.0012\n",
      "Epoch 1 | Step 1810 | Loss: 9.1760\n",
      "Epoch 1 | Step 1820 | Loss: 9.3804\n",
      "Epoch 1 | Step 1830 | Loss: 9.1314\n",
      "Epoch 1 | Step 1840 | Loss: 9.0429\n",
      "Epoch 1 | Step 1850 | Loss: 8.2747\n",
      "Epoch 1 | Step 1860 | Loss: 9.4800\n",
      "Epoch 1 | Step 1870 | Loss: 6.6796\n",
      "Epoch 1 | Step 1880 | Loss: 8.5105\n",
      "Epoch 1 | Step 1890 | Loss: 7.7853\n",
      "Epoch 1 | Step 1900 | Loss: 8.3463\n",
      "Epoch 1 | Step 1910 | Loss: 9.2732\n",
      "Epoch 1 | Step 1920 | Loss: 8.5029\n",
      "Epoch 1 | Step 1930 | Loss: 9.4501\n",
      "Epoch 1 | Step 1940 | Loss: 9.0922\n",
      "Epoch 1 | Step 1950 | Loss: 9.9270\n",
      "Epoch 1 | Step 1960 | Loss: 8.6415\n",
      "Epoch 1 | Step 1970 | Loss: 8.7606\n",
      "Epoch 1 | Step 1980 | Loss: 9.3298\n",
      "Epoch 1 | Step 1990 | Loss: 9.8967\n",
      "Epoch 1 | Step 2000 | Loss: 9.1403\n",
      "Epoch 1 | Step 2010 | Loss: 7.2362\n",
      "Epoch 1 | Step 2020 | Loss: 7.4350\n",
      "Epoch 1 | Step 2030 | Loss: 9.6629\n",
      "Epoch 1 | Step 2040 | Loss: 10.7972\n",
      "Epoch 1 | Step 2050 | Loss: 8.3780\n",
      "Epoch 1 | Step 2060 | Loss: 9.0725\n",
      "Epoch 1 | Step 2070 | Loss: 9.0124\n",
      "Epoch 1 | Step 2080 | Loss: 8.2322\n",
      "Epoch 1 | Step 2090 | Loss: 8.8989\n",
      "Epoch 1 | Step 2100 | Loss: 9.2183\n",
      "Epoch 1 | Step 2110 | Loss: 8.1690\n",
      "Epoch 1 | Step 2120 | Loss: 9.1320\n",
      "Epoch 1 | Step 2130 | Loss: 9.5162\n",
      "Epoch 1 | Step 2140 | Loss: 9.6212\n",
      "Epoch 1 | Step 2150 | Loss: 8.7193\n",
      "Epoch 1 | Step 2160 | Loss: 9.1111\n",
      "Epoch 1 | Step 2170 | Loss: 8.7893\n",
      "Epoch 1 | Step 2180 | Loss: 9.1246\n",
      "Epoch 1 | Step 2190 | Loss: 9.7375\n",
      "Epoch 1 | Step 2200 | Loss: 9.2323\n",
      "Epoch 1 | Step 2210 | Loss: 8.7084\n",
      "Epoch 1 | Step 2220 | Loss: 9.0908\n",
      "Epoch 1 | Step 2230 | Loss: 9.4202\n",
      "Epoch 1 | Step 2240 | Loss: 9.0042\n",
      "Epoch 1 | Step 2250 | Loss: 8.4522\n",
      "Epoch 1 | Step 2260 | Loss: 7.8007\n",
      "Epoch 1 | Step 2270 | Loss: 8.8886\n",
      "Epoch 1 | Step 2280 | Loss: 7.9769\n",
      "Epoch 1 | Step 2290 | Loss: 8.9138\n",
      "Epoch 1 | Step 2300 | Loss: 9.9620\n",
      "Epoch 1 | Step 2310 | Loss: 9.5332\n",
      "Epoch 1 | Step 2320 | Loss: 9.0855\n",
      "Epoch 1 | Step 2330 | Loss: 9.4541\n",
      "Epoch 1 | Step 2340 | Loss: 6.8699\n",
      "Epoch 1 | Step 2350 | Loss: 9.9368\n",
      "Epoch 1 | Step 2360 | Loss: 8.2467\n",
      "Epoch 1 | Step 2370 | Loss: 9.2752\n",
      "Epoch 1 | Step 2380 | Loss: 7.9786\n",
      "Epoch 1 | Step 2390 | Loss: 8.1943\n",
      "Epoch 1 | Step 2400 | Loss: 9.3971\n",
      "Epoch 1 | Step 2410 | Loss: 8.2681\n",
      "Epoch 1 | Step 2420 | Loss: 8.8649\n",
      "Epoch 1 | Step 2430 | Loss: 8.6781\n",
      "Epoch 1 | Step 2440 | Loss: 9.0727\n",
      "Epoch 1 | Step 2450 | Loss: 8.5623\n",
      "Epoch 1 | Step 2460 | Loss: 6.3274\n",
      "Epoch 1 | Step 2470 | Loss: 8.7445\n",
      "Epoch 1 | Step 2480 | Loss: 9.3438\n",
      "Epoch 1 | Step 2490 | Loss: 7.1263\n",
      "Epoch 1 | Step 2500 | Loss: 8.5334\n",
      "Epoch 1 | Step 2510 | Loss: 9.5123\n",
      "Epoch 1 | Step 2520 | Loss: 8.2599\n",
      "Epoch 1 | Step 2530 | Loss: 8.9207\n",
      "Epoch 1 | Step 2540 | Loss: 10.5450\n",
      "Epoch 1 | Step 2550 | Loss: 8.6791\n",
      "Epoch 2 | Step 0 | Loss: 9.1358\n",
      "Epoch 2 | Step 10 | Loss: 9.3101\n",
      "Epoch 2 | Step 20 | Loss: 8.6729\n",
      "Epoch 2 | Step 30 | Loss: 8.7405\n",
      "Epoch 2 | Step 40 | Loss: 8.6126\n",
      "Epoch 2 | Step 50 | Loss: 9.0364\n",
      "Epoch 2 | Step 60 | Loss: 9.2865\n",
      "Epoch 2 | Step 70 | Loss: 8.9732\n",
      "Epoch 2 | Step 80 | Loss: 9.5073\n",
      "Epoch 2 | Step 90 | Loss: 8.7661\n",
      "Epoch 2 | Step 100 | Loss: 9.1600\n",
      "Epoch 2 | Step 110 | Loss: 8.4698\n",
      "Epoch 2 | Step 120 | Loss: 9.2014\n",
      "Epoch 2 | Step 130 | Loss: 9.0673\n",
      "Epoch 2 | Step 140 | Loss: 9.1181\n",
      "Epoch 2 | Step 150 | Loss: 9.2584\n",
      "Epoch 2 | Step 160 | Loss: 8.2464\n",
      "Epoch 2 | Step 170 | Loss: 8.7376\n",
      "Epoch 2 | Step 180 | Loss: 8.7688\n",
      "Epoch 2 | Step 190 | Loss: 8.3642\n",
      "Epoch 2 | Step 200 | Loss: 8.1674\n",
      "Epoch 2 | Step 210 | Loss: 8.3093\n",
      "Epoch 2 | Step 220 | Loss: 9.3870\n",
      "Epoch 2 | Step 230 | Loss: 9.5158\n",
      "Epoch 2 | Step 240 | Loss: 6.9881\n",
      "Epoch 2 | Step 250 | Loss: 8.5853\n",
      "Epoch 2 | Step 260 | Loss: 6.9535\n",
      "Epoch 2 | Step 270 | Loss: 9.3312\n",
      "Epoch 2 | Step 280 | Loss: 8.3072\n",
      "Epoch 2 | Step 290 | Loss: 8.7506\n",
      "Epoch 2 | Step 300 | Loss: 8.7600\n",
      "Epoch 2 | Step 310 | Loss: 9.5581\n",
      "Epoch 2 | Step 320 | Loss: 8.6531\n",
      "Epoch 2 | Step 330 | Loss: 9.6256\n",
      "Epoch 2 | Step 340 | Loss: 8.8797\n",
      "Epoch 2 | Step 350 | Loss: 7.3647\n",
      "Epoch 2 | Step 360 | Loss: 8.5796\n",
      "Epoch 2 | Step 370 | Loss: 7.6998\n",
      "Epoch 2 | Step 380 | Loss: 7.8003\n",
      "Epoch 2 | Step 390 | Loss: 9.2147\n",
      "Epoch 2 | Step 400 | Loss: 9.0772\n",
      "Epoch 2 | Step 410 | Loss: 9.4047\n",
      "Epoch 2 | Step 420 | Loss: 8.8202\n",
      "Epoch 2 | Step 430 | Loss: 8.9368\n",
      "Epoch 2 | Step 440 | Loss: 9.2266\n",
      "Epoch 2 | Step 450 | Loss: 9.6750\n",
      "Epoch 2 | Step 460 | Loss: 8.5345\n",
      "Epoch 2 | Step 470 | Loss: 8.7972\n",
      "Epoch 2 | Step 480 | Loss: 8.9231\n",
      "Epoch 2 | Step 490 | Loss: 8.6374\n",
      "Epoch 2 | Step 500 | Loss: 8.8000\n",
      "Epoch 2 | Step 510 | Loss: 9.3661\n",
      "Epoch 2 | Step 520 | Loss: 8.7178\n",
      "Epoch 2 | Step 530 | Loss: 9.7663\n",
      "Epoch 2 | Step 540 | Loss: 6.5159\n",
      "Epoch 2 | Step 550 | Loss: 8.3508\n",
      "Epoch 2 | Step 560 | Loss: 10.0439\n",
      "Epoch 2 | Step 570 | Loss: 9.0998\n",
      "Epoch 2 | Step 580 | Loss: 9.1486\n",
      "Epoch 2 | Step 590 | Loss: 9.2668\n",
      "Epoch 2 | Step 600 | Loss: 9.2526\n",
      "Epoch 2 | Step 610 | Loss: 6.4999\n",
      "Epoch 2 | Step 620 | Loss: 8.9069\n",
      "Epoch 2 | Step 630 | Loss: 9.3941\n",
      "Epoch 2 | Step 640 | Loss: 8.6745\n",
      "Epoch 2 | Step 650 | Loss: 8.2801\n",
      "Epoch 2 | Step 660 | Loss: 6.9665\n",
      "Epoch 2 | Step 670 | Loss: 8.5640\n",
      "Epoch 2 | Step 680 | Loss: 6.0654\n",
      "Epoch 2 | Step 690 | Loss: 9.1531\n",
      "Epoch 2 | Step 700 | Loss: 8.2275\n",
      "Epoch 2 | Step 710 | Loss: 9.4196\n",
      "Epoch 2 | Step 720 | Loss: 8.9887\n",
      "Epoch 2 | Step 730 | Loss: 9.0748\n",
      "Epoch 2 | Step 740 | Loss: 9.3940\n",
      "Epoch 2 | Step 750 | Loss: 9.1028\n",
      "Epoch 2 | Step 760 | Loss: 7.7288\n",
      "Epoch 2 | Step 770 | Loss: 9.3168\n",
      "Epoch 2 | Step 780 | Loss: 9.0838\n",
      "Epoch 2 | Step 790 | Loss: 7.1796\n",
      "Epoch 2 | Step 800 | Loss: 9.4034\n",
      "Epoch 2 | Step 810 | Loss: 9.6343\n",
      "Epoch 2 | Step 820 | Loss: 7.7226\n",
      "Epoch 2 | Step 830 | Loss: 9.1390\n",
      "Epoch 2 | Step 840 | Loss: 9.3080\n",
      "Epoch 2 | Step 850 | Loss: 8.4411\n",
      "Epoch 2 | Step 860 | Loss: 8.7563\n",
      "Epoch 2 | Step 870 | Loss: 9.3319\n",
      "Epoch 2 | Step 880 | Loss: 7.6521\n",
      "Epoch 2 | Step 890 | Loss: 9.4246\n",
      "Epoch 2 | Step 900 | Loss: 8.4904\n",
      "Epoch 2 | Step 910 | Loss: 8.9144\n",
      "Epoch 2 | Step 920 | Loss: 9.0315\n",
      "Epoch 2 | Step 930 | Loss: 9.5258\n",
      "Epoch 2 | Step 940 | Loss: 8.7182\n",
      "Epoch 2 | Step 950 | Loss: 8.5142\n",
      "Epoch 2 | Step 960 | Loss: 9.2164\n",
      "Epoch 2 | Step 970 | Loss: 8.9957\n",
      "Epoch 2 | Step 980 | Loss: 6.8278\n",
      "Epoch 2 | Step 990 | Loss: 9.2832\n",
      "Epoch 2 | Step 1000 | Loss: 7.3580\n",
      "Epoch 2 | Step 1010 | Loss: 8.4510\n",
      "Epoch 2 | Step 1020 | Loss: 9.0041\n",
      "Epoch 2 | Step 1030 | Loss: 7.6607\n",
      "Epoch 2 | Step 1040 | Loss: 9.0088\n",
      "Epoch 2 | Step 1050 | Loss: 9.0733\n",
      "Epoch 2 | Step 1060 | Loss: 8.4230\n",
      "Epoch 2 | Step 1070 | Loss: 7.9987\n",
      "Epoch 2 | Step 1080 | Loss: 10.0305\n",
      "Epoch 2 | Step 1090 | Loss: 9.0202\n",
      "Epoch 2 | Step 1100 | Loss: 10.2305\n",
      "Epoch 2 | Step 1110 | Loss: 9.6174\n",
      "Epoch 2 | Step 1120 | Loss: 9.3304\n",
      "Epoch 2 | Step 1130 | Loss: 9.0881\n",
      "Epoch 2 | Step 1140 | Loss: 8.9220\n",
      "Epoch 2 | Step 1150 | Loss: 8.9410\n",
      "Epoch 2 | Step 1160 | Loss: 8.4389\n",
      "Epoch 2 | Step 1170 | Loss: 7.3476\n",
      "Epoch 2 | Step 1180 | Loss: 8.7152\n",
      "Epoch 2 | Step 1190 | Loss: 9.0288\n",
      "Epoch 2 | Step 1200 | Loss: 8.6567\n",
      "Epoch 2 | Step 1210 | Loss: 7.3706\n",
      "Epoch 2 | Step 1220 | Loss: 9.1115\n",
      "Epoch 2 | Step 1230 | Loss: 9.2625\n",
      "Epoch 2 | Step 1240 | Loss: 8.2530\n",
      "Epoch 2 | Step 1250 | Loss: 10.1088\n",
      "Epoch 2 | Step 1260 | Loss: 9.1818\n",
      "Epoch 2 | Step 1270 | Loss: 7.6767\n",
      "Epoch 2 | Step 1280 | Loss: 7.4368\n",
      "Epoch 2 | Step 1290 | Loss: 9.6389\n",
      "Epoch 2 | Step 1300 | Loss: 8.9319\n",
      "Epoch 2 | Step 1310 | Loss: 8.6525\n",
      "Epoch 2 | Step 1320 | Loss: 8.8517\n",
      "Epoch 2 | Step 1330 | Loss: 8.7451\n",
      "Epoch 2 | Step 1340 | Loss: 9.2227\n",
      "Epoch 2 | Step 1350 | Loss: 8.9227\n",
      "Epoch 2 | Step 1360 | Loss: 8.0814\n",
      "Epoch 2 | Step 1370 | Loss: 9.1984\n",
      "Epoch 2 | Step 1380 | Loss: 8.4649\n",
      "Epoch 2 | Step 1390 | Loss: 8.9392\n",
      "Epoch 2 | Step 1400 | Loss: 9.0577\n",
      "Epoch 2 | Step 1410 | Loss: 9.3329\n",
      "Epoch 2 | Step 1420 | Loss: 6.4893\n",
      "Epoch 2 | Step 1430 | Loss: 9.1619\n",
      "Epoch 2 | Step 1440 | Loss: 8.6755\n",
      "Epoch 2 | Step 1450 | Loss: 7.7824\n",
      "Epoch 2 | Step 1460 | Loss: 9.0306\n",
      "Epoch 2 | Step 1470 | Loss: 7.2309\n",
      "Epoch 2 | Step 1480 | Loss: 7.9031\n",
      "Epoch 2 | Step 1490 | Loss: 9.3020\n",
      "Epoch 2 | Step 1500 | Loss: 7.9909\n",
      "Epoch 2 | Step 1510 | Loss: 9.3852\n",
      "Epoch 2 | Step 1520 | Loss: 9.1690\n",
      "Epoch 2 | Step 1530 | Loss: 8.6211\n",
      "Epoch 2 | Step 1540 | Loss: 8.0718\n",
      "Epoch 2 | Step 1550 | Loss: 9.0917\n",
      "Epoch 2 | Step 1560 | Loss: 9.2341\n",
      "Epoch 2 | Step 1570 | Loss: 9.8455\n",
      "Epoch 2 | Step 1580 | Loss: 9.1452\n",
      "Epoch 2 | Step 1590 | Loss: 9.5342\n",
      "Epoch 2 | Step 1600 | Loss: 8.3613\n",
      "Epoch 2 | Step 1610 | Loss: 8.9061\n",
      "Epoch 2 | Step 1620 | Loss: 9.1032\n",
      "Epoch 2 | Step 1630 | Loss: 9.6028\n",
      "Epoch 2 | Step 1640 | Loss: 7.1627\n",
      "Epoch 2 | Step 1650 | Loss: 8.9977\n",
      "Epoch 2 | Step 1660 | Loss: 9.1251\n",
      "Epoch 2 | Step 1670 | Loss: 8.2699\n",
      "Epoch 2 | Step 1680 | Loss: 9.3882\n",
      "Epoch 2 | Step 1690 | Loss: 7.7344\n",
      "Epoch 2 | Step 1700 | Loss: 8.0229\n",
      "Epoch 2 | Step 1710 | Loss: 8.6956\n",
      "Epoch 2 | Step 1720 | Loss: 8.3766\n",
      "Epoch 2 | Step 1730 | Loss: 7.5973\n",
      "Epoch 2 | Step 1740 | Loss: 9.1220\n",
      "Epoch 2 | Step 1750 | Loss: 9.2072\n",
      "Epoch 2 | Step 1760 | Loss: 7.4976\n",
      "Epoch 2 | Step 1770 | Loss: 9.2864\n",
      "Epoch 2 | Step 1780 | Loss: 8.4468\n",
      "Epoch 2 | Step 1790 | Loss: 6.8260\n",
      "Epoch 2 | Step 1800 | Loss: 8.5212\n",
      "Epoch 2 | Step 1810 | Loss: 8.2322\n",
      "Epoch 2 | Step 1820 | Loss: 8.7958\n",
      "Epoch 2 | Step 1830 | Loss: 8.9798\n",
      "Epoch 2 | Step 1840 | Loss: 9.2520\n",
      "Epoch 2 | Step 1850 | Loss: 8.2714\n",
      "Epoch 2 | Step 1860 | Loss: 9.1825\n",
      "Epoch 2 | Step 1870 | Loss: 8.2447\n",
      "Epoch 2 | Step 1880 | Loss: 9.0234\n",
      "Epoch 2 | Step 1890 | Loss: 8.0462\n",
      "Epoch 2 | Step 1900 | Loss: 9.7939\n",
      "Epoch 2 | Step 1910 | Loss: 9.7253\n",
      "Epoch 2 | Step 1920 | Loss: 7.6923\n",
      "Epoch 2 | Step 1930 | Loss: 8.3895\n",
      "Epoch 2 | Step 1940 | Loss: 9.2401\n",
      "Epoch 2 | Step 1950 | Loss: 9.6770\n",
      "Epoch 2 | Step 1960 | Loss: 8.3984\n",
      "Epoch 2 | Step 1970 | Loss: 6.9011\n",
      "Epoch 2 | Step 1980 | Loss: 9.2779\n",
      "Epoch 2 | Step 1990 | Loss: 9.2235\n",
      "Epoch 2 | Step 2000 | Loss: 8.9163\n",
      "Epoch 2 | Step 2010 | Loss: 8.2200\n",
      "Epoch 2 | Step 2020 | Loss: 9.0869\n",
      "Epoch 2 | Step 2030 | Loss: 8.5474\n",
      "Epoch 2 | Step 2040 | Loss: 9.0596\n",
      "Epoch 2 | Step 2050 | Loss: 9.1451\n",
      "Epoch 2 | Step 2060 | Loss: 8.0727\n",
      "Epoch 2 | Step 2070 | Loss: 8.6955\n",
      "Epoch 2 | Step 2080 | Loss: 9.3663\n",
      "Epoch 2 | Step 2090 | Loss: 8.2267\n",
      "Epoch 2 | Step 2100 | Loss: 8.1656\n",
      "Epoch 2 | Step 2110 | Loss: 9.3836\n",
      "Epoch 2 | Step 2120 | Loss: 8.7983\n",
      "Epoch 2 | Step 2130 | Loss: 7.5946\n",
      "Epoch 2 | Step 2140 | Loss: 9.5049\n",
      "Epoch 2 | Step 2150 | Loss: 9.0628\n",
      "Epoch 2 | Step 2160 | Loss: 9.0280\n",
      "Epoch 2 | Step 2170 | Loss: 8.5182\n",
      "Epoch 2 | Step 2180 | Loss: 9.2345\n",
      "Epoch 2 | Step 2190 | Loss: 9.3191\n",
      "Epoch 2 | Step 2200 | Loss: 7.8603\n",
      "Epoch 2 | Step 2210 | Loss: 8.1075\n",
      "Epoch 2 | Step 2220 | Loss: 8.6944\n",
      "Epoch 2 | Step 2230 | Loss: 9.1013\n",
      "Epoch 2 | Step 2240 | Loss: 8.7999\n",
      "Epoch 2 | Step 2250 | Loss: 9.3951\n",
      "Epoch 2 | Step 2260 | Loss: 6.4571\n",
      "Epoch 2 | Step 2270 | Loss: 8.6773\n",
      "Epoch 2 | Step 2280 | Loss: 9.0711\n",
      "Epoch 2 | Step 2290 | Loss: 8.9076\n",
      "Epoch 2 | Step 2300 | Loss: 9.6154\n",
      "Epoch 2 | Step 2310 | Loss: 8.7480\n",
      "Epoch 2 | Step 2320 | Loss: 9.2141\n",
      "Epoch 2 | Step 2330 | Loss: 9.0067\n",
      "Epoch 2 | Step 2340 | Loss: 8.3100\n",
      "Epoch 2 | Step 2350 | Loss: 9.1794\n",
      "Epoch 2 | Step 2360 | Loss: 8.6958\n",
      "Epoch 2 | Step 2370 | Loss: 9.1574\n",
      "Epoch 2 | Step 2380 | Loss: 7.9789\n",
      "Epoch 2 | Step 2390 | Loss: 8.7082\n",
      "Epoch 2 | Step 2400 | Loss: 8.4222\n",
      "Epoch 2 | Step 2410 | Loss: 8.7881\n",
      "Epoch 2 | Step 2420 | Loss: 8.7577\n",
      "Epoch 2 | Step 2430 | Loss: 9.2681\n",
      "Epoch 2 | Step 2440 | Loss: 8.9660\n",
      "Epoch 2 | Step 2450 | Loss: 8.8185\n",
      "Epoch 2 | Step 2460 | Loss: 8.6621\n",
      "Epoch 2 | Step 2470 | Loss: 8.4834\n",
      "Epoch 2 | Step 2480 | Loss: 9.1373\n",
      "Epoch 2 | Step 2490 | Loss: 9.8917\n",
      "Epoch 2 | Step 2500 | Loss: 9.2653\n",
      "Epoch 2 | Step 2510 | Loss: 9.5037\n",
      "Epoch 2 | Step 2520 | Loss: 8.9125\n",
      "Epoch 2 | Step 2530 | Loss: 8.9259\n",
      "Epoch 2 | Step 2540 | Loss: 9.3989\n",
      "Epoch 2 | Step 2550 | Loss: 8.3708\n",
      "Epoch 3 | Step 0 | Loss: 8.5307\n",
      "Epoch 3 | Step 10 | Loss: 9.0197\n",
      "Epoch 3 | Step 20 | Loss: 8.7037\n",
      "Epoch 3 | Step 30 | Loss: 7.4897\n",
      "Epoch 3 | Step 40 | Loss: 8.6688\n",
      "Epoch 3 | Step 50 | Loss: 9.0195\n",
      "Epoch 3 | Step 60 | Loss: 9.0735\n",
      "Epoch 3 | Step 70 | Loss: 7.6637\n",
      "Epoch 3 | Step 80 | Loss: 8.4254\n",
      "Epoch 3 | Step 90 | Loss: 8.1951\n",
      "Epoch 3 | Step 100 | Loss: 6.8615\n",
      "Epoch 3 | Step 110 | Loss: 8.8743\n",
      "Epoch 3 | Step 120 | Loss: 8.4063\n",
      "Epoch 3 | Step 130 | Loss: 8.7188\n",
      "Epoch 3 | Step 140 | Loss: 9.2086\n",
      "Epoch 3 | Step 150 | Loss: 10.7040\n",
      "Epoch 3 | Step 160 | Loss: 8.6688\n",
      "Epoch 3 | Step 170 | Loss: 8.8204\n",
      "Epoch 3 | Step 180 | Loss: 6.9769\n",
      "Epoch 3 | Step 190 | Loss: 7.8006\n",
      "Epoch 3 | Step 200 | Loss: 7.5363\n",
      "Epoch 3 | Step 210 | Loss: 8.6103\n",
      "Epoch 3 | Step 220 | Loss: 8.1580\n",
      "Epoch 3 | Step 230 | Loss: 9.2101\n",
      "Epoch 3 | Step 240 | Loss: 9.3096\n",
      "Epoch 3 | Step 250 | Loss: 8.6128\n",
      "Epoch 3 | Step 260 | Loss: 9.2332\n",
      "Epoch 3 | Step 270 | Loss: 9.2032\n",
      "Epoch 3 | Step 280 | Loss: 9.3510\n",
      "Epoch 3 | Step 290 | Loss: 9.1415\n",
      "Epoch 3 | Step 300 | Loss: 9.1369\n",
      "Epoch 3 | Step 310 | Loss: 6.7092\n",
      "Epoch 3 | Step 320 | Loss: 9.8711\n",
      "Epoch 3 | Step 330 | Loss: 9.4103\n",
      "Epoch 3 | Step 340 | Loss: 8.3073\n",
      "Epoch 3 | Step 350 | Loss: 8.9087\n",
      "Epoch 3 | Step 360 | Loss: 9.6496\n",
      "Epoch 3 | Step 370 | Loss: 9.0624\n",
      "Epoch 3 | Step 380 | Loss: 8.5598\n",
      "Epoch 3 | Step 390 | Loss: 9.3678\n",
      "Epoch 3 | Step 400 | Loss: 9.3025\n",
      "Epoch 3 | Step 410 | Loss: 6.7321\n",
      "Epoch 3 | Step 420 | Loss: 9.4621\n",
      "Epoch 3 | Step 430 | Loss: 8.2607\n",
      "Epoch 3 | Step 440 | Loss: 8.9647\n",
      "Epoch 3 | Step 450 | Loss: 9.2744\n",
      "Epoch 3 | Step 460 | Loss: 8.2389\n",
      "Epoch 3 | Step 470 | Loss: 7.6975\n",
      "Epoch 3 | Step 480 | Loss: 9.1774\n",
      "Epoch 3 | Step 490 | Loss: 9.1761\n",
      "Epoch 3 | Step 500 | Loss: 7.5492\n",
      "Epoch 3 | Step 510 | Loss: 8.7681\n",
      "Epoch 3 | Step 520 | Loss: 8.8698\n",
      "Epoch 3 | Step 530 | Loss: 9.4244\n",
      "Epoch 3 | Step 540 | Loss: 8.7483\n",
      "Epoch 3 | Step 550 | Loss: 8.3471\n",
      "Epoch 3 | Step 560 | Loss: 7.8214\n",
      "Epoch 3 | Step 570 | Loss: 8.5700\n",
      "Epoch 3 | Step 580 | Loss: 9.3664\n",
      "Epoch 3 | Step 590 | Loss: 9.2180\n",
      "Epoch 3 | Step 600 | Loss: 7.0913\n",
      "Epoch 3 | Step 610 | Loss: 8.7442\n",
      "Epoch 3 | Step 620 | Loss: 9.6417\n",
      "Epoch 3 | Step 630 | Loss: 8.0915\n",
      "Epoch 3 | Step 640 | Loss: 8.9616\n",
      "Epoch 3 | Step 650 | Loss: 8.7613\n",
      "Epoch 3 | Step 660 | Loss: 8.3850\n",
      "Epoch 3 | Step 670 | Loss: 8.6710\n",
      "Epoch 3 | Step 680 | Loss: 9.3586\n",
      "Epoch 3 | Step 690 | Loss: 7.8753\n",
      "Epoch 3 | Step 700 | Loss: 7.9725\n",
      "Epoch 3 | Step 710 | Loss: 8.9916\n",
      "Epoch 3 | Step 720 | Loss: 8.5271\n",
      "Epoch 3 | Step 730 | Loss: 8.8834\n",
      "Epoch 3 | Step 740 | Loss: 9.1468\n",
      "Epoch 3 | Step 750 | Loss: 9.6277\n",
      "Epoch 3 | Step 760 | Loss: 8.9489\n",
      "Epoch 3 | Step 770 | Loss: 8.5842\n",
      "Epoch 3 | Step 780 | Loss: 7.9942\n",
      "Epoch 3 | Step 790 | Loss: 7.6607\n",
      "Epoch 3 | Step 800 | Loss: 9.5106\n",
      "Epoch 3 | Step 810 | Loss: 8.7154\n",
      "Epoch 3 | Step 820 | Loss: 9.9253\n",
      "Epoch 3 | Step 830 | Loss: 8.8927\n",
      "Epoch 3 | Step 840 | Loss: 9.0429\n",
      "Epoch 3 | Step 850 | Loss: 7.2666\n",
      "Epoch 3 | Step 860 | Loss: 9.4304\n",
      "Epoch 3 | Step 870 | Loss: 8.7727\n",
      "Epoch 3 | Step 880 | Loss: 8.5480\n",
      "Epoch 3 | Step 890 | Loss: 8.7673\n",
      "Epoch 3 | Step 900 | Loss: 7.2873\n",
      "Epoch 3 | Step 910 | Loss: 9.7598\n",
      "Epoch 3 | Step 920 | Loss: 8.5570\n",
      "Epoch 3 | Step 930 | Loss: 9.6412\n",
      "Epoch 3 | Step 940 | Loss: 9.1492\n",
      "Epoch 3 | Step 950 | Loss: 8.8323\n",
      "Epoch 3 | Step 960 | Loss: 8.9778\n",
      "Epoch 3 | Step 970 | Loss: 9.1166\n",
      "Epoch 3 | Step 980 | Loss: 8.1863\n",
      "Epoch 3 | Step 990 | Loss: 8.1858\n",
      "Epoch 3 | Step 1000 | Loss: 8.9647\n",
      "Epoch 3 | Step 1010 | Loss: 9.2907\n",
      "Epoch 3 | Step 1020 | Loss: 8.9082\n",
      "Epoch 3 | Step 1030 | Loss: 7.6651\n",
      "Epoch 3 | Step 1040 | Loss: 7.5794\n",
      "Epoch 3 | Step 1050 | Loss: 9.5118\n",
      "Epoch 3 | Step 1060 | Loss: 9.2811\n",
      "Epoch 3 | Step 1070 | Loss: 8.8526\n",
      "Epoch 3 | Step 1080 | Loss: 8.9336\n",
      "Epoch 3 | Step 1090 | Loss: 9.4176\n",
      "Epoch 3 | Step 1100 | Loss: 7.7213\n",
      "Epoch 3 | Step 1110 | Loss: 8.9886\n",
      "Epoch 3 | Step 1120 | Loss: 9.2081\n",
      "Epoch 3 | Step 1130 | Loss: 8.8183\n",
      "Epoch 3 | Step 1140 | Loss: 9.7201\n",
      "Epoch 3 | Step 1150 | Loss: 7.2846\n",
      "Epoch 3 | Step 1160 | Loss: 8.0330\n",
      "Epoch 3 | Step 1170 | Loss: 9.2970\n",
      "Epoch 3 | Step 1180 | Loss: 8.5484\n",
      "Epoch 3 | Step 1190 | Loss: 7.6772\n",
      "Epoch 3 | Step 1200 | Loss: 8.7976\n",
      "Epoch 3 | Step 1210 | Loss: 7.3781\n",
      "Epoch 3 | Step 1220 | Loss: 9.4515\n",
      "Epoch 3 | Step 1230 | Loss: 8.9392\n",
      "Epoch 3 | Step 1240 | Loss: 8.8996\n",
      "Epoch 3 | Step 1250 | Loss: 8.7971\n",
      "Epoch 3 | Step 1260 | Loss: 7.9063\n",
      "Epoch 3 | Step 1270 | Loss: 9.0586\n",
      "Epoch 3 | Step 1280 | Loss: 9.0521\n",
      "Epoch 3 | Step 1290 | Loss: 8.9908\n",
      "Epoch 3 | Step 1300 | Loss: 9.7767\n",
      "Epoch 3 | Step 1310 | Loss: 8.3697\n",
      "Epoch 3 | Step 1320 | Loss: 8.9584\n",
      "Epoch 3 | Step 1330 | Loss: 8.2042\n",
      "Epoch 3 | Step 1340 | Loss: 8.6916\n",
      "Epoch 3 | Step 1350 | Loss: 9.3718\n",
      "Epoch 3 | Step 1360 | Loss: 6.7804\n",
      "Epoch 3 | Step 1370 | Loss: 8.7956\n",
      "Epoch 3 | Step 1380 | Loss: 8.3533\n",
      "Epoch 3 | Step 1390 | Loss: 8.7871\n",
      "Epoch 3 | Step 1400 | Loss: 8.7559\n",
      "Epoch 3 | Step 1410 | Loss: 8.5100\n",
      "Epoch 3 | Step 1420 | Loss: 8.3162\n",
      "Epoch 3 | Step 1430 | Loss: 9.1836\n",
      "Epoch 3 | Step 1440 | Loss: 9.4628\n",
      "Epoch 3 | Step 1450 | Loss: 7.5075\n",
      "Epoch 3 | Step 1460 | Loss: 8.2552\n",
      "Epoch 3 | Step 1470 | Loss: 9.2564\n",
      "Epoch 3 | Step 1480 | Loss: 9.0680\n",
      "Epoch 3 | Step 1490 | Loss: 9.0155\n",
      "Epoch 3 | Step 1500 | Loss: 8.6722\n",
      "Epoch 3 | Step 1510 | Loss: 9.2713\n",
      "Epoch 3 | Step 1520 | Loss: 7.1464\n",
      "Epoch 3 | Step 1530 | Loss: 8.9091\n",
      "Epoch 3 | Step 1540 | Loss: 8.5819\n",
      "Epoch 3 | Step 1550 | Loss: 8.4600\n",
      "Epoch 3 | Step 1560 | Loss: 7.8741\n",
      "Epoch 3 | Step 1570 | Loss: 9.3639\n",
      "Epoch 3 | Step 1580 | Loss: 9.0996\n",
      "Epoch 3 | Step 1590 | Loss: 8.7448\n",
      "Epoch 3 | Step 1600 | Loss: 8.4926\n",
      "Epoch 3 | Step 1610 | Loss: 9.0582\n",
      "Epoch 3 | Step 1620 | Loss: 10.0639\n",
      "Epoch 3 | Step 1630 | Loss: 8.8148\n",
      "Epoch 3 | Step 1640 | Loss: 9.8363\n",
      "Epoch 3 | Step 1650 | Loss: 8.7524\n",
      "Epoch 3 | Step 1660 | Loss: 9.4145\n",
      "Epoch 3 | Step 1670 | Loss: 8.6821\n",
      "Epoch 3 | Step 1680 | Loss: 8.8520\n",
      "Epoch 3 | Step 1690 | Loss: 8.3626\n",
      "Epoch 3 | Step 1700 | Loss: 9.2274\n",
      "Epoch 3 | Step 1710 | Loss: 7.4582\n",
      "Epoch 3 | Step 1720 | Loss: 7.5573\n",
      "Epoch 3 | Step 1730 | Loss: 8.8048\n",
      "Epoch 3 | Step 1740 | Loss: 8.4954\n",
      "Epoch 3 | Step 1750 | Loss: 7.0487\n",
      "Epoch 3 | Step 1760 | Loss: 8.8917\n",
      "Epoch 3 | Step 1770 | Loss: 8.3816\n",
      "Epoch 3 | Step 1780 | Loss: 8.4130\n",
      "Epoch 3 | Step 1790 | Loss: 8.2930\n",
      "Epoch 3 | Step 1800 | Loss: 9.1341\n",
      "Epoch 3 | Step 1810 | Loss: 8.1513\n",
      "Epoch 3 | Step 1820 | Loss: 8.6611\n",
      "Epoch 3 | Step 1830 | Loss: 7.2340\n",
      "Epoch 3 | Step 1840 | Loss: 9.9253\n",
      "Epoch 3 | Step 1850 | Loss: 8.2874\n",
      "Epoch 3 | Step 1860 | Loss: 8.5219\n",
      "Epoch 3 | Step 1870 | Loss: 8.9858\n",
      "Epoch 3 | Step 1880 | Loss: 9.4440\n",
      "Epoch 3 | Step 1890 | Loss: 8.6690\n",
      "Epoch 3 | Step 1900 | Loss: 8.8939\n",
      "Epoch 3 | Step 1910 | Loss: 8.9449\n",
      "Epoch 3 | Step 1920 | Loss: 8.5301\n",
      "Epoch 3 | Step 1930 | Loss: 9.0631\n",
      "Epoch 3 | Step 1940 | Loss: 8.4635\n",
      "Epoch 3 | Step 1950 | Loss: 8.1504\n",
      "Epoch 3 | Step 1960 | Loss: 8.2838\n",
      "Epoch 3 | Step 1970 | Loss: 9.0889\n",
      "Epoch 3 | Step 1980 | Loss: 7.6529\n",
      "Epoch 3 | Step 1990 | Loss: 9.2254\n",
      "Epoch 3 | Step 2000 | Loss: 9.3434\n",
      "Epoch 3 | Step 2010 | Loss: 8.3575\n",
      "Epoch 3 | Step 2020 | Loss: 8.9507\n",
      "Epoch 3 | Step 2030 | Loss: 9.1825\n",
      "Epoch 3 | Step 2040 | Loss: 8.3942\n",
      "Epoch 3 | Step 2050 | Loss: 8.6436\n",
      "Epoch 3 | Step 2060 | Loss: 8.9100\n",
      "Epoch 3 | Step 2070 | Loss: 8.9102\n",
      "Epoch 3 | Step 2080 | Loss: 8.3179\n",
      "Epoch 3 | Step 2090 | Loss: 9.2927\n",
      "Epoch 3 | Step 2100 | Loss: 9.6045\n",
      "Epoch 3 | Step 2110 | Loss: 9.0533\n",
      "Epoch 3 | Step 2120 | Loss: 8.8240\n",
      "Epoch 3 | Step 2130 | Loss: 9.1515\n",
      "Epoch 3 | Step 2140 | Loss: 7.8941\n",
      "Epoch 3 | Step 2150 | Loss: 9.6430\n",
      "Epoch 3 | Step 2160 | Loss: 9.4153\n",
      "Epoch 3 | Step 2170 | Loss: 8.8667\n",
      "Epoch 3 | Step 2180 | Loss: 9.1500\n",
      "Epoch 3 | Step 2190 | Loss: 7.4619\n",
      "Epoch 3 | Step 2200 | Loss: 9.0411\n",
      "Epoch 3 | Step 2210 | Loss: 8.5511\n",
      "Epoch 3 | Step 2220 | Loss: 9.0616\n",
      "Epoch 3 | Step 2230 | Loss: 8.6849\n",
      "Epoch 3 | Step 2240 | Loss: 9.2303\n",
      "Epoch 3 | Step 2250 | Loss: 8.2581\n",
      "Epoch 3 | Step 2260 | Loss: 9.2278\n",
      "Epoch 3 | Step 2270 | Loss: 8.4620\n",
      "Epoch 3 | Step 2280 | Loss: 9.0539\n",
      "Epoch 3 | Step 2290 | Loss: 9.2649\n",
      "Epoch 3 | Step 2300 | Loss: 8.9226\n",
      "Epoch 3 | Step 2310 | Loss: 7.4932\n",
      "Epoch 3 | Step 2320 | Loss: 7.5952\n",
      "Epoch 3 | Step 2330 | Loss: 8.7719\n",
      "Epoch 3 | Step 2340 | Loss: 9.1231\n",
      "Epoch 3 | Step 2350 | Loss: 8.4063\n",
      "Epoch 3 | Step 2360 | Loss: 9.1822\n",
      "Epoch 3 | Step 2370 | Loss: 8.5718\n",
      "Epoch 3 | Step 2380 | Loss: 8.4343\n",
      "Epoch 3 | Step 2390 | Loss: 8.7306\n",
      "Epoch 3 | Step 2400 | Loss: 9.4105\n",
      "Epoch 3 | Step 2410 | Loss: 6.6899\n",
      "Epoch 3 | Step 2420 | Loss: 9.5893\n",
      "Epoch 3 | Step 2430 | Loss: 8.9016\n",
      "Epoch 3 | Step 2440 | Loss: 9.0308\n",
      "Epoch 3 | Step 2450 | Loss: 7.5024\n",
      "Epoch 3 | Step 2460 | Loss: 8.5879\n",
      "Epoch 3 | Step 2470 | Loss: 8.6885\n",
      "Epoch 3 | Step 2480 | Loss: 9.7467\n",
      "Epoch 3 | Step 2490 | Loss: 8.4371\n",
      "Epoch 3 | Step 2500 | Loss: 8.0718\n",
      "Epoch 3 | Step 2510 | Loss: 7.8327\n",
      "Epoch 3 | Step 2520 | Loss: 8.5724\n",
      "Epoch 3 | Step 2530 | Loss: 8.9904\n",
      "Epoch 3 | Step 2540 | Loss: 8.9183\n",
      "Epoch 3 | Step 2550 | Loss: 9.5068\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/Qwen3-TTS/finetuning/sft_12hz.py\", line 161, in <module>\n",
      "    train()\n",
      "  File \"/workspace/Qwen3-TTS/finetuning/sft_12hz.py\", line 128, in train\n",
      "    shutil.copytree(MODEL_PATH, output_dir, dirs_exist_ok=True)\n",
      "  File \"/usr/lib/python3.11/shutil.py\", line 573, in copytree\n",
      "    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/shutil.py\", line 527, in _copytree\n",
      "    raise Error(errors)\n",
      "shutil.Error: [('./Qwen3-TTS-12Hz-1.7B-Base/model.safetensors', '../../output_whisper/checkpoint-epoch-3/model.safetensors', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/vocab.json', '../../output_whisper/checkpoint-epoch-3/vocab.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/tokenizer_config.json', '../../output_whisper/checkpoint-epoch-3/tokenizer_config.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/preprocessor_config.json', '../../output_whisper/checkpoint-epoch-3/preprocessor_config.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/README.md', '../../output_whisper/checkpoint-epoch-3/README.md', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.gitattributes', '../../output_whisper/checkpoint-epoch-3/.gitattributes', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/generation_config.json', '../../output_whisper/checkpoint-epoch-3/generation_config.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/config.json', '../../output_whisper/checkpoint-epoch-3/config.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/merges.txt', '../../output_whisper/checkpoint-epoch-3/merges.txt', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/speech_tokenizer/model.safetensors', '../../output_whisper/checkpoint-epoch-3/speech_tokenizer/model.safetensors', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/speech_tokenizer/preprocessor_config.json', '../../output_whisper/checkpoint-epoch-3/speech_tokenizer/preprocessor_config.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/speech_tokenizer/configuration.json', '../../output_whisper/checkpoint-epoch-3/speech_tokenizer/configuration.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/speech_tokenizer/config.json', '../../output_whisper/checkpoint-epoch-3/speech_tokenizer/config.json', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/model.safetensors.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/model.safetensors.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/vocab.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/vocab.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/tokenizer_config.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/tokenizer_config.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/preprocessor_config.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/preprocessor_config.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/README.md.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/README.md.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/.gitattributes.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/.gitattributes.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/generation_config.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/generation_config.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/config.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/config.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/merges.txt.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/merges.txt.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/speech_tokenizer/model.safetensors.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/speech_tokenizer/model.safetensors.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/speech_tokenizer/preprocessor_config.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/speech_tokenizer/preprocessor_config.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/speech_tokenizer/configuration.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/speech_tokenizer/configuration.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/download/speech_tokenizer/config.json.metadata', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/download/speech_tokenizer/config.json.metadata', '[Errno 122] Disk quota exceeded'), ('./Qwen3-TTS-12Hz-1.7B-Base/.cache/huggingface/.gitignore', '../../output_whisper/checkpoint-epoch-3/.cache/huggingface/.gitignore', '[Errno 122] Disk quota exceeded')]\n"
     ]
    }
   ],
   "source": [
    "!python sft_12hz.py --init_model_path \"./Qwen3-TTS-12Hz-1.7B-Base\" --output_model_path \"../../output_whisper\" --train_jsonl \"../../manifest_whisper_train_with_code_fixed.jsonl\" --batch_size 1 --lr 1e-6 --num_epochs 10 --speaker_name multi_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: sox: not found\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:529: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Epoch 0 | Step 0 | Loss: 18.4551\n",
      "Epoch 0 | Step 10 | Loss: 14.4413\n",
      "Epoch 0 | Step 20 | Loss: 12.2113\n",
      "Epoch 0 | Step 30 | Loss: 14.2152\n",
      "Epoch 0 | Step 40 | Loss: 14.2521\n",
      "Epoch 0 | Step 50 | Loss: 12.7657\n",
      "Epoch 0 | Step 60 | Loss: 14.6656\n",
      "Epoch 0 | Step 70 | Loss: 12.3758\n",
      "Epoch 0 | Step 80 | Loss: 12.6625\n",
      "Epoch 0 | Step 90 | Loss: 12.4507\n",
      "Epoch 0 | Step 100 | Loss: 14.8802\n",
      "Epoch 0 | Step 110 | Loss: 10.6096\n",
      "Epoch 0 | Step 120 | Loss: 12.3457\n",
      "Epoch 0 | Step 130 | Loss: 13.8565\n",
      "Epoch 0 | Step 140 | Loss: 12.3735\n",
      "Epoch 0 | Step 150 | Loss: 14.5992\n",
      "Epoch 0 | Step 160 | Loss: 11.6684\n",
      "Epoch 0 | Step 170 | Loss: 13.4503\n",
      "Epoch 0 | Step 180 | Loss: 12.9987\n",
      "Epoch 0 | Step 190 | Loss: 12.4055\n",
      "Epoch 0 | Step 200 | Loss: 11.4691\n",
      "Epoch 0 | Step 210 | Loss: 15.1537\n",
      "Epoch 0 | Step 220 | Loss: 12.5567\n",
      "Epoch 0 | Step 230 | Loss: 11.5729\n",
      "Epoch 0 | Step 240 | Loss: 12.2939\n",
      "Epoch 0 | Step 250 | Loss: 10.0011\n",
      "Epoch 0 | Step 260 | Loss: 11.6539\n",
      "Epoch 0 | Step 270 | Loss: 12.3856\n",
      "Epoch 0 | Step 280 | Loss: 9.7359\n",
      "Epoch 0 | Step 290 | Loss: 13.4257\n",
      "Epoch 0 | Step 300 | Loss: 11.9043\n",
      "Epoch 0 | Step 310 | Loss: 12.4029\n",
      "Epoch 0 | Step 320 | Loss: 11.6631\n",
      "Epoch 0 | Step 330 | Loss: 11.5264\n",
      "Epoch 0 | Step 340 | Loss: 11.8227\n",
      "Epoch 0 | Step 350 | Loss: 12.2490\n",
      "Epoch 0 | Step 360 | Loss: 12.3906\n",
      "Epoch 0 | Step 370 | Loss: 11.9356\n",
      "Epoch 0 | Step 380 | Loss: 10.8672\n",
      "Epoch 0 | Step 390 | Loss: 12.6795\n",
      "Epoch 0 | Step 400 | Loss: 10.8483\n",
      "Epoch 0 | Step 410 | Loss: 11.2035\n",
      "Epoch 0 | Step 420 | Loss: 10.0664\n",
      "Epoch 0 | Step 430 | Loss: 11.8042\n",
      "Epoch 0 | Step 440 | Loss: 11.6801\n",
      "Epoch 0 | Step 450 | Loss: 11.7375\n",
      "Epoch 0 | Step 460 | Loss: 10.8583\n",
      "Epoch 0 | Step 470 | Loss: 12.4807\n",
      "Epoch 0 | Step 480 | Loss: 11.5125\n",
      "Epoch 0 | Step 490 | Loss: 10.9668\n",
      "Epoch 0 | Step 500 | Loss: 11.1233\n",
      "Epoch 0 | Step 510 | Loss: 11.4005\n",
      "Epoch 0 | Step 520 | Loss: 11.1534\n",
      "Epoch 0 | Step 530 | Loss: 11.7472\n",
      "Epoch 0 | Step 540 | Loss: 12.3616\n",
      "Epoch 0 | Step 550 | Loss: 12.2826\n",
      "Epoch 0 | Step 560 | Loss: 11.1850\n",
      "Epoch 0 | Step 570 | Loss: 10.2269\n",
      "Epoch 0 | Step 580 | Loss: 12.1933\n",
      "Epoch 0 | Step 590 | Loss: 12.1867\n",
      "Epoch 0 | Step 600 | Loss: 11.0065\n",
      "Epoch 0 | Step 610 | Loss: 11.9434\n",
      "Epoch 0 | Step 620 | Loss: 11.0145\n",
      "Epoch 0 | Step 630 | Loss: 10.5682\n",
      "Epoch 0 | Step 640 | Loss: 11.0227\n",
      "Epoch 0 | Step 650 | Loss: 12.0016\n",
      "Epoch 0 | Step 660 | Loss: 11.0490\n",
      "Epoch 0 | Step 670 | Loss: 10.8508\n",
      "Epoch 0 | Step 680 | Loss: 11.5026\n",
      "Epoch 0 | Step 690 | Loss: 10.1761\n",
      "Epoch 0 | Step 700 | Loss: 10.7128\n",
      "Epoch 0 | Step 710 | Loss: 12.1554\n",
      "Epoch 0 | Step 720 | Loss: 9.4797\n",
      "Epoch 0 | Step 730 | Loss: 12.6069\n",
      "Epoch 0 | Step 740 | Loss: 10.6143\n",
      "Epoch 0 | Step 750 | Loss: 10.7208\n",
      "Epoch 0 | Step 760 | Loss: 9.8851\n",
      "Epoch 0 | Step 770 | Loss: 10.2392\n",
      "Epoch 0 | Step 780 | Loss: 9.9317\n",
      "Epoch 0 | Step 790 | Loss: 10.5817\n",
      "Epoch 0 | Step 800 | Loss: 6.9965\n",
      "Epoch 0 | Step 810 | Loss: 9.8188\n",
      "Epoch 0 | Step 820 | Loss: 10.9951\n",
      "Epoch 0 | Step 830 | Loss: 11.1859\n",
      "Epoch 0 | Step 840 | Loss: 10.1425\n",
      "Epoch 0 | Step 850 | Loss: 10.4367\n",
      "Epoch 0 | Step 860 | Loss: 10.6763\n",
      "Epoch 0 | Step 870 | Loss: 9.1926\n",
      "Epoch 0 | Step 880 | Loss: 10.4072\n",
      "Epoch 0 | Step 890 | Loss: 10.4789\n",
      "Epoch 0 | Step 900 | Loss: 10.3615\n",
      "Epoch 0 | Step 910 | Loss: 9.7089\n",
      "Epoch 0 | Step 920 | Loss: 10.6591\n",
      "Epoch 0 | Step 930 | Loss: 9.4995\n",
      "Epoch 0 | Step 940 | Loss: 9.4562\n",
      "Epoch 0 | Step 950 | Loss: 11.4166\n",
      "Epoch 0 | Step 960 | Loss: 9.7883\n",
      "Epoch 0 | Step 970 | Loss: 9.2312\n",
      "Epoch 0 | Step 980 | Loss: 9.7638\n",
      "Epoch 0 | Step 990 | Loss: 10.1504\n",
      "Epoch 0 | Step 1000 | Loss: 10.3744\n",
      "Epoch 0 | Step 1010 | Loss: 10.4208\n",
      "Epoch 0 | Step 1020 | Loss: 9.8263\n",
      "Epoch 0 | Step 1030 | Loss: 10.2581\n",
      "Epoch 0 | Step 1040 | Loss: 9.8667\n",
      "Epoch 0 | Step 1050 | Loss: 10.4306\n",
      "Epoch 0 | Step 1060 | Loss: 9.4172\n",
      "Epoch 0 | Step 1070 | Loss: 9.9049\n",
      "Epoch 0 | Step 1080 | Loss: 9.9488\n",
      "Epoch 0 | Step 1090 | Loss: 9.6416\n",
      "Epoch 0 | Step 1100 | Loss: 9.7042\n",
      "Epoch 0 | Step 1110 | Loss: 8.0042\n",
      "Epoch 0 | Step 1120 | Loss: 9.4061\n",
      "Epoch 0 | Step 1130 | Loss: 10.2877\n",
      "Epoch 0 | Step 1140 | Loss: 10.4858\n",
      "Epoch 0 | Step 1150 | Loss: 10.8499\n",
      "Epoch 0 | Step 1160 | Loss: 9.2641\n",
      "Epoch 0 | Step 1170 | Loss: 9.7515\n",
      "Epoch 0 | Step 1180 | Loss: 10.3921\n",
      "Epoch 0 | Step 1190 | Loss: 9.1966\n",
      "Epoch 0 | Step 1200 | Loss: 9.8890\n",
      "Epoch 0 | Step 1210 | Loss: 10.2440\n",
      "Epoch 0 | Step 1220 | Loss: 9.7971\n",
      "Epoch 0 | Step 1230 | Loss: 9.6991\n",
      "Epoch 0 | Step 1240 | Loss: 10.4394\n",
      "Epoch 0 | Step 1250 | Loss: 8.0152\n",
      "Epoch 0 | Step 1260 | Loss: 8.7969\n",
      "Epoch 0 | Step 1270 | Loss: 10.2774\n",
      "Epoch 0 | Step 1280 | Loss: 10.2131\n",
      "Epoch 0 | Step 1290 | Loss: 9.3245\n",
      "Epoch 0 | Step 1300 | Loss: 10.6013\n",
      "Epoch 0 | Step 1310 | Loss: 8.6696\n",
      "Epoch 0 | Step 1320 | Loss: 9.1179\n",
      "Epoch 0 | Step 1330 | Loss: 10.0920\n",
      "Epoch 0 | Step 1340 | Loss: 9.3361\n",
      "Epoch 0 | Step 1350 | Loss: 8.6958\n",
      "Epoch 0 | Step 1360 | Loss: 9.4668\n",
      "Epoch 0 | Step 1370 | Loss: 9.5775\n",
      "Epoch 0 | Step 1380 | Loss: 9.2929\n",
      "Epoch 0 | Step 1390 | Loss: 10.7365\n",
      "Epoch 0 | Step 1400 | Loss: 8.9644\n",
      "Epoch 0 | Step 1410 | Loss: 8.9829\n",
      "Epoch 0 | Step 1420 | Loss: 10.2902\n",
      "Epoch 0 | Step 1430 | Loss: 9.8115\n",
      "Epoch 0 | Step 1440 | Loss: 8.5144\n",
      "Epoch 0 | Step 1450 | Loss: 10.2285\n",
      "Epoch 0 | Step 1460 | Loss: 9.2955\n",
      "Epoch 0 | Step 1470 | Loss: 9.7702\n",
      "Epoch 0 | Step 1480 | Loss: 10.2406\n",
      "Epoch 0 | Step 1490 | Loss: 9.6927\n",
      "Epoch 0 | Step 1500 | Loss: 10.4365\n",
      "Epoch 0 | Step 1510 | Loss: 9.6646\n",
      "Epoch 0 | Step 1520 | Loss: 8.7579\n",
      "Epoch 0 | Step 1530 | Loss: 10.1893\n",
      "Epoch 0 | Step 1540 | Loss: 10.3571\n",
      "Epoch 0 | Step 1550 | Loss: 10.5019\n",
      "Epoch 0 | Step 1560 | Loss: 9.5455\n",
      "Epoch 0 | Step 1570 | Loss: 10.0140\n",
      "Epoch 0 | Step 1580 | Loss: 10.1605\n",
      "Epoch 0 | Step 1590 | Loss: 10.5224\n",
      "Epoch 0 | Step 1600 | Loss: 9.8278\n",
      "Epoch 0 | Step 1610 | Loss: 10.0323\n",
      "Epoch 0 | Step 1620 | Loss: 9.2452\n",
      "Epoch 0 | Step 1630 | Loss: 9.2184\n",
      "Epoch 0 | Step 1640 | Loss: 9.9641\n",
      "Epoch 0 | Step 1650 | Loss: 10.4908\n",
      "Epoch 0 | Step 1660 | Loss: 9.9207\n",
      "Epoch 0 | Step 1670 | Loss: 9.2726\n",
      "Epoch 0 | Step 1680 | Loss: 8.9687\n",
      "Epoch 0 | Step 1690 | Loss: 10.0713\n",
      "Epoch 0 | Step 1700 | Loss: 10.5002\n",
      "Epoch 0 | Step 1710 | Loss: 10.3973\n",
      "Epoch 0 | Step 1720 | Loss: 8.0340\n",
      "Epoch 0 | Step 1730 | Loss: 8.9377\n",
      "Epoch 0 | Step 1740 | Loss: 8.6657\n",
      "Epoch 0 | Step 1750 | Loss: 9.4779\n",
      "Epoch 0 | Step 1760 | Loss: 9.9824\n",
      "Epoch 0 | Step 1770 | Loss: 9.0684\n",
      "Epoch 0 | Step 1780 | Loss: 8.4632\n",
      "Epoch 0 | Step 1790 | Loss: 7.6451\n",
      "Epoch 0 | Step 1800 | Loss: 9.7379\n",
      "Epoch 0 | Step 1810 | Loss: 8.0516\n",
      "Epoch 0 | Step 1820 | Loss: 8.6287\n",
      "Epoch 0 | Step 1830 | Loss: 9.3603\n",
      "Epoch 0 | Step 1840 | Loss: 9.6456\n",
      "Epoch 0 | Step 1850 | Loss: 9.6412\n",
      "Epoch 0 | Step 1860 | Loss: 9.8852\n",
      "Epoch 0 | Step 1870 | Loss: 9.0952\n",
      "Epoch 0 | Step 1880 | Loss: 10.1824\n",
      "Epoch 0 | Step 1890 | Loss: 9.3585\n",
      "Epoch 0 | Step 1900 | Loss: 9.5960\n",
      "Epoch 0 | Step 1910 | Loss: 8.0955\n",
      "Epoch 0 | Step 1920 | Loss: 9.3788\n",
      "Epoch 0 | Step 1930 | Loss: 9.8514\n",
      "Epoch 0 | Step 1940 | Loss: 8.4237\n",
      "Epoch 0 | Step 1950 | Loss: 9.5485\n",
      "Epoch 0 | Step 1960 | Loss: 9.2845\n",
      "Epoch 0 | Step 1970 | Loss: 10.1198\n",
      "Epoch 0 | Step 1980 | Loss: 7.4868\n",
      "Epoch 0 | Step 1990 | Loss: 9.4383\n",
      "Epoch 0 | Step 2000 | Loss: 9.4957\n",
      "Epoch 0 | Step 2010 | Loss: 8.7962\n",
      "Epoch 0 | Step 2020 | Loss: 9.8396\n",
      "Epoch 0 | Step 2030 | Loss: 9.1702\n",
      "Epoch 0 | Step 2040 | Loss: 10.8031\n",
      "Epoch 0 | Step 2050 | Loss: 9.5875\n",
      "Epoch 0 | Step 2060 | Loss: 9.2929\n",
      "Epoch 0 | Step 2070 | Loss: 9.3441\n",
      "Epoch 0 | Step 2080 | Loss: 9.7785\n",
      "Epoch 0 | Step 2090 | Loss: 9.5736\n",
      "Epoch 0 | Step 2100 | Loss: 9.5577\n",
      "Epoch 0 | Step 2110 | Loss: 8.5921\n",
      "Epoch 0 | Step 2120 | Loss: 6.2046\n",
      "Epoch 0 | Step 2130 | Loss: 10.1724\n",
      "Epoch 0 | Step 2140 | Loss: 9.3664\n",
      "Epoch 0 | Step 2150 | Loss: 9.1731\n",
      "Epoch 0 | Step 2160 | Loss: 9.5158\n",
      "Epoch 0 | Step 2170 | Loss: 8.7370\n",
      "Epoch 0 | Step 2180 | Loss: 8.7708\n",
      "Epoch 0 | Step 2190 | Loss: 9.6117\n",
      "Epoch 0 | Step 2200 | Loss: 10.5666\n",
      "Epoch 0 | Step 2210 | Loss: 10.6192\n",
      "Epoch 0 | Step 2220 | Loss: 9.3056\n",
      "Epoch 0 | Step 2230 | Loss: 9.6577\n",
      "Epoch 0 | Step 2240 | Loss: 8.1206\n",
      "Epoch 0 | Step 2250 | Loss: 9.7384\n",
      "Epoch 0 | Step 2260 | Loss: 9.6950\n",
      "Epoch 0 | Step 2270 | Loss: 9.4198\n",
      "Epoch 0 | Step 2280 | Loss: 7.7885\n",
      "Epoch 0 | Step 2290 | Loss: 9.7739\n",
      "Epoch 0 | Step 2300 | Loss: 9.7581\n",
      "Epoch 0 | Step 2310 | Loss: 9.6619\n",
      "Epoch 0 | Step 2320 | Loss: 9.4238\n",
      "Epoch 0 | Step 2330 | Loss: 9.5987\n",
      "Epoch 0 | Step 2340 | Loss: 9.4175\n",
      "Epoch 0 | Step 2350 | Loss: 9.3672\n",
      "Epoch 0 | Step 2360 | Loss: 9.7833\n",
      "Epoch 0 | Step 2370 | Loss: 9.8706\n",
      "Epoch 0 | Step 2380 | Loss: 7.8111\n",
      "Epoch 0 | Step 2390 | Loss: 7.8223\n",
      "Epoch 0 | Step 2400 | Loss: 10.1465\n",
      "Epoch 0 | Step 2410 | Loss: 9.2793\n",
      "Epoch 0 | Step 2420 | Loss: 10.2314\n",
      "Epoch 0 | Step 2430 | Loss: 9.5235\n",
      "Epoch 0 | Step 2440 | Loss: 9.5794\n",
      "Epoch 0 | Step 2450 | Loss: 9.3731\n",
      "Epoch 0 | Step 2460 | Loss: 9.7974\n",
      "Epoch 0 | Step 2470 | Loss: 8.6424\n",
      "Epoch 0 | Step 2480 | Loss: 9.4985\n",
      "Epoch 0 | Step 2490 | Loss: 9.4648\n",
      "Epoch 0 | Step 2500 | Loss: 9.6812\n",
      "Epoch 0 | Step 2510 | Loss: 9.8169\n",
      "Epoch 0 | Step 2520 | Loss: 8.5137\n",
      "Epoch 0 | Step 2530 | Loss: 9.9945\n",
      "Epoch 0 | Step 2540 | Loss: 8.9378\n",
      "Epoch 0 | Step 2550 | Loss: 9.1566\n",
      "Epoch 0 | Step 2560 | Loss: 8.8128\n",
      "Epoch 0 | Step 2570 | Loss: 9.7016\n",
      "Epoch 0 | Step 2580 | Loss: 9.7124\n",
      "Epoch 1 | Step 0 | Loss: 9.0722\n",
      "Epoch 1 | Step 10 | Loss: 10.1048\n",
      "Epoch 1 | Step 20 | Loss: 9.5705\n",
      "Epoch 1 | Step 30 | Loss: 8.6100\n",
      "Epoch 1 | Step 40 | Loss: 8.3822\n",
      "Epoch 1 | Step 50 | Loss: 7.8897\n",
      "Epoch 1 | Step 60 | Loss: 9.0050\n",
      "Epoch 1 | Step 70 | Loss: 7.2575\n",
      "Epoch 1 | Step 80 | Loss: 9.1559\n",
      "Epoch 1 | Step 90 | Loss: 8.9374\n",
      "Epoch 1 | Step 100 | Loss: 9.1756\n",
      "Epoch 1 | Step 110 | Loss: 8.8924\n",
      "Epoch 1 | Step 120 | Loss: 9.1317\n",
      "Epoch 1 | Step 130 | Loss: 8.3873\n",
      "Epoch 1 | Step 140 | Loss: 11.7096\n",
      "Epoch 1 | Step 150 | Loss: 6.8707\n",
      "Epoch 1 | Step 160 | Loss: 8.1934\n",
      "Epoch 1 | Step 170 | Loss: 9.2418\n",
      "Epoch 1 | Step 180 | Loss: 9.0219\n",
      "Epoch 1 | Step 190 | Loss: 9.2141\n",
      "Epoch 1 | Step 200 | Loss: 9.8907\n",
      "Epoch 1 | Step 210 | Loss: 7.1354\n",
      "Epoch 1 | Step 220 | Loss: 9.1181\n",
      "Epoch 1 | Step 230 | Loss: 10.2634\n",
      "Epoch 1 | Step 240 | Loss: 9.5116\n",
      "Epoch 1 | Step 250 | Loss: 9.7285\n",
      "Epoch 1 | Step 260 | Loss: 9.1417\n",
      "Epoch 1 | Step 270 | Loss: 9.8380\n",
      "Epoch 1 | Step 280 | Loss: 10.1177\n",
      "Epoch 1 | Step 290 | Loss: 9.4506\n",
      "Epoch 1 | Step 300 | Loss: 9.3623\n",
      "Epoch 1 | Step 310 | Loss: 9.0914\n",
      "Epoch 1 | Step 320 | Loss: 8.8004\n",
      "Epoch 1 | Step 330 | Loss: 8.8951\n",
      "Epoch 1 | Step 340 | Loss: 8.8035\n",
      "Epoch 1 | Step 350 | Loss: 8.9734\n",
      "Epoch 1 | Step 360 | Loss: 8.2867\n",
      "Epoch 1 | Step 370 | Loss: 10.0904\n",
      "Epoch 1 | Step 380 | Loss: 9.6490\n",
      "Epoch 1 | Step 390 | Loss: 10.3086\n",
      "Epoch 1 | Step 400 | Loss: 9.0300\n",
      "Epoch 1 | Step 410 | Loss: 8.5588\n",
      "Epoch 1 | Step 420 | Loss: 9.4845\n",
      "Epoch 1 | Step 430 | Loss: 8.8344\n",
      "Epoch 1 | Step 440 | Loss: 8.6620\n",
      "Epoch 1 | Step 450 | Loss: 9.9417\n",
      "Epoch 1 | Step 460 | Loss: 9.6150\n",
      "Epoch 1 | Step 470 | Loss: 7.6936\n",
      "Epoch 1 | Step 480 | Loss: 8.8237\n",
      "Epoch 1 | Step 490 | Loss: 8.7859\n",
      "Epoch 1 | Step 500 | Loss: 9.6196\n",
      "Epoch 1 | Step 510 | Loss: 9.2387\n",
      "Epoch 1 | Step 520 | Loss: 7.5721\n",
      "Epoch 1 | Step 530 | Loss: 9.4586\n",
      "Epoch 1 | Step 540 | Loss: 8.3930\n",
      "Epoch 1 | Step 550 | Loss: 9.9207\n",
      "Epoch 1 | Step 560 | Loss: 7.9759\n",
      "Epoch 1 | Step 570 | Loss: 8.5837\n",
      "Epoch 1 | Step 580 | Loss: 9.9046\n",
      "Epoch 1 | Step 590 | Loss: 9.0809\n",
      "Epoch 1 | Step 600 | Loss: 9.4865\n",
      "Epoch 1 | Step 610 | Loss: 7.5798\n",
      "Epoch 1 | Step 620 | Loss: 9.1960\n",
      "Epoch 1 | Step 630 | Loss: 9.3944\n",
      "Epoch 1 | Step 640 | Loss: 9.9739\n",
      "Epoch 1 | Step 650 | Loss: 9.0662\n",
      "Epoch 1 | Step 660 | Loss: 6.8245\n",
      "Epoch 1 | Step 670 | Loss: 8.8834\n",
      "Epoch 1 | Step 680 | Loss: 9.8451\n",
      "Epoch 1 | Step 690 | Loss: 10.2992\n",
      "Epoch 1 | Step 700 | Loss: 7.9244\n",
      "Epoch 1 | Step 710 | Loss: 8.8247\n",
      "Epoch 1 | Step 720 | Loss: 10.0226\n",
      "Epoch 1 | Step 730 | Loss: 9.3517\n",
      "Epoch 1 | Step 740 | Loss: 6.2530\n",
      "Epoch 1 | Step 750 | Loss: 8.9641\n",
      "Epoch 1 | Step 760 | Loss: 9.7753\n",
      "Epoch 1 | Step 770 | Loss: 8.7223\n",
      "Epoch 1 | Step 780 | Loss: 8.0425\n",
      "Epoch 1 | Step 790 | Loss: 8.7388\n",
      "Epoch 1 | Step 800 | Loss: 9.0206\n",
      "Epoch 1 | Step 810 | Loss: 9.0551\n",
      "Epoch 1 | Step 820 | Loss: 9.4510\n",
      "Epoch 1 | Step 830 | Loss: 7.3739\n",
      "Epoch 1 | Step 840 | Loss: 9.2777\n",
      "Epoch 1 | Step 850 | Loss: 8.9559\n",
      "Epoch 1 | Step 860 | Loss: 7.8448\n",
      "Epoch 1 | Step 870 | Loss: 9.6329\n",
      "Epoch 1 | Step 880 | Loss: 9.3752\n",
      "Epoch 1 | Step 890 | Loss: 8.9976\n",
      "Epoch 1 | Step 900 | Loss: 9.5490\n",
      "Epoch 1 | Step 910 | Loss: 9.4717\n",
      "Epoch 1 | Step 920 | Loss: 8.7771\n",
      "Epoch 1 | Step 930 | Loss: 8.6818\n",
      "Epoch 1 | Step 940 | Loss: 8.4581\n",
      "Epoch 1 | Step 950 | Loss: 9.0946\n",
      "Epoch 1 | Step 960 | Loss: 6.6459\n",
      "Epoch 1 | Step 970 | Loss: 9.6341\n",
      "Epoch 1 | Step 980 | Loss: 7.4458\n",
      "Epoch 1 | Step 990 | Loss: 8.9305\n",
      "Epoch 1 | Step 1000 | Loss: 8.5622\n",
      "Epoch 1 | Step 1010 | Loss: 9.4291\n",
      "Epoch 1 | Step 1020 | Loss: 8.7767\n",
      "Epoch 1 | Step 1030 | Loss: 9.1048\n",
      "Epoch 1 | Step 1040 | Loss: 9.3862\n",
      "Epoch 1 | Step 1050 | Loss: 7.2337\n",
      "Epoch 1 | Step 1060 | Loss: 8.9669\n",
      "Epoch 1 | Step 1070 | Loss: 9.3011\n",
      "Epoch 1 | Step 1080 | Loss: 8.5773\n",
      "Epoch 1 | Step 1090 | Loss: 9.0770\n",
      "Epoch 1 | Step 1100 | Loss: 8.5143\n",
      "Epoch 1 | Step 1110 | Loss: 8.9970\n",
      "Epoch 1 | Step 1120 | Loss: 8.6253\n",
      "Epoch 1 | Step 1130 | Loss: 7.1620\n",
      "Epoch 1 | Step 1140 | Loss: 9.5833\n",
      "Epoch 1 | Step 1150 | Loss: 9.6312\n",
      "Epoch 1 | Step 1160 | Loss: 8.8412\n",
      "Epoch 1 | Step 1170 | Loss: 8.5216\n",
      "Epoch 1 | Step 1180 | Loss: 9.1301\n",
      "Epoch 1 | Step 1190 | Loss: 9.8791\n",
      "Epoch 1 | Step 1200 | Loss: 9.6323\n",
      "Epoch 1 | Step 1210 | Loss: 9.2641\n",
      "Epoch 1 | Step 1220 | Loss: 9.3893\n",
      "Epoch 1 | Step 1230 | Loss: 8.8350\n",
      "Epoch 1 | Step 1240 | Loss: 8.5514\n",
      "Epoch 1 | Step 1250 | Loss: 10.5818\n",
      "Epoch 1 | Step 1260 | Loss: 8.8075\n",
      "Epoch 1 | Step 1270 | Loss: 9.0637\n",
      "Epoch 1 | Step 1280 | Loss: 9.3281\n",
      "Epoch 1 | Step 1290 | Loss: 8.4754\n",
      "Epoch 1 | Step 1300 | Loss: 9.0154\n",
      "Epoch 1 | Step 1310 | Loss: 8.4726\n",
      "Epoch 1 | Step 1320 | Loss: 8.6022\n",
      "Epoch 1 | Step 1330 | Loss: 8.9652\n",
      "Epoch 1 | Step 1340 | Loss: 9.0165\n",
      "Epoch 1 | Step 1350 | Loss: 8.7220\n",
      "Epoch 1 | Step 1360 | Loss: 9.1372\n",
      "Epoch 1 | Step 1370 | Loss: 8.3633\n",
      "Epoch 1 | Step 1380 | Loss: 9.7877\n",
      "Epoch 1 | Step 1390 | Loss: 9.3883\n",
      "Epoch 1 | Step 1400 | Loss: 9.2142\n",
      "Epoch 1 | Step 1410 | Loss: 9.5260\n",
      "Epoch 1 | Step 1420 | Loss: 8.9060\n",
      "Epoch 1 | Step 1430 | Loss: 9.1402\n",
      "Epoch 1 | Step 1440 | Loss: 9.2900\n",
      "Epoch 1 | Step 1450 | Loss: 9.3770\n",
      "Epoch 1 | Step 1460 | Loss: 10.1714\n",
      "Epoch 1 | Step 1470 | Loss: 10.0308\n",
      "Epoch 1 | Step 1480 | Loss: 9.6953\n",
      "Epoch 1 | Step 1490 | Loss: 7.1703\n",
      "Epoch 1 | Step 1500 | Loss: 8.6644\n",
      "Epoch 1 | Step 1510 | Loss: 7.7700\n",
      "Epoch 1 | Step 1520 | Loss: 8.3342\n",
      "Epoch 1 | Step 1530 | Loss: 9.7432\n",
      "Epoch 1 | Step 1540 | Loss: 8.6363\n",
      "Epoch 1 | Step 1550 | Loss: 9.8146\n",
      "Epoch 1 | Step 1560 | Loss: 9.3389\n",
      "Epoch 1 | Step 1570 | Loss: 8.3822\n",
      "Epoch 1 | Step 1580 | Loss: 7.2153\n",
      "Epoch 1 | Step 1590 | Loss: 9.5704\n",
      "Epoch 1 | Step 1600 | Loss: 7.8195\n",
      "Epoch 1 | Step 1610 | Loss: 8.3916\n",
      "Epoch 1 | Step 1620 | Loss: 8.9788\n",
      "Epoch 1 | Step 1630 | Loss: 7.3933\n",
      "Epoch 1 | Step 1640 | Loss: 8.9018\n",
      "Epoch 1 | Step 1650 | Loss: 9.4947\n",
      "Epoch 1 | Step 1660 | Loss: 9.4184\n",
      "Epoch 1 | Step 1670 | Loss: 8.9984\n",
      "Epoch 1 | Step 1680 | Loss: 9.3276\n",
      "Epoch 1 | Step 1690 | Loss: 9.0442\n",
      "Epoch 1 | Step 1700 | Loss: 7.6373\n",
      "Epoch 1 | Step 1710 | Loss: 7.9841\n",
      "Epoch 1 | Step 1720 | Loss: 8.3200\n",
      "Epoch 1 | Step 1730 | Loss: 6.5987\n",
      "Epoch 1 | Step 1740 | Loss: 7.5304\n",
      "Epoch 1 | Step 1750 | Loss: 8.3106\n",
      "Epoch 1 | Step 1760 | Loss: 9.6847\n",
      "Epoch 1 | Step 1770 | Loss: 9.0355\n",
      "Epoch 1 | Step 1780 | Loss: 10.1834\n",
      "Epoch 1 | Step 1790 | Loss: 9.4198\n",
      "Epoch 1 | Step 1800 | Loss: 8.7538\n",
      "Epoch 1 | Step 1810 | Loss: 9.2079\n",
      "Epoch 1 | Step 1820 | Loss: 9.4737\n",
      "Epoch 1 | Step 1830 | Loss: 8.0640\n",
      "Epoch 1 | Step 1840 | Loss: 8.7762\n",
      "Epoch 1 | Step 1850 | Loss: 8.9882\n",
      "Epoch 1 | Step 1860 | Loss: 9.2603\n",
      "Epoch 1 | Step 1870 | Loss: 8.6264\n",
      "Epoch 1 | Step 1880 | Loss: 9.4126\n",
      "Epoch 1 | Step 1890 | Loss: 8.1742\n",
      "Epoch 1 | Step 1900 | Loss: 7.8696\n",
      "Epoch 1 | Step 1910 | Loss: 9.9220\n",
      "Epoch 1 | Step 1920 | Loss: 8.6359\n",
      "Epoch 1 | Step 1930 | Loss: 9.3582\n",
      "Epoch 1 | Step 1940 | Loss: 9.3050\n",
      "Epoch 1 | Step 1950 | Loss: 9.0987\n",
      "Epoch 1 | Step 1960 | Loss: 9.2213\n",
      "Epoch 1 | Step 1970 | Loss: 9.8427\n",
      "Epoch 1 | Step 1980 | Loss: 8.7815\n",
      "Epoch 1 | Step 1990 | Loss: 7.2787\n",
      "Epoch 1 | Step 2000 | Loss: 9.1470\n",
      "Epoch 1 | Step 2010 | Loss: 7.4192\n",
      "Epoch 1 | Step 2020 | Loss: 8.7273\n",
      "Epoch 1 | Step 2030 | Loss: 9.3548\n",
      "Epoch 1 | Step 2040 | Loss: 8.2037\n",
      "Epoch 1 | Step 2050 | Loss: 9.5067\n",
      "Epoch 1 | Step 2060 | Loss: 9.0117\n",
      "Epoch 1 | Step 2070 | Loss: 9.9970\n",
      "Epoch 1 | Step 2080 | Loss: 9.9713\n",
      "Epoch 1 | Step 2090 | Loss: 8.1658\n",
      "Epoch 1 | Step 2100 | Loss: 9.3443\n",
      "Epoch 1 | Step 2110 | Loss: 9.4045\n",
      "Epoch 1 | Step 2120 | Loss: 8.8281\n",
      "Epoch 1 | Step 2130 | Loss: 7.3250\n",
      "Epoch 1 | Step 2140 | Loss: 8.6219\n",
      "Epoch 1 | Step 2150 | Loss: 7.8630\n",
      "Epoch 1 | Step 2160 | Loss: 8.6561\n",
      "Epoch 1 | Step 2170 | Loss: 9.1927\n",
      "Epoch 1 | Step 2180 | Loss: 9.7268\n",
      "Epoch 1 | Step 2190 | Loss: 9.7923\n",
      "Epoch 1 | Step 2200 | Loss: 8.7177\n",
      "Epoch 1 | Step 2210 | Loss: 8.6729\n",
      "Epoch 1 | Step 2220 | Loss: 9.2944\n",
      "Epoch 1 | Step 2230 | Loss: 9.2227\n",
      "Epoch 1 | Step 2240 | Loss: 9.6531\n",
      "Epoch 1 | Step 2250 | Loss: 8.4576\n",
      "Epoch 1 | Step 2260 | Loss: 8.7503\n",
      "Epoch 1 | Step 2270 | Loss: 9.8767\n",
      "Epoch 1 | Step 2280 | Loss: 8.7801\n",
      "Epoch 1 | Step 2290 | Loss: 5.9814\n",
      "Epoch 1 | Step 2300 | Loss: 9.2854\n",
      "Epoch 1 | Step 2310 | Loss: 8.8423\n",
      "Epoch 1 | Step 2320 | Loss: 7.5251\n",
      "Epoch 1 | Step 2330 | Loss: 9.1717\n",
      "Epoch 1 | Step 2340 | Loss: 9.7209\n",
      "Epoch 1 | Step 2350 | Loss: 9.0447\n",
      "Epoch 1 | Step 2360 | Loss: 6.8884\n",
      "Epoch 1 | Step 2370 | Loss: 9.0991\n",
      "Epoch 1 | Step 2380 | Loss: 8.9896\n",
      "Epoch 1 | Step 2390 | Loss: 9.2137\n",
      "Epoch 1 | Step 2400 | Loss: 8.8243\n",
      "Epoch 1 | Step 2410 | Loss: 8.6239\n",
      "Epoch 1 | Step 2420 | Loss: 8.9901\n",
      "Epoch 1 | Step 2430 | Loss: 9.0407\n",
      "Epoch 1 | Step 2440 | Loss: 9.1487\n",
      "Epoch 1 | Step 2450 | Loss: 8.8790\n",
      "Epoch 1 | Step 2460 | Loss: 8.5262\n",
      "Epoch 1 | Step 2470 | Loss: 9.7338\n",
      "Epoch 1 | Step 2480 | Loss: 8.5321\n",
      "Epoch 1 | Step 2490 | Loss: 8.7701\n",
      "Epoch 1 | Step 2500 | Loss: 9.1223\n",
      "Epoch 1 | Step 2510 | Loss: 9.6949\n",
      "Epoch 1 | Step 2520 | Loss: 9.1018\n",
      "Epoch 1 | Step 2530 | Loss: 9.4333\n",
      "Epoch 1 | Step 2540 | Loss: 9.4564\n",
      "Epoch 1 | Step 2550 | Loss: 9.8710\n",
      "Epoch 1 | Step 2560 | Loss: 9.3134\n",
      "Epoch 1 | Step 2570 | Loss: 8.6136\n",
      "Epoch 1 | Step 2580 | Loss: 7.9385\n",
      "Epoch 2 | Step 0 | Loss: 7.4883\n",
      "Epoch 2 | Step 10 | Loss: 8.9588\n",
      "Epoch 2 | Step 20 | Loss: 5.8611\n",
      "Epoch 2 | Step 30 | Loss: 8.9011\n",
      "Epoch 2 | Step 40 | Loss: 9.6027\n",
      "Epoch 2 | Step 50 | Loss: 7.9314\n",
      "Epoch 2 | Step 60 | Loss: 8.2933\n",
      "Epoch 2 | Step 70 | Loss: 10.0366\n",
      "Epoch 2 | Step 80 | Loss: 7.7022\n",
      "Epoch 2 | Step 90 | Loss: 8.2922\n",
      "Epoch 2 | Step 100 | Loss: 9.6358\n",
      "Epoch 2 | Step 110 | Loss: 9.2243\n",
      "Epoch 2 | Step 120 | Loss: 8.6311\n",
      "Epoch 2 | Step 130 | Loss: 9.4012\n",
      "Epoch 2 | Step 140 | Loss: 8.8848\n",
      "Epoch 2 | Step 150 | Loss: 7.5535\n",
      "Epoch 2 | Step 160 | Loss: 10.0379\n",
      "Epoch 2 | Step 170 | Loss: 8.9477\n",
      "Epoch 2 | Step 180 | Loss: 7.7814\n",
      "Epoch 2 | Step 190 | Loss: 9.4739\n",
      "Epoch 2 | Step 200 | Loss: 9.7370\n",
      "Epoch 2 | Step 210 | Loss: 9.4723\n",
      "Epoch 2 | Step 220 | Loss: 8.5656\n",
      "Epoch 2 | Step 230 | Loss: 9.5281\n",
      "Epoch 2 | Step 240 | Loss: 8.8397\n",
      "Epoch 2 | Step 250 | Loss: 9.0365\n",
      "Epoch 2 | Step 260 | Loss: 8.6376\n",
      "Epoch 2 | Step 270 | Loss: 8.2360\n",
      "Epoch 2 | Step 280 | Loss: 8.9317\n",
      "Epoch 2 | Step 290 | Loss: 8.9108\n",
      "Epoch 2 | Step 300 | Loss: 9.3640\n",
      "Epoch 2 | Step 310 | Loss: 9.5042\n",
      "Epoch 2 | Step 320 | Loss: 7.9823\n",
      "Epoch 2 | Step 330 | Loss: 6.5094\n",
      "Epoch 2 | Step 340 | Loss: 7.7523\n",
      "Epoch 2 | Step 350 | Loss: 9.6370\n",
      "Epoch 2 | Step 360 | Loss: 9.4941\n",
      "Epoch 2 | Step 370 | Loss: 9.2363\n",
      "Epoch 2 | Step 380 | Loss: 8.7788\n",
      "Epoch 2 | Step 390 | Loss: 8.8060\n",
      "Epoch 2 | Step 400 | Loss: 8.9983\n",
      "Epoch 2 | Step 410 | Loss: 8.1920\n",
      "Epoch 2 | Step 420 | Loss: 9.4003\n",
      "Epoch 2 | Step 430 | Loss: 9.2512\n",
      "Epoch 2 | Step 440 | Loss: 9.4342\n",
      "Epoch 2 | Step 450 | Loss: 9.3915\n",
      "Epoch 2 | Step 460 | Loss: 9.3462\n",
      "Epoch 2 | Step 470 | Loss: 8.9448\n",
      "Epoch 2 | Step 480 | Loss: 9.1326\n",
      "Epoch 2 | Step 490 | Loss: 8.4257\n",
      "Epoch 2 | Step 500 | Loss: 9.2644\n",
      "Epoch 2 | Step 510 | Loss: 8.2561\n",
      "Epoch 2 | Step 520 | Loss: 8.9064\n",
      "Epoch 2 | Step 530 | Loss: 8.4224\n",
      "Epoch 2 | Step 540 | Loss: 9.3154\n",
      "Epoch 2 | Step 550 | Loss: 9.6933\n",
      "Epoch 2 | Step 560 | Loss: 9.0939\n",
      "Epoch 2 | Step 570 | Loss: 9.8266\n",
      "Epoch 2 | Step 580 | Loss: 9.2247\n",
      "Epoch 2 | Step 590 | Loss: 8.6663\n",
      "Epoch 2 | Step 600 | Loss: 8.7245\n",
      "Epoch 2 | Step 610 | Loss: 9.3999\n",
      "Epoch 2 | Step 620 | Loss: 10.2054\n",
      "Epoch 2 | Step 630 | Loss: 8.8088\n",
      "Epoch 2 | Step 640 | Loss: 8.5668\n",
      "Epoch 2 | Step 650 | Loss: 7.8486\n",
      "Epoch 2 | Step 660 | Loss: 8.5426\n",
      "Epoch 2 | Step 670 | Loss: 7.8582\n",
      "Epoch 2 | Step 680 | Loss: 9.3163\n",
      "Epoch 2 | Step 690 | Loss: 9.2320\n",
      "Epoch 2 | Step 700 | Loss: 9.4031\n",
      "Epoch 2 | Step 710 | Loss: 9.2597\n",
      "Epoch 2 | Step 720 | Loss: 8.8914\n",
      "Epoch 2 | Step 730 | Loss: 9.7412\n",
      "Epoch 2 | Step 740 | Loss: 9.0279\n",
      "Epoch 2 | Step 750 | Loss: 9.9034\n",
      "Epoch 2 | Step 760 | Loss: 8.6106\n",
      "Epoch 2 | Step 770 | Loss: 8.7479\n",
      "Epoch 2 | Step 780 | Loss: 9.1761\n",
      "Epoch 2 | Step 790 | Loss: 8.7247\n",
      "Epoch 2 | Step 800 | Loss: 8.4942\n",
      "Epoch 2 | Step 810 | Loss: 8.8740\n",
      "Epoch 2 | Step 820 | Loss: 9.5412\n",
      "Epoch 2 | Step 830 | Loss: 8.5672\n",
      "Epoch 2 | Step 840 | Loss: 8.9518\n",
      "Epoch 2 | Step 850 | Loss: 9.4548\n",
      "Epoch 2 | Step 860 | Loss: 8.0278\n",
      "Epoch 2 | Step 870 | Loss: 8.9583\n",
      "Epoch 2 | Step 880 | Loss: 9.3117\n",
      "Epoch 2 | Step 890 | Loss: 9.0416\n",
      "Epoch 2 | Step 900 | Loss: 8.3446\n",
      "Epoch 2 | Step 910 | Loss: 8.8171\n",
      "Epoch 2 | Step 920 | Loss: 8.4499\n",
      "Epoch 2 | Step 930 | Loss: 7.0503\n",
      "Epoch 2 | Step 940 | Loss: 9.1322\n",
      "Epoch 2 | Step 950 | Loss: 9.1120\n",
      "Epoch 2 | Step 960 | Loss: 8.8617\n",
      "Epoch 2 | Step 970 | Loss: 8.3495\n",
      "Epoch 2 | Step 980 | Loss: 9.1921\n",
      "Epoch 2 | Step 990 | Loss: 9.8044\n",
      "Epoch 2 | Step 1000 | Loss: 9.0785\n",
      "Epoch 2 | Step 1010 | Loss: 8.7151\n",
      "Epoch 2 | Step 1020 | Loss: 8.7962\n",
      "Epoch 2 | Step 1030 | Loss: 8.9787\n",
      "Epoch 2 | Step 1040 | Loss: 8.5177\n",
      "Epoch 2 | Step 1050 | Loss: 8.9754\n",
      "Epoch 2 | Step 1060 | Loss: 9.5531\n",
      "Epoch 2 | Step 1070 | Loss: 9.5306\n",
      "Epoch 2 | Step 1080 | Loss: 9.1677\n",
      "Epoch 2 | Step 1090 | Loss: 8.2611\n",
      "Epoch 2 | Step 1100 | Loss: 7.7967\n",
      "Epoch 2 | Step 1110 | Loss: 8.9183\n",
      "Epoch 2 | Step 1120 | Loss: 9.4552\n",
      "Epoch 2 | Step 1130 | Loss: 9.6311\n",
      "Epoch 2 | Step 1140 | Loss: 8.8078\n",
      "Epoch 2 | Step 1150 | Loss: 8.4485\n",
      "Epoch 2 | Step 1160 | Loss: 9.0446\n",
      "Epoch 2 | Step 1170 | Loss: 8.6854\n",
      "Epoch 2 | Step 1180 | Loss: 8.8122\n",
      "Epoch 2 | Step 1190 | Loss: 8.6751\n",
      "Epoch 2 | Step 1200 | Loss: 9.0210\n",
      "Epoch 2 | Step 1210 | Loss: 8.2549\n",
      "Epoch 2 | Step 1220 | Loss: 7.3150\n",
      "Epoch 2 | Step 1230 | Loss: 10.2265\n",
      "Epoch 2 | Step 1240 | Loss: 9.0484\n",
      "Epoch 2 | Step 1250 | Loss: 7.2965\n",
      "Epoch 2 | Step 1260 | Loss: 6.1495\n",
      "Epoch 2 | Step 1270 | Loss: 9.0045\n",
      "Epoch 2 | Step 1280 | Loss: 8.2648\n",
      "Epoch 2 | Step 1290 | Loss: 8.9059\n",
      "Epoch 2 | Step 1300 | Loss: 9.1205\n",
      "Epoch 2 | Step 1310 | Loss: 9.7338\n",
      "Epoch 2 | Step 1320 | Loss: 8.7853\n",
      "Epoch 2 | Step 1330 | Loss: 9.1124\n",
      "Epoch 2 | Step 1340 | Loss: 8.8251\n",
      "Epoch 2 | Step 1350 | Loss: 8.9413\n",
      "Epoch 2 | Step 1360 | Loss: 7.6014\n",
      "Epoch 2 | Step 1370 | Loss: 8.7378\n",
      "Epoch 2 | Step 1380 | Loss: 9.2291\n",
      "Epoch 2 | Step 1390 | Loss: 9.6616\n",
      "Epoch 2 | Step 1400 | Loss: 9.8124\n",
      "Epoch 2 | Step 1410 | Loss: 9.6762\n",
      "Epoch 2 | Step 1420 | Loss: 7.0470\n",
      "Epoch 2 | Step 1430 | Loss: 9.3501\n",
      "Epoch 2 | Step 1440 | Loss: 8.9795\n",
      "Epoch 2 | Step 1450 | Loss: 8.1855\n",
      "Epoch 2 | Step 1460 | Loss: 8.8096\n",
      "Epoch 2 | Step 1470 | Loss: 8.8135\n",
      "Epoch 2 | Step 1480 | Loss: 9.5409\n",
      "Epoch 2 | Step 1490 | Loss: 8.4205\n",
      "Epoch 2 | Step 1500 | Loss: 8.9313\n",
      "Epoch 2 | Step 1510 | Loss: 9.7973\n",
      "Epoch 2 | Step 1520 | Loss: 8.5424\n",
      "Epoch 2 | Step 1530 | Loss: 9.3101\n",
      "Epoch 2 | Step 1540 | Loss: 8.9301\n",
      "Epoch 2 | Step 1550 | Loss: 8.2868\n",
      "Epoch 2 | Step 1560 | Loss: 9.4151\n",
      "Epoch 2 | Step 1570 | Loss: 9.3685\n",
      "Epoch 2 | Step 1580 | Loss: 10.1615\n",
      "Epoch 2 | Step 1590 | Loss: 8.3047\n",
      "Epoch 2 | Step 1600 | Loss: 8.8291\n",
      "Epoch 2 | Step 1610 | Loss: 9.0518\n",
      "Epoch 2 | Step 1620 | Loss: 9.1650\n",
      "Epoch 2 | Step 1630 | Loss: 9.1178\n",
      "Epoch 2 | Step 1640 | Loss: 9.3427\n",
      "Epoch 2 | Step 1650 | Loss: 6.7774\n",
      "Epoch 2 | Step 1660 | Loss: 8.5252\n",
      "Epoch 2 | Step 1670 | Loss: 9.1080\n",
      "Epoch 2 | Step 1680 | Loss: 9.1089\n",
      "Epoch 2 | Step 1690 | Loss: 8.9791\n",
      "Epoch 2 | Step 1700 | Loss: 9.1957\n",
      "Epoch 2 | Step 1710 | Loss: 8.4255\n",
      "Epoch 2 | Step 1720 | Loss: 9.2341\n",
      "Epoch 2 | Step 1730 | Loss: 6.2973\n",
      "Epoch 2 | Step 1740 | Loss: 9.2529\n",
      "Epoch 2 | Step 1750 | Loss: 9.0463\n",
      "Epoch 2 | Step 1760 | Loss: 9.3715\n",
      "Epoch 2 | Step 1770 | Loss: 9.7346\n",
      "Epoch 2 | Step 1780 | Loss: 8.5053\n",
      "Epoch 2 | Step 1790 | Loss: 7.3510\n",
      "Epoch 2 | Step 1800 | Loss: 8.3399\n",
      "Epoch 2 | Step 1810 | Loss: 9.1386\n",
      "Epoch 2 | Step 1820 | Loss: 9.4660\n",
      "Epoch 2 | Step 1830 | Loss: 9.4413\n",
      "Epoch 2 | Step 1840 | Loss: 7.9713\n",
      "Epoch 2 | Step 1850 | Loss: 9.0468\n",
      "Epoch 2 | Step 1860 | Loss: 8.7743\n",
      "Epoch 2 | Step 1870 | Loss: 8.1281\n",
      "Epoch 2 | Step 1880 | Loss: 9.6242\n",
      "Epoch 2 | Step 1890 | Loss: 9.0312\n",
      "Epoch 2 | Step 1900 | Loss: 9.7079\n",
      "Epoch 2 | Step 1910 | Loss: 7.9803\n",
      "Epoch 2 | Step 1920 | Loss: 8.0481\n",
      "Epoch 2 | Step 1930 | Loss: 8.9677\n",
      "Epoch 2 | Step 1940 | Loss: 9.4391\n",
      "Epoch 2 | Step 1950 | Loss: 8.1774\n",
      "Epoch 2 | Step 1960 | Loss: 9.1125\n",
      "Epoch 2 | Step 1970 | Loss: 8.8459\n",
      "Epoch 2 | Step 1980 | Loss: 8.2678\n",
      "Epoch 2 | Step 1990 | Loss: 9.2426\n",
      "Epoch 2 | Step 2000 | Loss: 8.6704\n",
      "Epoch 2 | Step 2010 | Loss: 9.4140\n",
      "Epoch 2 | Step 2020 | Loss: 8.8447\n",
      "Epoch 2 | Step 2030 | Loss: 5.7631\n",
      "Epoch 2 | Step 2040 | Loss: 8.5301\n",
      "Epoch 2 | Step 2050 | Loss: 8.3807\n",
      "Epoch 2 | Step 2060 | Loss: 8.9994\n",
      "Epoch 2 | Step 2070 | Loss: 9.6212\n",
      "Epoch 2 | Step 2080 | Loss: 8.2753\n",
      "Epoch 2 | Step 2090 | Loss: 8.8458\n",
      "Epoch 2 | Step 2100 | Loss: 8.4186\n",
      "Epoch 2 | Step 2110 | Loss: 9.8459\n",
      "Epoch 2 | Step 2120 | Loss: 9.4649\n",
      "Epoch 2 | Step 2130 | Loss: 8.9239\n",
      "Epoch 2 | Step 2140 | Loss: 7.6977\n",
      "Epoch 2 | Step 2150 | Loss: 9.3516\n",
      "Epoch 2 | Step 2160 | Loss: 8.4293\n",
      "Epoch 2 | Step 2170 | Loss: 8.0454\n",
      "Epoch 2 | Step 2180 | Loss: 8.9251\n",
      "Epoch 2 | Step 2190 | Loss: 9.2099\n",
      "Epoch 2 | Step 2200 | Loss: 7.1274\n",
      "Epoch 2 | Step 2210 | Loss: 9.2477\n",
      "Epoch 2 | Step 2220 | Loss: 8.3023\n",
      "Epoch 2 | Step 2230 | Loss: 7.8516\n",
      "Epoch 2 | Step 2240 | Loss: 7.7768\n",
      "Epoch 2 | Step 2250 | Loss: 8.7965\n",
      "Epoch 2 | Step 2260 | Loss: 9.3011\n",
      "Epoch 2 | Step 2270 | Loss: 9.3090\n",
      "Epoch 2 | Step 2280 | Loss: 9.2171\n",
      "Epoch 2 | Step 2290 | Loss: 8.7981\n",
      "Epoch 2 | Step 2300 | Loss: 8.5081\n",
      "Epoch 2 | Step 2310 | Loss: 8.6058\n",
      "Epoch 2 | Step 2320 | Loss: 8.9407\n",
      "Epoch 2 | Step 2330 | Loss: 7.7628\n",
      "Epoch 2 | Step 2340 | Loss: 7.9056\n",
      "Epoch 2 | Step 2350 | Loss: 8.9651\n",
      "Epoch 2 | Step 2360 | Loss: 9.1038\n",
      "Epoch 2 | Step 2370 | Loss: 9.3937\n",
      "Epoch 2 | Step 2380 | Loss: 7.7100\n",
      "Epoch 2 | Step 2390 | Loss: 9.5541\n",
      "Epoch 2 | Step 2400 | Loss: 8.3808\n",
      "Epoch 2 | Step 2410 | Loss: 9.2094\n",
      "Epoch 2 | Step 2420 | Loss: 7.8426\n",
      "Epoch 2 | Step 2430 | Loss: 7.6614\n",
      "Epoch 2 | Step 2440 | Loss: 8.4277\n",
      "Epoch 2 | Step 2450 | Loss: 9.6444\n",
      "Epoch 2 | Step 2460 | Loss: 7.4947\n",
      "Epoch 2 | Step 2470 | Loss: 9.6819\n",
      "Epoch 2 | Step 2480 | Loss: 8.4013\n",
      "Epoch 2 | Step 2490 | Loss: 9.2005\n",
      "Epoch 2 | Step 2500 | Loss: 6.7830\n",
      "Epoch 2 | Step 2510 | Loss: 8.6689\n",
      "Epoch 2 | Step 2520 | Loss: 7.0524\n",
      "Epoch 2 | Step 2530 | Loss: 9.1347\n",
      "Epoch 2 | Step 2540 | Loss: 7.9549\n",
      "Epoch 2 | Step 2550 | Loss: 8.9451\n",
      "Epoch 2 | Step 2560 | Loss: 8.7609\n",
      "Epoch 2 | Step 2570 | Loss: 8.9842\n",
      "Epoch 2 | Step 2580 | Loss: 8.5272\n",
      "Epoch 3 | Step 0 | Loss: 8.1435\n",
      "Epoch 3 | Step 10 | Loss: 8.8342\n",
      "Epoch 3 | Step 20 | Loss: 8.0691\n",
      "Epoch 3 | Step 30 | Loss: 8.8260\n",
      "Epoch 3 | Step 40 | Loss: 9.1186\n",
      "Epoch 3 | Step 50 | Loss: 8.7660\n",
      "Epoch 3 | Step 60 | Loss: 8.2203\n",
      "Epoch 3 | Step 70 | Loss: 6.7707\n",
      "Epoch 3 | Step 80 | Loss: 8.9254\n",
      "Epoch 3 | Step 90 | Loss: 9.5738\n",
      "Epoch 3 | Step 100 | Loss: 9.3118\n",
      "Epoch 3 | Step 110 | Loss: 9.4239\n",
      "Epoch 3 | Step 120 | Loss: 8.0945\n",
      "Epoch 3 | Step 130 | Loss: 8.8591\n",
      "Epoch 3 | Step 140 | Loss: 8.3334\n",
      "Epoch 3 | Step 150 | Loss: 9.7755\n",
      "Epoch 3 | Step 160 | Loss: 7.8007\n",
      "Epoch 3 | Step 170 | Loss: 9.0529\n",
      "Epoch 3 | Step 180 | Loss: 6.8354\n",
      "Epoch 3 | Step 190 | Loss: 8.4823\n",
      "Epoch 3 | Step 200 | Loss: 8.8639\n",
      "Epoch 3 | Step 210 | Loss: 8.6304\n",
      "Epoch 3 | Step 220 | Loss: 9.4717\n",
      "Epoch 3 | Step 230 | Loss: 9.0067\n",
      "Epoch 3 | Step 240 | Loss: 9.4757\n",
      "Epoch 3 | Step 250 | Loss: 8.6535\n",
      "Epoch 3 | Step 260 | Loss: 9.2243\n",
      "Epoch 3 | Step 270 | Loss: 8.7871\n",
      "Epoch 3 | Step 280 | Loss: 8.7334\n",
      "Epoch 3 | Step 290 | Loss: 8.9021\n",
      "Epoch 3 | Step 300 | Loss: 8.7440\n",
      "Epoch 3 | Step 310 | Loss: 8.6542\n",
      "Epoch 3 | Step 320 | Loss: 8.8911\n",
      "Epoch 3 | Step 330 | Loss: 9.0166\n",
      "Epoch 3 | Step 340 | Loss: 8.9236\n",
      "Epoch 3 | Step 350 | Loss: 7.0135\n",
      "Epoch 3 | Step 360 | Loss: 8.5748\n",
      "Epoch 3 | Step 370 | Loss: 8.6371\n",
      "Epoch 3 | Step 380 | Loss: 9.0481\n",
      "Epoch 3 | Step 390 | Loss: 7.7731\n",
      "Epoch 3 | Step 400 | Loss: 8.8809\n",
      "Epoch 3 | Step 410 | Loss: 8.0137\n",
      "Epoch 3 | Step 420 | Loss: 8.7020\n",
      "Epoch 3 | Step 430 | Loss: 8.9342\n",
      "Epoch 3 | Step 440 | Loss: 9.1665\n",
      "Epoch 3 | Step 450 | Loss: 8.2945\n",
      "Epoch 3 | Step 460 | Loss: 9.0263\n",
      "Epoch 3 | Step 470 | Loss: 8.5664\n",
      "Epoch 3 | Step 480 | Loss: 7.0737\n",
      "Epoch 3 | Step 490 | Loss: 9.0318\n",
      "Epoch 3 | Step 500 | Loss: 7.8421\n",
      "Epoch 3 | Step 510 | Loss: 8.6429\n",
      "Epoch 3 | Step 520 | Loss: 8.6058\n",
      "Epoch 3 | Step 530 | Loss: 9.5797\n",
      "Epoch 3 | Step 540 | Loss: 9.4740\n",
      "Epoch 3 | Step 550 | Loss: 8.9470\n",
      "Epoch 3 | Step 560 | Loss: 8.7019\n",
      "Epoch 3 | Step 570 | Loss: 7.6493\n",
      "Epoch 3 | Step 580 | Loss: 6.9133\n",
      "Epoch 3 | Step 590 | Loss: 9.2054\n",
      "Epoch 3 | Step 600 | Loss: 9.4022\n",
      "Epoch 3 | Step 610 | Loss: 9.1698\n",
      "Epoch 3 | Step 620 | Loss: 9.7835\n",
      "Epoch 3 | Step 630 | Loss: 9.1349\n",
      "Epoch 3 | Step 640 | Loss: 7.6811\n",
      "Epoch 3 | Step 650 | Loss: 9.7340\n",
      "Epoch 3 | Step 660 | Loss: 9.1044\n",
      "Epoch 3 | Step 670 | Loss: 7.3575\n",
      "Epoch 3 | Step 680 | Loss: 9.8069\n",
      "Epoch 3 | Step 690 | Loss: 9.0536\n",
      "Epoch 3 | Step 700 | Loss: 8.4543\n",
      "Epoch 3 | Step 710 | Loss: 8.0696\n",
      "Epoch 3 | Step 720 | Loss: 8.4960\n",
      "Epoch 3 | Step 730 | Loss: 7.8829\n",
      "Epoch 3 | Step 740 | Loss: 7.7163\n",
      "Epoch 3 | Step 750 | Loss: 8.4765\n",
      "Epoch 3 | Step 760 | Loss: 8.5256\n",
      "Epoch 3 | Step 770 | Loss: 8.9818\n",
      "Epoch 3 | Step 780 | Loss: 9.0295\n",
      "Epoch 3 | Step 790 | Loss: 9.5272\n",
      "Epoch 3 | Step 800 | Loss: 9.1467\n",
      "Epoch 3 | Step 810 | Loss: 7.6863\n",
      "Epoch 3 | Step 820 | Loss: 8.8524\n",
      "Epoch 3 | Step 830 | Loss: 9.5499\n",
      "Epoch 3 | Step 840 | Loss: 8.9500\n",
      "Epoch 3 | Step 850 | Loss: 9.2022\n",
      "Epoch 3 | Step 860 | Loss: 8.5453\n",
      "Epoch 3 | Step 870 | Loss: 9.4280\n",
      "Epoch 3 | Step 880 | Loss: 9.6486\n",
      "Epoch 3 | Step 890 | Loss: 8.4610\n",
      "Epoch 3 | Step 900 | Loss: 9.8350\n",
      "Epoch 3 | Step 910 | Loss: 7.6633\n",
      "Epoch 3 | Step 920 | Loss: 9.7662\n",
      "Epoch 3 | Step 930 | Loss: 9.1759\n",
      "Epoch 3 | Step 940 | Loss: 9.2975\n",
      "Epoch 3 | Step 950 | Loss: 8.6690\n",
      "Epoch 3 | Step 960 | Loss: 9.5335\n",
      "Epoch 3 | Step 970 | Loss: 9.4194\n",
      "Epoch 3 | Step 980 | Loss: 8.6665\n",
      "Epoch 3 | Step 990 | Loss: 9.4659\n",
      "Epoch 3 | Step 1000 | Loss: 8.5241\n",
      "Epoch 3 | Step 1010 | Loss: 6.8380\n",
      "Epoch 3 | Step 1020 | Loss: 7.6172\n",
      "Epoch 3 | Step 1030 | Loss: 8.6810\n",
      "Epoch 3 | Step 1040 | Loss: 9.5865\n",
      "Epoch 3 | Step 1050 | Loss: 9.2709\n",
      "Epoch 3 | Step 1060 | Loss: 7.3403\n",
      "Epoch 3 | Step 1070 | Loss: 7.3386\n",
      "Epoch 3 | Step 1080 | Loss: 8.8478\n",
      "Epoch 3 | Step 1090 | Loss: 9.3769\n",
      "Epoch 3 | Step 1100 | Loss: 8.3388\n",
      "Epoch 3 | Step 1110 | Loss: 9.0768\n",
      "Epoch 3 | Step 1120 | Loss: 8.5732\n",
      "Epoch 3 | Step 1130 | Loss: 6.7381\n",
      "Epoch 3 | Step 1140 | Loss: 9.3753\n",
      "Epoch 3 | Step 1150 | Loss: 8.7921\n",
      "Epoch 3 | Step 1160 | Loss: 9.3351\n",
      "Epoch 3 | Step 1170 | Loss: 6.6959\n",
      "Epoch 3 | Step 1180 | Loss: 7.8700\n",
      "Epoch 3 | Step 1190 | Loss: 8.1040\n",
      "Epoch 3 | Step 1200 | Loss: 6.4059\n",
      "Epoch 3 | Step 1210 | Loss: 8.8747\n",
      "Epoch 3 | Step 1220 | Loss: 9.3165\n",
      "Epoch 3 | Step 1230 | Loss: 7.7929\n",
      "Epoch 3 | Step 1240 | Loss: 9.0946\n",
      "Epoch 3 | Step 1250 | Loss: 8.5609\n",
      "Epoch 3 | Step 1260 | Loss: 8.2173\n",
      "Epoch 3 | Step 1270 | Loss: 9.0360\n",
      "Epoch 3 | Step 1280 | Loss: 8.7558\n",
      "Epoch 3 | Step 1290 | Loss: 10.1589\n",
      "Epoch 3 | Step 1300 | Loss: 8.1752\n",
      "Epoch 3 | Step 1310 | Loss: 9.4884\n",
      "Epoch 3 | Step 1320 | Loss: 8.1679\n",
      "Epoch 3 | Step 1330 | Loss: 8.1944\n",
      "Epoch 3 | Step 1340 | Loss: 7.3867\n",
      "Epoch 3 | Step 1350 | Loss: 8.7483\n",
      "Epoch 3 | Step 1360 | Loss: 8.0190\n",
      "Epoch 3 | Step 1370 | Loss: 8.4922\n",
      "Epoch 3 | Step 1380 | Loss: 8.0142\n",
      "Epoch 3 | Step 1390 | Loss: 9.5969\n",
      "Epoch 3 | Step 1400 | Loss: 9.1881\n",
      "Epoch 3 | Step 1410 | Loss: 9.5080\n",
      "Epoch 3 | Step 1420 | Loss: 7.7328\n",
      "Epoch 3 | Step 1430 | Loss: 10.0506\n",
      "Epoch 3 | Step 1440 | Loss: 8.3367\n",
      "Epoch 3 | Step 1450 | Loss: 9.2490\n",
      "Epoch 3 | Step 1460 | Loss: 8.9580\n",
      "Epoch 3 | Step 1470 | Loss: 9.3902\n",
      "Epoch 3 | Step 1480 | Loss: 9.6771\n",
      "Epoch 3 | Step 1490 | Loss: 9.0686\n",
      "Epoch 3 | Step 1500 | Loss: 6.0474\n",
      "Epoch 3 | Step 1510 | Loss: 9.0370\n",
      "Epoch 3 | Step 1520 | Loss: 8.4936\n",
      "Epoch 3 | Step 1530 | Loss: 8.6856\n",
      "Epoch 3 | Step 1540 | Loss: 7.5286\n",
      "Epoch 3 | Step 1550 | Loss: 9.0475\n",
      "Epoch 3 | Step 1560 | Loss: 7.4068\n",
      "Epoch 3 | Step 1570 | Loss: 9.1488\n",
      "Epoch 3 | Step 1580 | Loss: 8.3619\n",
      "Epoch 3 | Step 1590 | Loss: 8.9830\n",
      "Epoch 3 | Step 1600 | Loss: 8.9967\n",
      "Epoch 3 | Step 1610 | Loss: 9.5275\n",
      "Epoch 3 | Step 1620 | Loss: 10.2479\n",
      "Epoch 3 | Step 1630 | Loss: 9.2386\n",
      "Epoch 3 | Step 1640 | Loss: 8.2442\n",
      "Epoch 3 | Step 1650 | Loss: 8.8542\n",
      "Epoch 3 | Step 1660 | Loss: 9.3695\n",
      "Epoch 3 | Step 1670 | Loss: 9.4285\n",
      "Epoch 3 | Step 1680 | Loss: 6.6015\n",
      "Epoch 3 | Step 1690 | Loss: 7.4402\n",
      "Epoch 3 | Step 1700 | Loss: 9.4158\n",
      "Epoch 3 | Step 1710 | Loss: 6.9181\n",
      "Epoch 3 | Step 1720 | Loss: 8.7509\n",
      "Epoch 3 | Step 1730 | Loss: 9.3688\n",
      "Epoch 3 | Step 1740 | Loss: 9.2649\n",
      "Epoch 3 | Step 1750 | Loss: 8.6461\n",
      "Epoch 3 | Step 1760 | Loss: 7.4549\n",
      "Epoch 3 | Step 1770 | Loss: 9.3400\n",
      "Epoch 3 | Step 1780 | Loss: 8.3303\n",
      "Epoch 3 | Step 1790 | Loss: 8.8284\n",
      "Epoch 3 | Step 1800 | Loss: 8.9445\n",
      "Epoch 3 | Step 1810 | Loss: 6.1468\n",
      "Epoch 3 | Step 1820 | Loss: 8.2676\n",
      "Epoch 3 | Step 1830 | Loss: 8.3708\n",
      "Epoch 3 | Step 1840 | Loss: 9.1864\n",
      "Epoch 3 | Step 1850 | Loss: 7.4048\n",
      "Epoch 3 | Step 1860 | Loss: 9.8891\n",
      "Epoch 3 | Step 1870 | Loss: 8.7315\n",
      "Epoch 3 | Step 1880 | Loss: 8.6417\n",
      "Epoch 3 | Step 1890 | Loss: 8.3073\n",
      "Epoch 3 | Step 1900 | Loss: 6.6257\n",
      "Epoch 3 | Step 1910 | Loss: 8.5660\n",
      "Epoch 3 | Step 1920 | Loss: 9.1758\n",
      "Epoch 3 | Step 1930 | Loss: 9.6060\n",
      "Epoch 3 | Step 1940 | Loss: 6.2229\n",
      "Epoch 3 | Step 1950 | Loss: 8.8632\n",
      "Epoch 3 | Step 1960 | Loss: 8.4903\n",
      "Epoch 3 | Step 1970 | Loss: 7.8718\n",
      "Epoch 3 | Step 1980 | Loss: 8.7159\n",
      "Epoch 3 | Step 1990 | Loss: 8.8895\n",
      "Epoch 3 | Step 2000 | Loss: 8.9084\n",
      "Epoch 3 | Step 2010 | Loss: 8.8833\n",
      "Epoch 3 | Step 2020 | Loss: 9.4228\n",
      "Epoch 3 | Step 2030 | Loss: 9.4960\n",
      "Epoch 3 | Step 2040 | Loss: 9.2644\n",
      "Epoch 3 | Step 2050 | Loss: 7.3470\n",
      "Epoch 3 | Step 2060 | Loss: 9.8477\n",
      "Epoch 3 | Step 2070 | Loss: 9.6864\n",
      "Epoch 3 | Step 2080 | Loss: 8.4205\n",
      "Epoch 3 | Step 2090 | Loss: 9.4578\n",
      "Epoch 3 | Step 2100 | Loss: 9.5567\n",
      "Epoch 3 | Step 2110 | Loss: 8.5683\n",
      "Epoch 3 | Step 2120 | Loss: 8.6489\n",
      "Epoch 3 | Step 2130 | Loss: 9.7769\n",
      "Epoch 3 | Step 2140 | Loss: 8.6832\n",
      "Epoch 3 | Step 2150 | Loss: 7.9362\n",
      "Epoch 3 | Step 2160 | Loss: 9.1213\n",
      "Epoch 3 | Step 2170 | Loss: 9.4152\n",
      "Epoch 3 | Step 2180 | Loss: 7.6893\n",
      "Epoch 3 | Step 2190 | Loss: 8.8788\n",
      "Epoch 3 | Step 2200 | Loss: 7.8569\n",
      "Epoch 3 | Step 2210 | Loss: 7.6055\n",
      "Epoch 3 | Step 2220 | Loss: 8.7676\n",
      "Epoch 3 | Step 2230 | Loss: 9.1591\n",
      "Epoch 3 | Step 2240 | Loss: 9.0634\n",
      "Epoch 3 | Step 2250 | Loss: 9.2821\n",
      "Epoch 3 | Step 2260 | Loss: 9.0826\n",
      "Epoch 3 | Step 2270 | Loss: 9.8658\n",
      "Epoch 3 | Step 2280 | Loss: 9.1964\n",
      "Epoch 3 | Step 2290 | Loss: 7.4948\n",
      "Epoch 3 | Step 2300 | Loss: 10.0008\n",
      "Epoch 3 | Step 2310 | Loss: 9.1144\n",
      "Epoch 3 | Step 2320 | Loss: 8.6976\n",
      "Epoch 3 | Step 2330 | Loss: 9.2909\n",
      "Epoch 3 | Step 2340 | Loss: 9.4430\n",
      "Epoch 3 | Step 2350 | Loss: 8.5150\n",
      "Epoch 3 | Step 2360 | Loss: 8.3472\n",
      "Epoch 3 | Step 2370 | Loss: 7.2111\n",
      "Epoch 3 | Step 2380 | Loss: 8.5984\n",
      "Epoch 3 | Step 2390 | Loss: 6.7210\n",
      "Epoch 3 | Step 2400 | Loss: 8.8708\n",
      "Epoch 3 | Step 2410 | Loss: 9.5238\n",
      "Epoch 3 | Step 2420 | Loss: 9.8624\n",
      "Epoch 3 | Step 2430 | Loss: 8.2122\n",
      "Epoch 3 | Step 2440 | Loss: 9.1368\n",
      "Epoch 3 | Step 2450 | Loss: 8.5023\n",
      "Epoch 3 | Step 2460 | Loss: 7.3866\n",
      "Epoch 3 | Step 2470 | Loss: 8.8663\n",
      "Epoch 3 | Step 2480 | Loss: 9.6175\n",
      "Epoch 3 | Step 2490 | Loss: 9.0558\n"
     ]
    }
   ],
   "source": [
    "!python sft_12hz.py --init_model_path \"./Qwen3-TTS-12Hz-1.7B-Base\" --output_model_path \"../../output_pathumma\" --train_jsonl \"../../manifest_pathumma_train_with_code_fixed.jsonl\" --batch_size 1 --lr 1e-6 --num_epochs 10 --speaker_name multi_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f28d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from qwen_tts import Qwen3TTSModel\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "tts = Qwen3TTSModel.from_pretrained(\n",
    "    \"output_whisper/checkpoint-epoch-2\",\n",
    "    device_map=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "ref_dict = {\"Lisa\": \"C:/Users/User/Comvi/Voice/Lisa/dataset_denoised/gEMrqw-pAy4/SPEAKER_01_sent_0001_DeepFilterNet3.wav\",\n",
    "            \"Bambam\": \"C:/Users/User/Comvi/Voice/Bambam/dataset_denoised/zbo7Mk3ryaI/SPEAKER_00_sent_0001_DeepFilterNet3.wav\",\n",
    "            \"IU\": \"C:/Users/User/Comvi/Voice/IU/dataset_denoised/wCbUWU4l_Ko/SPEAKER_01_sent_0000_DeepFilterNet3.wav\",\n",
    "            \"IVE\": \"C:/Users/User/Comvi/Voice/IVE/dataset_denoised/_037bSnAyRg/SPEAKER_01_sent_0007_DeepFilterNet3.wav\"\n",
    "            }\n",
    "\n",
    "def generate_voice(speaker_name, text_input, output_file):\n",
    "    text_input = f\"[{speaker_name}] \" + text_input\n",
    "\n",
    "    wavs, sr = tts.generate_custom_voice(\n",
    "        text=text_input,\n",
    "        ref_audio=ref_dict[speaker_name],\n",
    "        speaker=None\n",
    "    )\n",
    "\n",
    "    sf.write(output_file, wavs[0], sr)\n",
    "\n",
    "# song1 = \"\"\"ระหว่างที่มีดาวดวงนึงโคจรในอวกาศ..\n",
    "# จากวันนั้นฉันก็มองโลกไม่เหมือนเดิม...\n",
    "# สิ่งที่สวยงามในชีวิตพบเจอ..\n",
    "# คงไม่เหลือคุณค่าอีกแล้ว.. ถูกมาแทนด้วยดวงตาคู่นั้นของ..เธอ...\n",
    "# ปฏิทินจากนี้มีไว้เพื่อนับวัน.. นาฬิกามีไว้เฝ้าคอยนับนาที...\n",
    "# ของชีวิตฉันนับต่อจากนี้ ที่จะมีไว้เพื่อเธอเท่านั้น...\n",
    "\n",
    "# ขออยู่ในชีวิตที่เหลือของเธอได้ไหม..\n",
    "# อยากลืมตาแล้วได้พบเธอจนวันสุดท้าย..\n",
    "# อยากเป็นคนที่ได้นอนดูดาวข้างเธออีกหมื่นวัน\n",
    "# และเอนไปจุมพิตเธอซักล้านครั้ง\n",
    "# อยู่กับฉันไปนานๆ... นะเธอ.....\n",
    "\n",
    "# ยังมีเพลงรักเป็นพันบทเพลงรอแชร์ให้เธอได้ฟัง..\n",
    "# ยังมีอีกหลายทริปที่เรานั้นยังต้องไปเที่ยวด้วยกัน...\n",
    "# ไม่รวมอีกเรื่องที่ยังสำคัญ ลูกเราที่สักวันจะเกิดมา\n",
    "# ด้วยพันธสัญญาความรักของเรา...\n",
    "\n",
    "# และมีดาวหางดวงนึงที่ยังโคจรในอวกาศ..\n",
    "# ในช่วงชีวิตจะมีหนึ่งครั้งที่มองเห็นได้ด้วยตา..\n",
    "# ดาวที่ฉันยังไม่เคยเห็นมาก่อน\n",
    "# ขอให้ถึงวันนั้น\n",
    "# ได้มีเธอรอดูมันด้วยกันกับฉัน...\n",
    "\n",
    "# ขออยู่ในชีวิตที่เหลือของเธอได้ไหม..\n",
    "# อยากลืมตาแล้วได้พบเธอจนวันสุดท้าย..\n",
    "# อยากเป็นคนที่ได้นอนดูดาวข้างเธออีกหมื่นวัน\n",
    "# และเอนไปจุมพิตเธอซักล้านครั้ง\n",
    "# อยู่กับฉันไปนานๆ... นะเธอ.....\n",
    "\n",
    "\n",
    "# ขออยู่ในชีวิตที่เหลือของเธอได้ไหม..\n",
    "# อยากลืมตาแล้วได้พบเธอจนวันสุดท้าย..\n",
    "# อยากเป็นคนที่ได้นอนดูดาวข้างเธออีกหมื่นวัน\n",
    "# และเอนไปจุมพิตเธอซักล้านครั้ง\n",
    "# อยู่กับฉันไปนานๆ... นะเธอ.....\n",
    "\n",
    "# อยากเป็นคนที่ได้นอนดูดาวข้างเธออีกหมื่นวัน\n",
    "# และเอนไปจุมพิตเธอซักล้านครั้ง.....\n",
    "# อยู่กับฉันไปนานๆ... นะเธอ.....\"\"\"\n",
    "\n",
    "# song2 = \"\"\"\n",
    "# ได้มาพ้อ..น้องแก้มอ่องต่องในวันสงกรานต์..\n",
    "# เต้นอยู่หน้าฮ้าน.. เมื่อคืนเทศกาลวันไหล...\n",
    "# เจ้ายิ้มเข้ามาหยอก บอกว่าฮักอ้ายเบิดใจ..\n",
    "# เข้ามาซบที่ตรงไหล่.. เจ้าว่าหัวใจเฮาตรงกัน\n",
    "\n",
    "# อยู่กันสองคืน.. เจ้ามาขอกลับไปเฮ็ดงาน..\n",
    "# ก่อนกระเจียวบาน.. บ่ต้องย่านสิกลับมาหา..\n",
    "# อ้ายก็ถ่าจนเศร้า.. กอดรูปเจ้าทั้งน้ำตา..\n",
    "# เจ้าคงลืมสัญญา..สามเดือนกว่ากว่าแล้วไปอยู่ไส....\n",
    "\n",
    "# อยาก...เจอ ได้แค่เพียงคิดฮอด รอวันได้กอด...เธอ\n",
    "# เฝ้าแต่ฝันละเมอกับคำว่าฮักที่เธอนั้นบอก\n",
    "# อ้ายบ่ฮู้ว่าจริงหรือหยอก\n",
    "# ย่านคนสวยน้องสิมาหลอกให้อ้ายช้ำ..\n",
    "\n",
    "# ดอกกระเจียวบาน.. อีกไม่นานก็คงสิเฉา..\n",
    "# อ้ายก็รอเจ้าอยู่คือเก่า ไปเป็นผู้สาวผู้ใดหนอ..นาง\n",
    "# นั่ง...คึดฮอด..บ่ได้นอนจนฟ้าสาง..\n",
    "# ย่านความฮักเฮาแตกม่าง\n",
    "# เจ้าลืมทุกอย่างของสองเฮา...\n",
    "\n",
    "# ดอกกระเจียวบาน.. ผ่านหน้าแล้งเจ้าไปอยู่ไส..\n",
    "# เข้าหน้าฝนบ่โดนเท่าไหร่..\n",
    "# น้ำตาของอ้ายกะไหลหย่าว...\n",
    "# ใจสวอย..อย่าให้คอยถึงหน้าหนาว....\n",
    "# จนดอกกระเจียวของอ้ายเหี่ยวเฉา\n",
    "# เจ้ายังบ่..คืนมา.....\n",
    "\n",
    "# อยาก...เจอ ได้แค่เพียงคิดฮอด รอวันได้กอด...เธอ\n",
    "# เฝ้าแต่ฝันละเมอกับคำว่าฮักที่เธอนั้นบอก\n",
    "# อ้ายบ่ฮู้ว่าจริงหรือหยอก\n",
    "# ย่านคนสวยน้องสิมาหลอกให้อ้ายช้ำ....\n",
    "\n",
    "# ดอกกระเจียวบาน.. อีกไม่นานก็คงสิเฉา..\n",
    "# อ้ายก็รอเจ้าอยู่คือเก่า ไปเป็นผู้สาวผู้ใดหนอ..นาง\n",
    "# นั่ง...คึดฮอด..บ่ได้นอนจนฟ้าสาง..\n",
    "# ย่านความฮักเฮาแตกม่าง\n",
    "# เจ้าลืมทุกอย่างของสองเฮา...\n",
    "\n",
    "# ดอกกระเจียวบาน.. ผ่านหน้าแล้งเจ้าไปอยู่ไส..\n",
    "# เข้าหน้าฝนบ่โดนเท่าไหร่..\n",
    "# น้ำตาของอ้ายกะไหลหย่าว...\n",
    "# ใจสวอย..อย่าให้คอยถึงหน้า....หนาว....\n",
    "# จนดอกกระเจียวของอ้ายเหี่ยวเฉา\n",
    "# เจ้ายังบ่..คืนมา........\n",
    "\n",
    "\n",
    "# ดอกกระเจียวบาน.. ผ่านหน้าแล้งเจ้าไปอยู่ไส..\n",
    "# เข้าหน้าฝนบ่โดนเท่าไหร่..\n",
    "# น้ำตาของอ้ายกะไหลหย่าว...\n",
    "# ใจสวอย..อย่าให้คอยถึงหน้า....หนาว....\n",
    "# จนดอกกระเจียวของอ้ายเหี่ยวเฉา\n",
    "# เจ้ายังบ่..คืนมา........\n",
    "\n",
    "# จนกระเจียวของอ้ายเหี่ยวเฉา\n",
    "# เจ้ายังบ่คืนมา....\"\"\"\n",
    "\n",
    "# generate_voice(\"Lisa\", song1, \"C:/Users/User/Comvi/Voice/Lisa_song1_whisper_epoch-2.wav\")\n",
    "# generate_voice(\"Lisa\", song2, \"C:/Users/User/Comvi/Voice/Lisa_song2_whisper_epoch-2.wav\")\n",
    "\n",
    "# generate_voice(\"Bambam\", song1, \"C:/Users/User/Comvi/Voice/Bambam_song1_whisper_epoch-2.wav\")\n",
    "# generate_voice(\"Bambam\", song2, \"C:/Users/User/Comvi/Voice/Bambam_song2_whisper_epoch-2.wav\")\n",
    "\n",
    "# generate_voice(\"IU\", song1, \"C:/Users/User/Comvi/Voice/IU_song1_whisper_epoch-2.wav\")\n",
    "# generate_voice(\"IU\", song2, \"C:/Users/User/Comvi/Voice/IU_song2_whisper_epoch-2.wav\")\n",
    "\n",
    "# generate_voice(\"IVE\", song1, \"C:/Users/User/Comvi/Voice/IVE_song1_whisper_epoch-2.wav\")\n",
    "# generate_voice(\"IVE\", song2, \"C:/Users/User/Comvi/Voice/IVE_song2_whisper_epoch-2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b6602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "song1 = \"\"\"ขออยู่ในชีวิตที่เหลือของเธอได้ไหม..\n",
    "อยากลืมตาแล้วได้พบเธอจนวันสุดท้าย..\n",
    "อยากเป็นคนที่ได้นอนดูดาวข้างเธออีกหมื่นวัน\n",
    "และเอนไปจุมพิตเธอซักล้านครั้ง\n",
    "อยู่กับฉันไปนานๆ... นะเธอ.....\"\"\"\n",
    "\n",
    "song2 = \"\"\"ดอกกระเจียวบาน.. อีกไม่นานก็คงสิเฉา..\n",
    "อ้ายก็รอเจ้าอยู่คือเก่า ไปเป็นผู้สาวผู้ใดหนอ..นาง\n",
    "นั่ง...คึดฮอด..บ่ได้นอนจนฟ้าสาง..\n",
    "ย่านความฮักเฮาแตกม่าง\n",
    "เจ้าลืมทุกอย่างของสองเฮา...\n",
    "\n",
    "ดอกกระเจียวบาน.. ผ่านหน้าแล้งเจ้าไปอยู่ไส..\n",
    "เข้าหน้าฝนบ่โดนเท่าไหร่..\n",
    "น้ำตาของอ้ายกะไหลหย่าว...\n",
    "ใจสวอย..อย่าให้คอยถึงหน้าหนาว....\n",
    "จนดอกกระเจียวของอ้ายเหี่ยวเฉา\n",
    "เจ้ายังบ่..คืนมา.....\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "generate_voice(\"Lisa\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/Lisa_song1_whisper_epoch-2.wav\")\n",
    "generate_voice(\"Lisa\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/Lisa_song2_whisper_epoch-2.wav\")\n",
    "\n",
    "generate_voice(\"Bambam\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/Bambam_song1_whisper_epoch-2.wav\")\n",
    "generate_voice(\"Bambam\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/Bambam_song2_whisper_epoch-2.wav\")\n",
    "\n",
    "generate_voice(\"IU\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/IU_song1_whisper_epoch-2.wav\")\n",
    "generate_voice(\"IU\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/IU_song2_whisper_epoch-2.wav\")\n",
    "\n",
    "generate_voice(\"IVE\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/IVE_song1_whisper_epoch-2.wav\")\n",
    "generate_voice(\"IVE\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/IVE_song2_whisper_epoch-2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1abfbaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from qwen_tts import Qwen3TTSModel\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "tts = Qwen3TTSModel.from_pretrained(\n",
    "    \"output_pathumma/checkpoint-epoch-2\",\n",
    "    device_map=device,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "ref_dict = {\"Lisa\": \"C:/Users/User/Comvi/Voice/Lisa/dataset_denoised/gEMrqw-pAy4/SPEAKER_01_sent_0001_DeepFilterNet3.wav\",\n",
    "            \"Bambam\": \"C:/Users/User/Comvi/Voice/Bambam/dataset_denoised/zbo7Mk3ryaI/SPEAKER_00_sent_0001_DeepFilterNet3.wav\",\n",
    "            \"IU\": \"C:/Users/User/Comvi/Voice/IU/dataset_denoised/wCbUWU4l_Ko/SPEAKER_01_sent_0000_DeepFilterNet3.wav\",\n",
    "            \"IVE\": \"C:/Users/User/Comvi/Voice/IVE/dataset_denoised/_037bSnAyRg/SPEAKER_01_sent_0007_DeepFilterNet3.wav\"\n",
    "            }\n",
    "\n",
    "def generate_voice(speaker_name, text_input, output_file):\n",
    "    text_input = f\"[{speaker_name}] \" + text_input\n",
    "\n",
    "    wavs, sr = tts.generate_custom_voice(\n",
    "        text=text_input,\n",
    "        ref_audio=ref_dict[speaker_name],\n",
    "        speaker=None\n",
    "    )\n",
    "\n",
    "    sf.write(output_file, wavs[0], sr)\n",
    "\n",
    "generate_voice(\"Lisa\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/Lisa_song1_pathumma_epoch-2.wav\")\n",
    "generate_voice(\"Lisa\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/Lisa_song2_pathumma_epoch-2.wav\")\n",
    "\n",
    "generate_voice(\"Bambam\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/Bambam_song1_pathumma_epoch-2.wav\")\n",
    "generate_voice(\"Bambam\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/Bambam_song2_pathumma_epoch-2.wav\")\n",
    "\n",
    "generate_voice(\"IU\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/IU_song1_pathumma_epoch-2.wav\")\n",
    "generate_voice(\"IU\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/IU_song2_pathumma_epoch-2.wav\")\n",
    "\n",
    "generate_voice(\"IVE\", song1, \"C:/Users/User/Comvi/Voice/inference_audio/IVE_song1_whisper_epoch-2.wav\")\n",
    "generate_voice(\"IVE\", song2, \"C:/Users/User/Comvi/Voice/inference_audio/IVE_song2_whisper_epoch-2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeac22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
